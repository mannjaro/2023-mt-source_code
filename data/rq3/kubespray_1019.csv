commit_hash,commit_date,message,filename,status,issue_num,diff,category,category_zukawa,category_watanabe,memo_zukawa,memo_watanabe
e38258381fd0d111cd339ce3e334920d5898dbe9,2016-08-02 14:56:31+00:00,Wait for static pods when setting up Fixes #390,roles/kubernetes/master/tasks/main.yml,MODIFY,390,"@@ -19,12 +19,7 @@
     src: manifests/kube-apiserver.manifest.j2
     dest: ""{{ kube_manifest_dir }}/kube-apiserver.manifest""
   register: apiserver_manifest
-
-- name: restart kubelet
-  service:
-    name: kubelet
-    state: restarted
-  when: apiserver_manifest.changed
+  notify: Master | restart kubelet
 
 - name: wait for the apiserver to be running
   uri: url=http://localhost:8080/healthz
@@ -58,8 +53,10 @@
   template:
     src: manifests/kube-controller-manager.manifest.j2
     dest: ""{{ kube_manifest_dir }}/kube-controller-manager.manifest""
+  notify: wait for kube-controller-manager
 
 - name: Write kube-scheduler manifest
   template:
     src: manifests/kube-scheduler.manifest.j2
     dest: ""{{ kube_manifest_dir }}/kube-scheduler.manifest""
+  notify: wait for kube-scheduler
",notify,notify,notify,一部削除されたタスクがあるが，全体の変更としては通知箇所の追加が見られる,notifyに変更している
e38258381fd0d111cd339ce3e334920d5898dbe9,2016-08-02 14:56:31+00:00,Wait for static pods when setting up Fixes #390,roles/kubernetes/master/handlers/main.yml,MODIFY,390,"@@ -1,4 +1,35 @@
 ---
-- name: restart kube-apiserver
-  set_fact:
-    restart_apimaster: True
+- name: Master | restart kubelet
+  command: /bin/true
+  notify:
+    - Kubelet | reload systemd
+    - Kubelet | reload kubelet
+
+- name: wait for master static pods
+  command: /bin/true
+  notify:
+    - wait for kube-scheduler
+    - wait for kube-controller-manager
+
+- name: Master | reload systemd
+  command: systemctl daemon-reload
+  when: ansible_service_mgr == ""systemd""
+
+- name: Master | reload kubelet
+  service:
+    name: kubelet
+    state: restarted
+
+- name: wait for kube-scheduler
+  uri: url=http://localhost:10251/healthz
+  register: scheduler_result
+  until: scheduler_result.status == 200
+  retries: 15
+  delay: 5
+
+- name: wait for kube-controller-manager
+  uri: url=http://localhost:10252/healthz
+  register: controller_manager_result
+  until: controller_manager_result.status == 200
+  retries: 15
+  delay: 5
",add_task,add_task,add_task,通知を行うだけのタスクやサービス関連のタスクなど複数のタスクが追加されている,タスクが追加されている
643b28f9d3524cb213dcd186be7bfebdfda4c5bb,2016-08-24 10:36:25+00:00,"Revert ""Fix resolv.conf search/nameserver"" This reverts commit 977f82c32c3e92155538af87842e6b4fb3a59141.",roles/dnsmasq/tasks/main.yml,MODIFY,None,"@@ -68,17 +68,9 @@
     resolvconffile: >-
       {%- if resolvconf.rc == 0 -%}/etc/resolvconf/resolv.conf.d/head{%- else -%}/etc/resolv.conf{%- endif -%}
 
-- name: generate search domains to resolvconf
-  set_fact:
-    searchentries=""{{ ([ 'default.svc.' + dns_domain, 'svc.' + dns_domain ] + searchdomains|default([])) | join(' ') }}""
-
-- name: generate nameservers to resolvconf
-  set_fact:
-    nameserverentries=""{{ nameservers|default([]) + [ dns_server ] }}""
-
 - name: Add search resolv.conf
   lineinfile:
-    line: ""search {{searchentries}}""
+    line: ""search {{ [ 'default.svc.' + dns_domain, 'svc.' + dns_domain, dns_domain ] | join(' ') }}""
     dest: ""{{resolvconffile}}""
     state: present
     insertbefore: BOF
@@ -87,13 +79,12 @@
 
 - name: Add local dnsmasq to resolv.conf
   lineinfile:
-    line: ""nameserver {{item}}""
+    line: ""nameserver {{dns_server}}""
     dest: ""{{resolvconffile}}""
     state: present
     insertafter: ""^search.*$""
     backup: yes
     follow: yes
-  with_items: ""{{nameserverentries}}""
 
 - name: Add options to resolv.conf
   lineinfile:
@@ -109,13 +100,11 @@
     - attempts:2
 
 - name: disable resolv.conf modification by dhclient
-  copy: src=dhclient_nodnsupdate dest=/etc/dhcp/dhclient-enter-hooks.d/znodnsupdate mode=0755 backup=yes
-  notify: Dnsmasq | restart network
+  copy: src=dhclient_nodnsupdate dest=/etc/dhcp/dhclient-enter-hooks.d/nodnsupdate mode=0755 backup=yes
   when: ansible_os_family == ""Debian""
 
 - name: disable resolv.conf modification by dhclient
-  copy: src=dhclient_nodnsupdate dest=/etc/dhcp/dhclient.d/znodnsupdate mode=u+x backup=yes
-  notify: Dnsmasq | restart network
+  copy: src=dhclient_nodnsupdate dest=/etc/dhcp/dhclient.d/nodnsupdate mode=u+x backup=yes
   when: ansible_os_family == ""RedHat""
 
 - name: update resolvconf
",edit_args,edit_args,edit_var,revert_commitである,set_factで設定していた値ではなくて、直接書く方法にRevertしてる。templateでの処理は似てるけど、ちょっと違うのでedit_varにした
643b28f9d3524cb213dcd186be7bfebdfda4c5bb,2016-08-24 10:36:25+00:00,"Revert ""Fix resolv.conf search/nameserver"" This reverts commit 977f82c32c3e92155538af87842e6b4fb3a59141.",roles/dnsmasq/handlers/main.yml,DELETE,None,"@@ -1,10 +0,0 @@
-- name: Dnsmasq | restart network
-  service:
-    name: >-
-      {% if ansible_os_family == ""RedHat"" -%}
-      network
-      {%- elif ansible_os_family == ""Debian"" -%}
-      networking
-      {%- endif %}
-    state: restarted
-  when: ansible_os_family != ""CoreOS""
",remove_task,remove_task,remove_task,削除されたタスクがどこにも移動していないため,taskの削除だけ
643b28f9d3524cb213dcd186be7bfebdfda4c5bb,2016-08-24 10:36:25+00:00,"Revert ""Fix resolv.conf search/nameserver"" This reverts commit 977f82c32c3e92155538af87842e6b4fb3a59141.",roles/dnsmasq/defaults/main.yml,DELETE,None,"@@ -1,12 +0,0 @@
----
-# Existing search/nameserver resolvconf entries will be purged and
-# ensured by this additional data:
-
-# Max of 4 names is allowed and no more than 256 - 17 chars total
-# (a 2 is reserved for the 'default.svc.' and'svc.')
-#searchdomains:
-#  - foo.bar.lc
-
-# Max of 2 is allowed here (a 1 is reserved for the dns_server)
-#nameservers:
-#  - 127.0.0.1
",comment,other,other,削除されている箇所がコメントのみであるため,そもそもコメントの部分を削除している
caa81f3ac2150bd8a54d7c311f978fa7f8bae6ab,2016-11-14 13:49:17+00:00,Fix etcd ssl for canal - Move CNI configuration from `kubernetes/node` role to `network_plugin/canal` - Create SSL dir for Canal and symlink etcd SSL files - Add needed options to `canal-config` configmap - Run flannel and calico-node containers with proper configuration,roles/network_plugin/canal/tasks/main.yml,MODIFY,None,"@@ -1,4 +1,28 @@
 ---
+- name: Canal | Write Canal cni config
+  template:
+    src: ""cni-canal.conf.j2""
+    dest: ""/etc/cni/net.d/10-canal.conf""
+    owner: kube
+
+- name: Canal | Create canal certs directory
+  file:
+    dest: ""{{ canal_cert_dir }}""
+    state: directory
+    mode: 0750
+    owner: root
+    group: root
+
+- name: Canal | Link etcd certificates for canal-node
+  file:
+    src: ""{{ etcd_cert_dir }}/{{ item.s }}""
+    dest: ""{{ canal_cert_dir }}/{{ item.d }}""
+    state: hard
+  with_items:
+    - {s: ""ca.pem"", d: ""ca_cert.crt""}
+    - {s: ""node.pem"", d: ""cert.crt""}
+    - {s: ""node-key.pem"", d: ""key.pem""}
+
 - name: Canal | Set Flannel etcd configuration
   command: |-
     {{ bin_dir }}/etcdctl --peers={{ etcd_access_addresses }} \
",add_task,add_task,add_task,Canal | Write Canal cni config'タスクのみ移動してきたが，それ以外は新規のタスクである,タスクの追加
caa81f3ac2150bd8a54d7c311f978fa7f8bae6ab,2016-11-14 13:49:17+00:00,Fix etcd ssl for canal - Move CNI configuration from `kubernetes/node` role to `network_plugin/canal` - Create SSL dir for Canal and symlink etcd SSL files - Add needed options to `canal-config` configmap - Run flannel and calico-node containers with proper configuration,roles/network_plugin/canal/defaults/main.yml,MODIFY,None,"@@ -9,3 +9,7 @@ canal_masquerade: ""true""
 
 # Log-level
 canal_log_level: ""info""
+
+# Etcd SSL dirs
+canal_cert_dir: /etc/canal/certs
+etcd_cert_dir: /etc/ssl/etcd/ssl
",edit_var,edit_var,edit_var,新規タスクに使われる変数が追加された,変数の追加
caa81f3ac2150bd8a54d7c311f978fa7f8bae6ab,2016-11-14 13:49:17+00:00,Fix etcd ssl for canal - Move CNI configuration from `kubernetes/node` role to `network_plugin/canal` - Create SSL dir for Canal and symlink etcd SSL files - Add needed options to `canal-config` configmap - Run flannel and calico-node containers with proper configuration,roles/kubernetes/node/tasks/main.yml,MODIFY,None,"@@ -11,13 +11,6 @@
     owner: kube
   when: kube_network_plugin == ""calico""
 
-- name: Write Canal cni config
-  template:
-    src: ""cni-canal.conf.j2""
-    dest: ""/etc/cni/net.d/10-canal.conf""
-    owner: kube
-  when: kube_network_plugin == ""canal""
-
 - name: Write kubelet config file
   template: src=kubelet.j2 dest={{ kube_config_dir }}/kubelet.env backup=yes
   notify:
",move,move,remove_task,roles/network_plugin/canal/tasks/main.ymlに移動している,タスクの削除
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/kubernetes/node/meta/main.yml,MODIFY,None,"@@ -28,13 +28,10 @@ dependencies:
     tags: [download, netchecker]
   - role: download
     file: ""{{ downloads.kubednsmasq }}""
-    when: not skip_dnsmasq_k8s|default(false)
     tags: [download, dnsmasq]
   - role: download
     file: ""{{ downloads.kubedns }}""
-    when: not skip_dnsmasq_k8s|default(false)
     tags: [download, dnsmasq]
   - role: download
     file: ""{{ downloads.exechealthz }}""
-    when: not skip_dnsmasq_k8s|default(false)
     tags: [download, dnsmasq]
",condition,condition,condition,whenディレクティブそのものが削除されている,whenの削除
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/kubernetes/preinstall/tasks/set_facts.yml,MODIFY,None,"@@ -49,3 +49,6 @@
     etcd_after_v3: etcd_version | version_compare(""v3.0.0"", "">="")
 - set_fact:
     etcd_container_bin_dir: ""{% if etcd_after_v3 %}/usr/local/bin/{% else %}/{% endif %}""
+- set_fact:
+    default_resolver: >-
+      {%- if cloud_provider is defined and cloud_provider == 'gce' -%}169.254.169.254{%- else -%}8.8.8.8{%- endif -%}
",edit_var,edit_var,edit_var,set_factモジュールを用いてdefault_resolverが追加されている,変数の追加
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/network_plugin/flannel/tasks/main.yml,MODIFY,None,"@@ -11,7 +11,7 @@
   template:
     src: flannel-pod.yml
     dest: /etc/kubernetes/manifests/flannel-pod.manifest
-  notify: delete default docker bridge
+  notify: Flannel | delete default docker bridge
 
 - name: Flannel | Wait for flannel subnet.env file presence
   wait_for:
@@ -67,7 +67,7 @@
     group: root
     mode: 0644
   notify:
-    - restart docker
+    - Flannel | restart docker
   when: ansible_service_mgr in [""sysvinit"",""upstart""]
 
 - name: Flannel | Create docker network systemd drop-in
@@ -75,7 +75,7 @@
     src: flannel-options.conf.j2
     dest: ""/etc/systemd/system/docker.service.d/flannel-options.conf""
   notify:
-    - restart docker
+    - Flannel | restart docker
   when: ansible_service_mgr == ""systemd""
 
 - meta: flush_handlers
",notify,notify,notify,notifyの名前に'Flannel | 'を追加した名前に,notify先をdockerからFlannelに変更している
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/kubernetes/preinstall/tasks/resolvconf.yml,RENAME,None,"@@ -4,17 +4,33 @@
   register: resolvconf
   ignore_errors: yes
   changed_when: false
+  tags: facts
+
+- name: check kubelet
+  stat:
+    path: ""{{ bin_dir }}/kubelet""
+  register: kubelet
+  changed_when: false
+  tags: facts
+
+- name: check if early DNS configuration stage
+  set_fact:
+    dns_early: >-
+      {%- if kubelet.stat.exists -%}false{%- else -%}true{%- endif -%}
+  tags: facts
 
 - name: target resolv.conf file
   set_fact:
     resolvconffile: >-
       {%- if resolvconf.rc == 0 -%}/etc/resolvconf/resolv.conf.d/head{%- else -%}/etc/resolv.conf{%- endif -%}
   when: ansible_os_family != ""CoreOS""
+  tags: facts
 
 - name: target temporary resolvconf cloud init file
   set_fact:
     resolvconffile: /tmp/resolveconf_cloud_init_conf
   when: ansible_os_family == ""CoreOS""
+  tags: facts
 
 - name: create temporary resolveconf cloud init file
   command: cp -f /etc/resolv.conf ""{{ resolvconffile }}""
@@ -24,16 +40,30 @@
   set_fact:
     searchentries:
       ""{{ ([ 'default.svc.' + dns_domain, 'svc.' + dns_domain ] + searchdomains|default([])) | join(' ') }}""
+  tags: facts
+
+- name: decide on dns server IP
+  set_fact:
+    dns_server_real: >-
+      {%- if dns_early|bool -%}{{default_resolver}}{%- else -%}{{dns_server}}{%- endif -%}
 
-- name: pick dnsmasq cluster IP
+- name: pick dnsmasq cluster IP or default resolver
   set_fact:
-    dnsmasq_server: >-
-      {%- if skip_dnsmasq|bool -%}{{ [ skydns_server ] + upstream_dns_servers|default([]) }}{%- else -%}{{ [ dns_server ] }}{%- endif -%}
+    dnsmasq_server: |-
+      {%- if skip_dnsmasq|bool and not dns_early|bool -%}
+        {{ [ skydns_server ] + upstream_dns_servers|default([]) }}
+      {%- elif dns_early|bool -%}
+        {{ [ dns_server_real ] + upstream_dns_servers|default([]) }}
+      {%- else -%}
+        {{ [ dns_server ] }}
+      {%- endif -%}
+  tags: facts
 
 - name: generate nameservers to resolvconf
   set_fact:
     nameserverentries:
       ""{{ dnsmasq_server|default([]) + nameservers|default([]) }}""
+  tags: facts
 
 - name: Remove search and nameserver options from resolvconf head
   lineinfile:
@@ -46,7 +76,7 @@
     - search
     - nameserver
   when: resolvconf.rc == 0
-  notify: Dnsmasq | update resolvconf
+  notify: Preinstall | update resolvconf
 
 - name: Remove search and nameserver options from resolvconf cloud init temporary file
   lineinfile:
@@ -59,7 +89,7 @@
     - search
     - nameserver
   when: ansible_os_family == ""CoreOS""
-  notify: Dnsmasq | update resolvconf for CoreOS
+  notify: Preinstall | update resolvconf for CoreOS
 
 - name: Add search domains to resolvconf file
   lineinfile:
@@ -69,7 +99,7 @@
     insertbefore: BOF
     backup: yes
     follow: yes
-  notify: Dnsmasq | update resolvconf
+  notify: Preinstall | update resolvconf
 
 - name: Add nameservers to resolv.conf
   blockinfile:
@@ -84,7 +114,7 @@
     backup: yes
     follow: yes
     marker: ""# Ansible nameservers {mark}""
-  notify: Dnsmasq | update resolvconf
+  notify: Preinstall | update resolvconf
 
 - name: Add options to resolv.conf
   lineinfile:
@@ -99,7 +129,7 @@
     - ndots:{{ ndots }}
     - timeout:2
     - attempts:2
-  notify: Dnsmasq | update resolvconf
+  notify: Preinstall | update resolvconf
 
 - name: Remove search and nameserver options from resolvconf base
   lineinfile:
@@ -112,16 +142,16 @@
     - search
     - nameserver
   when: resolvconf.rc == 0
-  notify: Dnsmasq | update resolvconf
+  notify: Preinstall | update resolvconf
 
 - name: disable resolv.conf modification by dhclient
   copy: src=dhclient_nodnsupdate dest=/etc/dhcp/dhclient-enter-hooks.d/znodnsupdate mode=0755
-  notify: Dnsmasq | restart network
+  notify: Preinstall | restart network
   when: ansible_os_family == ""Debian""
 
 - name: disable resolv.conf modification by dhclient
   copy: src=dhclient_nodnsupdate dest=/etc/dhcp/dhclient.d/nodnsupdate mode=u+x
-  notify: Dnsmasq | restart network
+  notify: Preinstall | restart network
   when: ansible_os_family == ""RedHat""
 
 - name: get temporary resolveconf cloud init file content
@@ -135,5 +165,5 @@
     src: resolvconf.j2
     owner: root
     mode: 0644
-  notify: Dnsmasq | update resolvconf for CoreOS
+  notify: Preinstall | update resolvconf for CoreOS
   when: ansible_os_family == ""CoreOS""
",edit_var,other,notify,"複数の変更
- tags
- add_task
- edit_var: 
 Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq
  was set up and before K8s apps to be created.
- notify: 
Move DNS/resolvconf configuration to preinstall role. Remove
  skip_dnsmasq_k8s var as not needed anymore.
",set_factでの変数名をいじっている部分と、notifyの変更がまざっているもの。notifyのほうが変更量が多そうなので。
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/kubernetes/preinstall/tasks/main.yml,MODIFY,None,"@@ -177,3 +177,6 @@
 
 - include: etchosts.yml
   tags: [bootstrap-os, etchosts]
+
+- include: resolvconf.yml
+  tags: [bootstrap-os, resolvconf]
",include,tags?,add_tasks,includeファイルの追加とそれに付随するタグの追加,タグは上の同じ。別のファイルにまとめてあるタスクを読み出しているだけなので
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/kubernetes/preinstall/handlers/main.yml,RENAME,None,"@@ -1,11 +1,11 @@
-- name: Dnsmasq | restart network
+- name: Preinstall | restart network
   command: /bin/true
   notify:
-    - Dnsmasq | reload network
-    - Dnsmasq | update resolvconf
+    - Preinstall | reload network
+    - Preinstall | update resolvconf
   when: ansible_os_family != ""CoreOS""
 
-- name: Dnsmasq | reload network
+- name: Preinstall | reload network
   service:
     name: >-
       {% if ansible_os_family == ""RedHat"" -%}
@@ -16,31 +16,30 @@
     state: restarted
   when: ansible_os_family != ""RedHat"" and ansible_os_family != ""CoreOS""
 
-- name: Dnsmasq | update resolvconf
+- name: Preinstall | update resolvconf
   command: /bin/true
   notify:
-    - Dnsmasq | reload resolvconf
-    - Dnsmasq | reload kubelet
+    - Preinstall | reload resolvconf
+    - Preinstall | reload kubelet
   when: ansible_os_family != ""CoreOS""
 
-- name: Dnsmasq | update resolvconf for CoreOS
+- name: Preinstall | update resolvconf for CoreOS
   command: /bin/true
   notify:
-    - Dnsmasq | apply resolvconf cloud-init
-    - Dnsmasq | reload kubelet
+    - Preinstall | apply resolvconf cloud-init
+    - Preinstall | reload kubelet
   when: ansible_os_family == ""CoreOS""
 
-- name: Dnsmasq | reload resolvconf
+- name: Preinstall | reload resolvconf
   command: /sbin/resolvconf -u
   ignore_errors: true
 
-- name: Dnsmasq | apply resolvconf cloud-init
+- name: Preinstall | apply resolvconf cloud-init
   command: /usr/bin/coreos-cloudinit --from-file {{ resolveconf_cloud_init_conf }}
   when: ansible_os_family == ""CoreOS""
 
-- name: Dnsmasq | reload kubelet
+- name: Preinstall | reload kubelet
   service:
     name: kubelet
     state: restarted
-  when: ""{{ inventory_hostname in groups['kube-master'] }}""
-  ignore_errors: true
+  when: ""{{ inventory_hostname in groups['kube-master'] and not dns_early|bool }}""
",rename_task,rename_task,notify,DnsMmasq -> Preinstallに変更した影響でnotifyの名前も変更されている,PR見てみると、ロールを移動しているので、notify先が変わっている、というので良さそう
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/kubernetes/preinstall/defaults/main.yml,MODIFY,None,"@@ -48,3 +48,7 @@ openstack_tenant_id: ""{{ lookup('env','OS_TENANT_ID')  }}""
 
 # All clients access each node individually, instead of using a load balancer.
 etcd_multiaccess: true
+
+# CoreOS cloud init config file to define /etc/resolv.conf content
+# for hostnet pods and infra needs
+resolveconf_cloud_init_conf: /etc/resolveconf_cloud_init.conf
",move,move,edit_var,roles/dnsmasq/defaults/main.ymlからの移動,変数追加
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/network_plugin/flannel/handlers/main.yml,MODIFY,None,"@@ -1,8 +1,42 @@
 ---
-- name: delete default docker bridge
+- name: Flannel | delete default docker bridge
   command: ip link delete docker0
   ignore_errors: yes
-  notify: restart docker
+  notify: Flannel | restart docker
+
+- name: Flannel | restart docker
+  command: /bin/true
+  notify:
+    - Flannel | reload systemd
+    - Flannel | reload docker.socket
+    - Flannel | reload docker
+    - Flannel | pause while Docker restarts
+    - Flannel | wait for docker
+
+- name : Flannel | reload systemd
+  shell: systemctl daemon-reload
+  when: ansible_service_mgr == ""systemd""
+
+- name: Flannel | reload docker.socket
+  service:
+    name: docker.socket
+    state: restarted
+  when: ansible_os_family == 'CoreOS'
+
+- name: Flannel | reload docker
+  service:
+    name: docker
+    state: restarted
+
+- name: Flannel | pause while Docker restarts
+  pause: seconds=10 prompt=""Waiting for docker restart""
+
+- name: Flannel | wait for docker
+  command: /usr/bin/docker images
+  register: docker_ready
+  retries: 10
+  delay: 5
+  until: docker_ready.rc == 0
 
 - name: Flannel | reload kubelet
   service:
",add_task,add_task,add_task,restart dockerからnotifyされるタスクがいくつか追加されている,タスク追加
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",inventory/group_vars/all.yml,MODIFY,None,"@@ -33,8 +33,8 @@ kube_users:
 
 # Kubernetes cluster name, also will be used as DNS domain
 cluster_name: cluster.local
-# Subdomains of DNS domain to be resolved via /etc/resolv.conf
-ndots: 5
+# Subdomains of DNS domain to be resolved via /etc/resolv.conf for hostnet pods
+ndots: 2
 # Deploy netchecker app to verify DNS resolve as an HTTP service
 deploy_netchecker: false
 
",edit_var,edit_var,edit_var,Reduce default ndots for hosts /etc/resolv.conf to 2.,pod数を変更している
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/dnsmasq/defaults/main.yml,MODIFY,None,"@@ -11,10 +11,6 @@
 #nameservers:
 #  - 127.0.0.1
 
-# CoreOS cloud init config file to define /etc/resolv.conf content
-# for hostnet pods and infra needs
-resolveconf_cloud_init_conf: /etc/resolveconf_cloud_init.conf
-
 # Versions
 dnsmasq_version: 2.72
 
@@ -25,9 +21,6 @@ dnsmasq_image_tag: ""{{ dnsmasq_version }}""
 # Skip dnsmasq setup
 skip_dnsmasq: false
 
-# Skip setting up dnsmasq daemonset
-skip_dnsmasq_k8s: ""{{ skip_dnsmasq }}""
-
 # Limits for dnsmasq/kubedns apps
 dns_cpu_limit: 100m
 dns_memory_limit: 170Mi
",move,edit_var,move,Remove skip_dnsmasq_k8s var as not needed anymore.,16行目のファイルに移動されている
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/dnsmasq/tasks/dnsmasq.yml,DELETE,None,"@@ -1,58 +0,0 @@
----
-- name: ensure dnsmasq.d directory exists
-  file:
-    path: /etc/dnsmasq.d
-    state: directory
-
-- name: ensure dnsmasq.d-available directory exists
-  file:
-    path: /etc/dnsmasq.d-available
-    state: directory
-
-- name: Write dnsmasq configuration
-  template:
-    src: 01-kube-dns.conf.j2
-    dest: /etc/dnsmasq.d-available/01-kube-dns.conf
-    mode: 0755
-    backup: yes
-
-- name: Stat dnsmasq configuration
-  stat: path=/etc/dnsmasq.d/01-kube-dns.conf
-  register: sym
-
-- name: Move previous configuration
-  command: mv /etc/dnsmasq.d/01-kube-dns.conf /etc/dnsmasq.d-available/01-kube-dns.conf.bak
-  changed_when: False
-  when: sym.stat.islnk is defined and sym.stat.islnk == False
-
-- name: Enable dnsmasq configuration
-  file:
-    src: /etc/dnsmasq.d-available/01-kube-dns.conf
-    dest: /etc/dnsmasq.d/01-kube-dns.conf
-    state: link
-
-- name: Create dnsmasq manifests
-  template: src={{item.file}} dest=/etc/kubernetes/{{item.file}}
-  with_items:
-    - {file: dnsmasq-ds.yml, type: ds}
-    - {file: dnsmasq-svc.yml, type: svc}
-  register: manifests
-  when: inventory_hostname == groups['kube-master'][0]
-
-- name: Start Resources
-  kube:
-    name: dnsmasq
-    namespace: kube-system
-    kubectl: ""{{bin_dir}}/kubectl""
-    resource: ""{{item.item.type}}""
-    filename: /etc/kubernetes/{{item.item.file}}
-    state: ""{{item.changed | ternary('latest','present') }}""
-  with_items: ""{{ manifests.results }}""
-  when: inventory_hostname == groups['kube-master'][0]
-
-- name: Check for dnsmasq port (pulling image and running container)
-  wait_for:
-    host: ""{{dns_server}}""
-    port: 53
-    delay: 5
-  when: inventory_hostname == groups['kube-node'][0]
",move,move,move,move to roles/dnsmasq/tasks/main.yml,23行目のファイルに移動
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/etcd/meta/main.yml,MODIFY,None,"@@ -3,8 +3,6 @@ dependencies:
   - role: adduser
     user: ""{{ addusers.etcd }}""
     when: ansible_os_family != 'CoreOS'
-  - role: docker
-    when: (ansible_os_family != ""CoreOS"" and etcd_deployment_type == ""docker"" or inventory_hostname in groups['k8s-cluster'])
   - role: download
     file: ""{{ downloads.etcd }}""
     tags: download
",move,role,move,roleが削除される,24行目のclusterに移動されている
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/docker/handlers/main.yml,MODIFY,None,"@@ -12,17 +12,17 @@
   shell: systemctl daemon-reload
   when: ansible_service_mgr == ""systemd""
 
-- name: Docker | reload docker
-  service:
-    name: docker
-    state: restarted
-
 - name: Docker | reload docker.socket
   service:
     name: docker.socket
     state: restarted
   when: ansible_os_family == 'CoreOS'
 
+- name: Docker | reload docker
+  service:
+    name: docker
+    state: restarted
+
 - name: Docker | pause while Docker restarts
   pause: seconds=10 prompt=""Waiting for docker restart""
 
",move,move,move,Docker | reload docker.socketの下に移動,同じファイル内での順序の移動
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",roles/dnsmasq/tasks/main.yml,MODIFY,None,"@@ -1,7 +1,61 @@
 ---
-- include: dnsmasq.yml
-  when: ""{{ not skip_dnsmasq_k8s|bool }}""
-  tags: dnsmasq
+- name: ensure dnsmasq.d directory exists
+  file:
+    path: /etc/dnsmasq.d
+    state: directory
+  tags: bootstrap-os
 
-- include: resolvconf.yml
-  tags: resolvconf
+- name: ensure dnsmasq.d-available directory exists
+  file:
+    path: /etc/dnsmasq.d-available
+    state: directory
+  tags: bootstrap-os
+
+- name: Write dnsmasq configuration
+  template:
+    src: 01-kube-dns.conf.j2
+    dest: /etc/dnsmasq.d-available/01-kube-dns.conf
+    mode: 0755
+    backup: yes
+
+- name: Stat dnsmasq configuration
+  stat: path=/etc/dnsmasq.d/01-kube-dns.conf
+  register: sym
+
+- name: Move previous configuration
+  command: mv /etc/dnsmasq.d/01-kube-dns.conf /etc/dnsmasq.d-available/01-kube-dns.conf.bak
+  changed_when: False
+  when: sym.stat.islnk is defined and sym.stat.islnk == False
+
+- name: Enable dnsmasq configuration
+  file:
+    src: /etc/dnsmasq.d-available/01-kube-dns.conf
+    dest: /etc/dnsmasq.d/01-kube-dns.conf
+    state: link
+
+- name: Create dnsmasq manifests
+  template: src={{item.file}} dest=/etc/kubernetes/{{item.file}}
+  with_items:
+    - {file: dnsmasq-ds.yml, type: ds}
+    - {file: dnsmasq-svc.yml, type: svc}
+  register: manifests
+  when: inventory_hostname == groups['kube-master'][0]
+
+- name: Start Resources
+  kube:
+    name: dnsmasq
+    namespace: kube-system
+    kubectl: ""{{bin_dir}}/kubectl""
+    resource: ""{{item.item.type}}""
+    filename: /etc/kubernetes/{{item.item.file}}
+    state: ""{{item.changed | ternary('latest','present') }}""
+  with_items: ""{{ manifests.results }}""
+  when: inventory_hostname == groups['kube-master'][0]
+
+- name: Check for dnsmasq port (pulling image and running container)
+  wait_for:
+    host: ""{{dns_server}}""
+    port: 53
+    delay: 5
+  when: inventory_hostname == groups['kube-node'][0]
+  tags: facts
",move,move,move,roles/dnsmasq/tasks/dnsmasq.ymlから移動,20行目のファイルから移動されている
a15d626771871613f98f2bc2c45156ae6c9caece,2016-12-09 16:30:55+00:00,"Preconfigure DNS stack and docker early In order to enable offline/intranet installation cases: * Move DNS/resolvconf configuration to preinstall role. Remove skip_dnsmasq_k8s var as not needed anymore. * Preconfigure DNS stack early, which may be the case when downloading artifacts from intranet repositories. Do not configure K8s DNS resolvers for hosts /etc/resolv.conf yet early (as they may be not existing). * Reconfigure K8s DNS resolvers for hosts only after kubedns/dnsmasq was set up and before K8s apps to be created. * Move docker install task to early stage as well and unbind it from the etcd role's specific install path. Fix external flannel dependency on docker role handlers. Also fix the docker restart handlers' steps ordering to match the expected sequence (the socket then the service). * Add default resolver fact, which is the cloud provider specific and remove hardcoded GCE resolver. * Reduce default ndots for hosts /etc/resolv.conf to 2. Multiple search domains combined with high ndots values lead to poor performance of DNS stack and make ansible workers to fail very often with the ""Timeout (12s) waiting for privilege escalation prompt:"" error. * Update docs. Signed-off-by: Bogdan Dobrelya <bdobrelia@mirantis.com>",cluster.yml,MODIFY,None,"@@ -16,6 +16,7 @@
   any_errors_fatal: true
   roles:
     - { role: kubernetes/preinstall, tags: preinstall }
+    - { role: docker, tags: docker }
 
 - hosts: etcd:!k8s-cluster
   any_errors_fatal: true
@@ -40,6 +41,7 @@
   any_errors_fatal: true
   roles:
     - { role: dnsmasq, tags: dnsmasq }
+    - { role: kubernetes/preinstall, tags: resolvconf }
 
 - hosts: kube-master[0]
   any_errors_fatal: true
",role,role,move,実行するロールを追加する,21行目のファイルとの移動に対応。role: kubernetes/preinstallは追加っぽいけど。
1f9f8853793db50699c676ad0821fd0b101d7e85,2016-12-30 09:55:26+00:00,"Fix etcd cert generation to support large deployments Due to bash max args limits, we should pass all node filenames and base64-encoded tar data through stdin/stdout instead. Fixes #832",roles/etcd/tasks/gen_certs.yml,MODIFY,832,"@@ -73,7 +73,9 @@
   tags: facts
 
 - name: Gen_certs | Gather etcd master certs
-  shell: ""tar cfz - -C {{ etcd_cert_dir }} {{ my_master_certs|join(' ') }} {{ all_node_certs|join(' ') }}| base64 --wrap=0""
+  shell: ""tar cfz - -C {{ etcd_cert_dir }} -T /dev/stdin <<< {{ my_master_certs|join(' ') }} {{ all_node_certs|join(' ') }} | base64 --wrap=0""
+  args:
+    executable: /bin/bash
   register: etcd_master_cert_data
   delegate_to: ""{{groups['etcd'][0]}}""
   when: inventory_hostname in groups['etcd'] and sync_certs|default(false) and
@@ -81,7 +83,9 @@
   notify: set etcd_secret_changed
 
 - name: Gen_certs | Gather etcd node certs
-  shell: ""tar cfz - -C {{ etcd_cert_dir }} {{ my_node_certs|join(' ') }} | base64 --wrap=0""
+  shell: ""tar cfz - -C {{ etcd_cert_dir }} -T /dev/stdin <<< {{ my_node_certs|join(' ') }} | base64 --wrap=0""
+  args:
+    executable: /bin/bash
   register: etcd_node_cert_data
   delegate_to: ""{{groups['etcd'][0]}}""
   when: inventory_hostname in groups['k8s-cluster'] and sync_certs|default(false) and
@@ -89,13 +93,17 @@
   notify: set etcd_secret_changed
 
 - name: Gen_certs | Copy certs on masters
-  shell: ""echo '{{etcd_master_cert_data.stdout|quote}}' | base64 -d | tar xz -C {{ etcd_cert_dir }}""
+  shell: ""base64 -d <<< '{{etcd_master_cert_data.stdout|quote}}' | tar xz -C {{ etcd_cert_dir }}""
+  args:
+    executable: /bin/bash
   changed_when: false
   when: inventory_hostname in groups['etcd'] and sync_certs|default(false) and
         inventory_hostname != groups['etcd'][0]
 
 - name: Gen_certs | Copy certs on nodes
-  shell: ""echo '{{etcd_node_cert_data.stdout|quote}}' | base64 -d | tar xz -C {{ etcd_cert_dir }}""
+  shell: ""base64 -d <<< '{{etcd_node_cert_data.stdout|quote}}' | tar xz -C {{ etcd_cert_dir }}""
+  args:
+    executable: /bin/bash
   changed_when: false
   when: sync_certs|default(false) and
         inventory_hostname not in groups['etcd']
",command,command,command,コマンドの内容が変更されている,シェルの内容の変更
e5fdc63bdd28eb479fe46dafe72a4d7dceb8b416,2017-01-25 22:09:21+00:00,Bugfix: skip cloud_config on etcd,roles/kubernetes/preinstall/tasks/main.yml,MODIFY,None,"@@ -156,7 +156,7 @@
     dest: ""{{ kube_config_dir }}/cloud_config""
     group: ""{{ kube_cert_group }}""
     mode: 0640
-  when: cloud_provider is defined and cloud_provider == ""openstack""
+  when: inventory_hostname in groups['k8s-cluster'] and cloud_provider is defined and cloud_provider == ""openstack""
   tags: [cloud-provider, openstack]
 
 - name: Write azure cloud-config
@@ -165,7 +165,7 @@
     dest: ""{{ kube_config_dir }}/cloud_config""
     group: ""{{ kube_cert_group }}""
     mode: 0640
-  when: cloud_provider is defined and cloud_provider == ""azure""
+  when: inventory_hostname in groups['k8s-cluster'] and cloud_provider is defined and cloud_provider == ""azure""
   tags: [cloud-provider, azure]
 
 - include: etchosts.yml
",condition,condition,condition,inventory_hostname in groups['k8s-cluster']が追加される,whenの変更
585afef945a2a996b77bbb23ccc427f944750572,2017-02-02 13:38:11+00:00,"Remove nsenter workaround - Docker 1.12 and further don't need nsenter hack. This patch removes it. Also, it bumps the minimal version to 1.12. Closes #776 Signed-off-by: Sergii Golovatiuk <sgolovatiuk@mirantis.com>",roles/kubernetes/secrets/tasks/gen_certs.yml,MODIFY,776,"@@ -160,6 +160,20 @@
       {%- endif %}
   tags: facts
 
+- name: SSL CA directories | Set SSL CA directories
+  set_fact:
+    ssl_ca_dirs: ""[
+      {% if ansible_os_family in ['CoreOS', 'Container Linux by CoreOS'] -%}
+      '/usr/share/ca-certificates',
+      {% elif ansible_os_family == 'RedHat' -%}
+      '/etc/pki/tls',
+      '/etc/pki/ca-trust',
+      {% elif ansible_os_family == 'Debian' -%}
+      '/usr/share/ca-certificates',
+      {% endif -%}
+    ]""
+  tags: facts
+
 - name: Gen_certs | add CA to trusted CA dir
   copy:
     src: ""{{ kube_cert_dir }}/ca.pem""
",edit_var,add_task,edit_var,,テンプレートで分岐しているけど、これは変数の定義になると思う
a098a32f7d6d60aa37fd7c80e4c3f543e19bfba9,2017-02-24 11:25:45+00:00,Uncomment one key/value in all.yml all.yaml shouldn't be empty otherwise ansible won't be able to merge 2 dicts. Related bug: ansible/issues/21889,inventory/group_vars/all.yml,MODIFY,None,"@@ -1,4 +1,3 @@
-
 ## The access_ip variable is used to define how other nodes should access
 ## the node.  This is used in flannel to allow other flannel nodes to see
 ## this node for example.  The access_ip is really useful AWS and Google
@@ -32,7 +31,7 @@
 ## modules.
 # kubelet_load_modules: false
 
-## Internal network total size. This is the prefix of the                                                                         
+## Internal network total size. This is the prefix of the
 ## entire network. Must be unused in your environment.
 #kube_network_prefix: 18
 
@@ -79,7 +78,6 @@
 #kpm_packages:
 #  - name: kube-system/grafana
 
-
 ## Certificate Management
 ## This setting determines whether certs are generated via scripts or whether a
 ## cluster of Hashicorp's Vault is started to issue certificates (using etcd
@@ -87,4 +85,4 @@
 #cert_management: script
 
 ## Please specify true if you want to perform a kernel upgrade
-#kernel_upgrade: false
+kernel_upgrade: false
",edit_var,edit_var,edit_var,kernel_upgrade: falseがコメントアウトから外れている,kernel_upgradeのコメントアウトを外しているだけ
3e457e4edf38b2817f330f2df90b70dc91df6aa7,2017-07-26 16:09:34+00:00,Enable weave seed mode for kubespray (#1414) * Enable weave seed mode for kubespray * fix task Weave seed | Set peers if existing peers * fix mac address variabilisation * fix default values * fix include seed condition * change weave var to default values * fix Set peers if existing peers,roles/uploads/defaults/main.yml,MODIFY,1414,"@@ -5,7 +5,7 @@ local_release_dir: /tmp
 etcd_version: v3.0.17
 calico_version: v0.23.0
 calico_cni_version: v1.5.6
-weave_version: v1.8.2
+weave_version: v2.0.1
 
 # Download URL's
 etcd_download_url: ""https://github.com/coreos/etcd/releases/download/{{ etcd_version }}/etcd-{{ etcd_version }}-linux-amd64.tar.gz""
",dependency,dependency,dependency,"コミットメッセージから
change weave var to default values",versionの変更
3e457e4edf38b2817f330f2df90b70dc91df6aa7,2017-07-26 16:09:34+00:00,Enable weave seed mode for kubespray (#1414) * Enable weave seed mode for kubespray * fix task Weave seed | Set peers if existing peers * fix mac address variabilisation * fix default values * fix include seed condition * change weave var to default values * fix Set peers if existing peers,roles/network_plugin/weave/tasks/main.yml,MODIFY,1414,"@@ -1,6 +1,9 @@
 ---
 - include: pre-upgrade.yml
 
+- include: seed.yml
+  when: weave_mode_seed
+
 - name: Weave | enable br_netfilter module
   modprobe:
     name: br_netfilter
",include,other,add_task,ファイルのインクルードを行う,seed.ymlに追加されたタスクを読み込んでいるだけ。seed.ymlはこのコミットで追加されている
3e457e4edf38b2817f330f2df90b70dc91df6aa7,2017-07-26 16:09:34+00:00,Enable weave seed mode for kubespray (#1414) * Enable weave seed mode for kubespray * fix task Weave seed | Set peers if existing peers * fix mac address variabilisation * fix default values * fix include seed condition * change weave var to default values * fix Set peers if existing peers,roles/network_plugin/weave/defaults/main.yml,MODIFY,1414,"@@ -4,3 +4,13 @@ weave_memory_limit: 400M
 weave_cpu_limit: 30m
 weave_memory_requests: 64M
 weave_cpu_requests: 10m
+
+# This two variable are automatically changed by the weave's role, do not manually change these values
+# To reset values :
+# weave_seed: unset
+# weave_peers: unset
+weave_seed: uninitialized
+weave_peers: uninitialized
+
+# this variable is use in seed mode
+weave_ip_current_cluster: ""{% for host in groups['k8s-cluster'] %}{{ hostvars[host]['ip'] | default(hostvars[host]['ansible_default_ipv4']['address']) }}{% if not loop.last %} {% endif %}{% endfor %}""
\ No newline at end of file
",edit_var,edit_var,edit_var,defaultsに含まれる変数がいくつか追加されている,defaultの変数の追加。実際の値は、33行目で設定している。
3e457e4edf38b2817f330f2df90b70dc91df6aa7,2017-07-26 16:09:34+00:00,Enable weave seed mode for kubespray (#1414) * Enable weave seed mode for kubespray * fix task Weave seed | Set peers if existing peers * fix mac address variabilisation * fix default values * fix include seed condition * change weave var to default values * fix Set peers if existing peers,roles/download/defaults/main.yml,MODIFY,1414,"@@ -25,7 +25,7 @@ etcd_version: v3.0.17
 calico_version: ""v1.1.3""
 calico_cni_version: ""v1.7.0""
 calico_policy_version: ""v0.5.4""
-weave_version: 1.8.2
+weave_version: 2.0.1
 flannel_version: v0.6.2
 pod_infra_version: 3.0
 
",dependency,dependency,dependency,"コミットメッセージから
change weave var to default values",weaveのバージョン変更
3e457e4edf38b2817f330f2df90b70dc91df6aa7,2017-07-26 16:09:34+00:00,Enable weave seed mode for kubespray (#1414) * Enable weave seed mode for kubespray * fix task Weave seed | Set peers if existing peers * fix mac address variabilisation * fix default values * fix include seed condition * change weave var to default values * fix Set peers if existing peers,inventory/group_vars/k8s-cluster.yml,MODIFY,1414,"@@ -74,6 +74,22 @@ kube_users:
 # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing
 kube_network_plugin: calico
 
+# weave's network password for encryption
+# if null then no network encryption
+# you can use --extra-vars to pass the password in command line
+weave_password: EnterPasswordHere
+
+# Weave uses consensus mode by default
+# Enabling seed mode allow to dynamically add or remove hosts
+# https://www.weave.works/docs/net/latest/ipam/
+weave_mode_seed: false
+
+# This two variable are automatically changed by the weave's role, do not manually change these values
+# To reset values :
+# weave_seed: uninitialized
+# weave_peers: uninitialized
+weave_seed: uninitialized
+weave_peers: uninitialized
 
 # Enable kubernetes network policies
 enable_network_policy: false
@@ -136,8 +152,3 @@ efk_enabled: false
 
 # Helm deployment
 helm_enabled: false
-
-# dnsmasq
-# dnsmasq_upstream_dns_servers:
-#  - /resolvethiszone.with/10.0.4.250
-#  - 8.8.8.8
",edit_var,edit_var,edit_var,,変数の追加。31行目で設定したdefault値の上書き
3e457e4edf38b2817f330f2df90b70dc91df6aa7,2017-07-26 16:09:34+00:00,Enable weave seed mode for kubespray (#1414) * Enable weave seed mode for kubespray * fix task Weave seed | Set peers if existing peers * fix mac address variabilisation * fix default values * fix include seed condition * change weave var to default values * fix Set peers if existing peers,roles/network_plugin/weave/tasks/seed.yml,ADD,1414,"@@ -0,0 +1,50 @@
+---
+- name: Weave seed | Set seed if first time
+  set_fact:
+    seed: '{% for host in groups[""k8s-cluster""] %}{{ hostvars[host][""ansible_default_ipv4""][""macaddress""] }}{% if not loop.last %},{% endif %}{% endfor %}'
+  when: ""weave_seed == 'uninitialized'""
+  run_once: true
+  tags: confweave
+
+- name: Weave seed | Set seed if not first time
+  set_fact:
+    seed: '{{ weave_seed }}'
+  when: ""weave_seed != 'uninitialized'""
+  run_once: true
+  tags: confweave
+
+- name: Weave seed | Set peers if fist time
+  set_fact:
+    peers: '{{ weave_ip_current_cluster }}'
+  when: ""weave_peers == 'uninitialized'""
+  run_once: true
+  tags: confweave
+
+- name: Weave seed | Set peers if existing peers
+  set_fact:
+    peers: '{{ weave_peers }}{% for ip in weave_ip_current_cluster.split("" "") %}{% if ip not in weave_peers.split("" "") %} {{ ip }}{% endif %}{% endfor %}'
+  when: ""weave_peers != 'uninitialized'""
+  run_once: true
+  tags: confweave
+
+- name: Weave seed | Save seed
+  lineinfile:
+    dest: ""./inventory/group_vars/k8s-cluster.yml""
+    state: present
+    regexp: '^weave_seed:'
+    line: 'weave_seed: {{ seed }}'
+  become: no
+  delegate_to: 127.0.0.1
+  run_once: true
+  tags: confweave
+
+- name: Weave seed | Save peers
+  lineinfile:
+    dest: ""./inventory/group_vars/k8s-cluster.yml""
+    state: present
+    regexp: '^weave_peers:'
+    line: 'weave_peers: {{ peers }}'
+  become: no
+  delegate_to: 127.0.0.1
+  run_once: true
+  tags: confweave
\ No newline at end of file
",add_task,add_task,add_task,タスクが追加されている,30行目のincludeと対応。タスク追加している
5efda3eda900eedf584bd1f5c5d68d7afddc14f7,2017-08-09 22:49:53+00:00,"Configurable docker yum repos, systemd fix * Make yum repos used for installing docker rpms configurable * TasksMax is only supported in systemd version >= 226 * Change to systemd file should restart docker",roles/docker/defaults/main.yml,MODIFY,None,"@@ -10,3 +10,6 @@ docker_repo_info:
   repos:
 
 docker_dns_servers_strict: yes
+
+docker_rh_repo_base_url: 'https://yum.dockerproject.org/repo/main/centos/7'
+docker_rh_repo_gpgkey: 'https://yum.dockerproject.org/gpg'
",move,edit_var,move,dockerのダウンロード先のURLとgpg鍵のURLを追加している,PR見てみると、templateの中身から移動されてきたっぽい。
5efda3eda900eedf584bd1f5c5d68d7afddc14f7,2017-08-09 22:49:53+00:00,"Configurable docker yum repos, systemd fix * Make yum repos used for installing docker rpms configurable * TasksMax is only supported in systemd version >= 226 * Change to systemd file should restart docker",roles/docker/tasks/systemd.yml,MODIFY,None,"@@ -10,11 +10,17 @@
     dest: /etc/systemd/system/docker.service.d/http-proxy.conf
   when: http_proxy is defined or https_proxy is defined or no_proxy is defined
 
+- name: get systemd version
+  command: rpm -q --qf '%{V}\n' systemd
+  register: systemd_version
+  when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS""] or is_atomic)
+
 - name: Write docker.service systemd file
   template:
     src: docker.service.j2
     dest: /etc/systemd/system/docker.service
   register: docker_service_file
+  notify: restart docker
   when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS""] or is_atomic)
 
 - name: Write docker.service systemd file for atomic
",add_task,add_task,add_task,,タスク追加
2645e88b0c1ae789ed212a68560ed7bf7fd93eac,2017-08-18 12:09:45+00:00,Fix vault setup partially (#1531) This does not address per-node certs and scheduler/proxy/controller-manager component certs which are now required. This should be handled in a follow-up patch.,roles/vault/tasks/shared/issue_cert.yml,MODIFY,1531,"@@ -18,6 +18,11 @@
 #   issue_cert_role:        The Vault role to issue the cert with
 #   issue_cert_url:         Url to reach Vault, including protocol and port
 
+- name: issue_cert | debug who issues certs
+  debug:
+    msg: ""{{ issue_cert_hosts }} issues certs""
+
+
 - name: issue_cert | Ensure target directory exists
   file:
     path: ""{{ issue_cert_path | dirname }}"" 
@@ -38,11 +43,16 @@
       format: ""{{ issue_cert_format | d('pem') }}""
       ip_sans: ""{{ issue_cert_ip_sans | default([]) | join(',') }}""
   register: issue_cert_result
-  when: inventory_hostname == issue_cert_hosts|first
+  delegate_to: ""{{ issue_cert_hosts|first }}""
+
+- name: issue_cert | results
+  debug:
+    msg: ""{{ issue_cert_result }}""
+
 
 - name: ""issue_cert | Copy {{ issue_cert_path }} cert to all hosts""
   copy:
-    content: ""{{ hostvars[issue_cert_hosts|first]['issue_cert_result']['json']['data']['certificate'] }}""
+    content: ""{{ issue_cert_result['json']['data']['certificate'] }}""
     dest: ""{{ issue_cert_path }}""
     group: ""{{ issue_cert_file_group | d('root' )}}""
     mode: ""{{ issue_cert_file_mode | d('0644') }}""
@@ -50,7 +60,7 @@
 
 - name: ""issue_cert | Copy key for {{ issue_cert_path }} to all hosts""
   copy:
-    content: ""{{ hostvars[issue_cert_hosts|first]['issue_cert_result']['json']['data']['private_key'] }}""
+    content: ""{{ issue_cert_result['json']['data']['private_key'] }}""
     dest: ""{{ issue_cert_path.rsplit('.', 1)|first }}-key.{{ issue_cert_path.rsplit('.', 1)|last }}""
     group: ""{{ issue_cert_file_group | d('root' )}}""
     mode: ""{{ issue_cert_file_mode | d('0640') }}""
@@ -58,7 +68,7 @@
 
 - name: issue_cert | Copy issuing CA cert
   copy:
-    content: ""{{ hostvars[issue_cert_hosts|first]['issue_cert_result']['json']['data']['issuing_ca'] }}""
+    content: ""{{ issue_cert_result['json']['data']['issuing_ca'] }}""
     dest: ""{{ issue_cert_path | dirname }}/ca.pem""
     group: ""{{ issue_cert_file_group | d('root' )}}""
     mode: ""{{ issue_cert_file_mode | d('0644') }}""
",edit_args,edit_args,edit_args,,使いたい変数の中身のアクセス方法が違うので。debugタスクの追加よりも、こっちが本質的だと思う。
2645e88b0c1ae789ed212a68560ed7bf7fd93eac,2017-08-18 12:09:45+00:00,Fix vault setup partially (#1531) This does not address per-node certs and scheduler/proxy/controller-manager component certs which are now required. This should be handled in a follow-up patch.,roles/vault/tasks/cluster/unseal.yml,MODIFY,1531,"@@ -1,5 +1,8 @@
 ---
 
+- name: cluster/unseal | Current sealed state
+  debug: "" Sealed? {{vault_is_sealed}}""
+
 - name: cluster/unseal | Unseal Vault
   uri:
     url: ""https://localhost:{{ vault_port }}/v1/sys/unseal""
",add_task,add_task,add_task,,debugタスクの追加
2645e88b0c1ae789ed212a68560ed7bf7fd93eac,2017-08-18 12:09:45+00:00,Fix vault setup partially (#1531) This does not address per-node certs and scheduler/proxy/controller-manager component certs which are now required. This should be handled in a follow-up patch.,roles/vault/tasks/shared/check_etcd.yml,MODIFY,1531,"@@ -1,9 +1,14 @@
 ---
 
-- name: check_etcd | Check if etcd is up an reachable
+- name: check_etcd | Check if etcd is up and reachable
   uri:
     url: ""{{ vault_etcd_url }}/health""
     validate_certs: no
+  until: vault_etcd_health_check.status == 200 or vault_etcd_health_check.status == 401
+  retries: 10
+  delay: 2
+  delegate_to: ""{{groups['etcd'][0]}}""
+  run_once: true
   failed_when: false
   register: vault_etcd_health_check
 
",directive,directive,directive,"until, retries等のdirectiveの追加",いっぱいdirective
2645e88b0c1ae789ed212a68560ed7bf7fd93eac,2017-08-18 12:09:45+00:00,Fix vault setup partially (#1531) This does not address per-node certs and scheduler/proxy/controller-manager component certs which are now required. This should be handled in a follow-up patch.,roles/vault/tasks/shared/check_vault.yml,MODIFY,1531,"@@ -12,7 +12,7 @@
   uri:
     url: ""{{ vault_config.listener.tcp.tls_disable|d()|ternary('http', 'https') }}://localhost:{{ vault_port }}/v1/sys/health""
     headers: ""{{ vault_client_headers }}""
-    status_code: 200,429,500,501
+    status_code: 200,429,500,501,503
     validate_certs: no
   ignore_errors: true
   register: vault_local_service_health
",edit_args,edit_args,edit_args,status_codeの追加,status_codeとして、有効なものの一覧に503を追加
2645e88b0c1ae789ed212a68560ed7bf7fd93eac,2017-08-18 12:09:45+00:00,Fix vault setup partially (#1531) This does not address per-node certs and scheduler/proxy/controller-manager component certs which are now required. This should be handled in a follow-up patch.,roles/vault/tasks/shared/find_leader.yml,MODIFY,1531,"@@ -5,7 +5,7 @@
     url: ""{{ vault_config.listener.tcp.tls_disable|d()|ternary('http', 'https') }}://localhost:{{ vault_port }}/v1/sys/health""
     headers: ""{{ hostvars[groups.vault|first]['vault_headers'] }}""
     method: HEAD
-    status_code: 200,429
+    status_code: 200,429,503
   register: vault_leader_check
   until: ""vault_leader_check|succeeded""
   retries: 10
@@ -14,7 +14,8 @@
   set_fact:
     vault_leader_url: ""{{ vault_config.listener.tcp.tls_disable|d()|ternary('http', 'https') }}://{{ item }}:{{ vault_port }}""
   with_items: ""{{ groups.vault }}""
-  when: ""hostvars[item]['vault_leader_check'].get('status') == 200""
-  run_once: true
+  when: ""hostvars[item]['vault_leader_check'].get('status') in [200,503]""
+  #run_once: true
 
-- debug: var=vault_leader_url verbosity=2
+- name: find_leader| show vault_leader_url
+  debug: var=vault_leader_url verbosity=2
",condition,edit_args,condition,status_codeの追加,名前の変更と、status_codeの追加、条件の追加って感じ。status_codeも条件、とみれば、conditionが良さそう
2645e88b0c1ae789ed212a68560ed7bf7fd93eac,2017-08-18 12:09:45+00:00,Fix vault setup partially (#1531) This does not address per-node certs and scheduler/proxy/controller-manager component certs which are now required. This should be handled in a follow-up patch.,roles/etcd/tasks/gen_certs_vault.yml,MODIFY,1531,"@@ -11,12 +11,12 @@
 - name: gen_certs_vault | Read in the local credentials
   command: cat /etc/vault/roles/etcd/userpass
   register: etcd_vault_creds_cat
-  when: inventory_hostname == groups.etcd|first
+  delegate_to: ""{{ groups['vault'][0] }}""
 
 - name: gen_certs_vault | Set facts for read Vault Creds
   set_fact:
-    etcd_vault_creds: ""{{ hostvars[groups.etcd|first]['etcd_vault_creds_cat']['stdout']|from_json }}""
-  when: inventory_hostname == groups.etcd|first
+    etcd_vault_creds: ""{{ etcd_vault_creds_cat.stdout|from_json }}""
+  delegate_to: ""{{ groups['vault'][0] }}""
 
 - name: gen_certs_vault | Log into Vault and obtain an token
   uri:
@@ -29,12 +29,12 @@
     body:
       password: ""{{ etcd_vault_creds.password }}""
   register: etcd_vault_login_result
-  when: inventory_hostname == groups.etcd|first
+  delegate_to: ""{{ groups['vault'][0] }}""
 
 - name: gen_certs_vault | Set fact for vault_client_token
   set_fact:
     vault_client_token:  ""{{ etcd_vault_login_result.get('json', {}).get('auth', {}).get('client_token') }}""
-  delegate_to: ""{{ groups['etcd'][0] }}""
+  run_once: true
 
 - name: gen_certs_vault | Set fact for Vault API token
   set_fact:
@@ -42,6 +42,7 @@
         Accept: application/json
         Content-Type: application/json
         X-Vault-Token: ""{{ vault_client_token }}""
+  run_once: true
   when: vault_client_token != """"
 
 # Issue master certs to Etcd nodes
",directive,directive,directive,"登録される値は[etcd_vault_creds_cat][stdout] -> etcd_vault_creds_cat.stdout なので変更の意味は薄い
whenやdelegate_toディレクティブの変更が当てられているため
",whenでホストを指定していた変わりに、delegete_toを使うようになった。他のディレクティブを使うようになったと見て。
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/shared/gen_ca.yml,MODIFY,None,"@@ -8,10 +8,10 @@
 - name: ""bootstrap/gen_ca | Generate {{ gen_ca_mount_path }} root CA""
   uri:
     url: ""{{ vault_leader_url }}/v1/{{ gen_ca_mount_path }}/root/generate/exported""
-    headers: ""{{ vault_headers }}""
+    headers: ""{{ gen_ca_vault_headers }}""
     method: POST
     body_format: json
-    body: ""{{ vault_ca_options }}""
+    body: ""{{ gen_ca_vault_options }}""
   register: vault_ca_gen
   delegate_to: ""{{ groups.vault|first }}""
   run_once: true
",edit_args,edit_args,edit_args,展開する変数名が異なるだけですが,引数が変わっている
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/shared/issue_cert.yml,MODIFY,None,"@@ -11,7 +11,6 @@
 #   issue_cert_file_mode:   Mode of the placed cert file
 #   issue_cert_file_owner:  Owner of the placed cert file and directory
 #   issue_cert_format:      Format for returned data. Can be pem, der, or pem_bundle
-#   issue_cert_headers:     Headers passed into the issue request
 #   issue_cert_hosts:       List of hosts to distribute the cert to
 #   issue_cert_ip_sans:     Requested IP Subject Alternative Names, in a list
 #   issue_cert_mount_path:  Mount point in Vault to make the request to
@@ -27,7 +26,47 @@
     mode: ""{{ issue_cert_dir_mode | d('0755') }}""
     owner: ""{{ issue_cert_file_owner | d('root') }}""
 
-- name: ""issue_cert | Generate the cert for {{ issue_cert_role }}""
+- name: ""issue_cert | Read in the local credentials""
+  command: cat {{ vault_roles_dir }}/{{ issue_cert_role }}/userpass
+  register: vault_creds_cat
+  delegate_to: ""{{ issue_cert_hosts|first }}""
+  run_once: true
+
+- name: gen_certs_vault | Set facts for read Vault Creds
+  set_fact:
+    user_vault_creds: ""{{ vault_creds_cat.stdout|from_json }}""
+  delegate_to: ""{{ issue_cert_hosts|first }}""
+  run_once: true
+
+- name: gen_certs_vault | Log into Vault and obtain an token
+  uri:
+    url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}/v1/auth/userpass/login/{{ user_vault_creds.username }}""
+    headers:
+      Accept: application/json
+      Content-Type: application/json
+    method: POST
+    body_format: json
+    body:
+      password: ""{{ user_vault_creds.password }}""
+  register: vault_login_result
+  delegate_to: ""{{ issue_cert_hosts|first }}""
+  run_once: true
+
+- name: gen_certs_vault | Set fact for vault_client_token
+  set_fact:
+    vault_client_token: ""{{ vault_login_result.get('json', {}).get('auth', {}).get('client_token') }}""
+  run_once: true
+
+- name: gen_certs_vault | Set fact for Vault API token
+  set_fact:
+    issue_cert_headers:
+      Accept: application/json
+      Content-Type: application/json
+      X-Vault-Token: ""{{ vault_client_token }}""
+  run_once: true
+  when: vault_client_token != """"
+
+- name: ""issue_cert | Generate {{ issue_cert_path }} for {{ issue_cert_role }} role""
   uri:
     url: ""{{ issue_cert_url }}/v1/{{ issue_cert_mount_path|d('pki') }}/issue/{{ issue_cert_role }}""
     headers: ""{{ issue_cert_headers }}""
@@ -70,7 +109,7 @@
 - name: issue_cert | Copy certificate serial to all hosts
   copy:
     content: ""{{ hostvars[issue_cert_hosts|first]['issue_cert_result']['json']['data']['serial_number'] }}""
-    dest: ""{{ issue_cert_path.rsplit('.', 1)|first }}.serial }}""
+    dest: ""{{ issue_cert_path.rsplit('.', 1)|first }}.serial""
     group: ""{{ issue_cert_file_group | d('root' )}}""
     mode: ""{{ issue_cert_file_mode | d('0640') }}""
     owner: ""{{ issue_cert_file_owner | d('root') }}""
",move,add_task,move,,51行目、54行目のファイルから移動されている
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/shared/gen_userpass.yml,MODIFY,None,"@@ -1,5 +1,4 @@
 ---
-
 - name: shared/gen_userpass | Create the Username/Password combo for the role
   uri:
     url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}/v1/auth/userpass/users/{{ gen_userpass_username }}""
",no_effect,other,other,行の削除,余計なハイフンの取り消し
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/shared/create_role.yml,MODIFY,None,"@@ -1,5 +1,4 @@
 ---
-
 # The JSON inside JSON here is intentional (Vault API wants it)
 - name: create_role | Create a policy for the new role allowing issuing
   uri:
@@ -22,7 +21,7 @@
     status_code: 204
   when: inventory_hostname == groups[create_role_group]|first
 
-- name: create_role | Create the new role in the {{ create_role_mount_path }} pki mount
+- name: create_role | Create {{ create_role_name }} role in the {{ create_role_mount_path }} pki mount
   uri:
     url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}/v1/{{ create_role_mount_path }}/roles/{{ create_role_name }}""
     headers: ""{{ hostvars[groups.vault|first]['vault_headers'] }}""
@@ -42,7 +41,7 @@
 - include: gen_userpass.yml
   vars:
     gen_userpass_group: ""{{ create_role_group }}""
-    gen_userpass_password: ""{{ create_role_password|d(''|to_uuid) }}""
+    gen_userpass_password: ""{{ create_role_password }}""
     gen_userpass_policies: ""{{ create_role_name }}""
     gen_userpass_role: ""{{ create_role_name }}""
     gen_userpass_username: ""{{ create_role_name }}""
",filter,edit_var,edit_args,renameとedit_varで競合,to_uuidしなくなった
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/kubernetes/secrets/tasks/sync_kube_master_certs.yml,MODIFY,None,"@@ -2,7 +2,7 @@
 
 - name: sync_kube_master_certs | Create list of needed kube admin certs
   set_fact:
-    kube_master_cert_list: ""{{ kube_master_cert_list|d([]) + ['admin-' + item + '.pem'] }}""
+    kube_admin_cert_list: ""{{ kube_admin_cert_list|d([]) + ['admin-' + item + '.pem'] }}""
   with_items: ""{{ groups['kube-master'] }}""
 
 - include: ../../../vault/tasks/shared/sync_file.yml
@@ -13,11 +13,11 @@
     sync_file_hosts: ""{{ groups['kube-master'] }}""
     sync_file_is_cert: true
     sync_file_owner: kube
-  with_items: ""{{ kube_master_cert_list|d([]) }}""
+  with_items: ""{{ kube_admin_cert_list|d([]) }}""
 
 - name: sync_kube_master_certs | Set facts for kube admin sync_file results
   set_fact:
-    kube_master_certs_needed: ""{{ kube_master_certs_needed|default([]) + [item.path] }}""
+    kube_admin_certs_needed: ""{{ kube_admin_certs_needed|default([]) + [item.path] }}""
   with_items: ""{{ sync_file_results|d([]) }}""
   when: item.no_srcs|bool
 
",edit_var,other,edit_var,変数名のrename,set_factで変数をいじっている
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/cluster/create_roles.yml,MODIFY,None,"@@ -1,18 +1,10 @@
 ---
-- include: ../shared/auth_backend.yml
-  vars:
-    auth_backend_description: A Username/Password Auth Backend primarily used for services needing to issue certificates
-    auth_backend_path: userpass
-    auth_backend_type: userpass
-  when: inventory_hostname == groups.vault|first
-
 - include: ../shared/create_role.yml
   vars:
     create_role_name: ""{{ item.name }}""
     create_role_group: ""{{ item.group }}""
+    create_role_password: ""{{ item.password }}""
     create_role_policy_rules: ""{{ item.policy_rules }}""
     create_role_options: ""{{ item.role_options }}""
-    create_role_mount_path: ""{{ item.mount_path }}""
-  with_items:
-    - ""{{ vault_etcd_role }}""
-    - ""{{ vault_kube_role }}""
+    create_role_mount_path: ""{{ vault_pki_mounts.kube.name }}""
+  with_items: ""{{ vault_pki_mounts.kube.roles }}""
",edit_var,edit_var,move,"create_role_password, create_role_mount_pathが追加される",50行目のファイルに移動されている。同じ変数の値を扱っているけど、変数名や定義の仕方が変わっているっぽい。
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,.gitlab-ci.yml,MODIFY,None,"@@ -366,6 +366,7 @@ before_script:
 
 .ubuntu_vault_sep_variables: &ubuntu_vault_sep_variables
 # stage: deploy-gce-part1
+  AUTHORIZATION_MODES: ""{ 'authorization_modes':  [ 'RBAC' ] }""
   KUBE_NETWORK_PLUGIN: canal
   CERT_MGMT: vault
   CLOUD_IMAGE: ubuntu-1604-xenial
",other,other,other,,ansibleとは関係なさそう？
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/cluster/main.yml,MODIFY,None,"@@ -5,8 +5,6 @@
 - include: ../shared/check_etcd.yml
   when: inventory_hostname in groups.vault
 
-## Vault Cluster Setup
-
 - include: configure.yml
   when: inventory_hostname in groups.vault
 
@@ -25,42 +23,22 @@
 - include: ../shared/find_leader.yml
   when: inventory_hostname in groups.vault
 
-- include: ../shared/create_mount.yml
-  vars:
-    create_mount_path: ""{{ vault_ca_options.common_name }}""
-    create_mount_default_lease_ttl: ""{{ vault_default_lease_ttl }}""
-    create_mount_max_lease_ttl: ""{{ vault_max_lease_ttl }}""
-    create_mount_description: ""Vault Root CA""
-    create_mount_cert_dir: ""{{ vault_cert_dir }}""
-    create_mount_config_ca_needed: true
-  when: inventory_hostname == groups.vault|first
-
-- include: ../shared/create_mount.yml
-  vars:
-    create_mount_path: ""{{ vault_etcd_mount_path }}""
-    create_mount_default_lease_ttl: ""{{ vault_etcd_default_lease_ttl }}""
-    create_mount_max_lease_ttl: ""{{ vault_etcd_max_lease_ttl }}""
-    create_mount_description: ""Etcd Root CA""
-    create_mount_cert_dir: ""{{ vault_etcd_cert_dir }}""
-    create_mount_config_ca_needed: true
-  when: inventory_hostname == groups.vault|first
-
-- include: ../shared/create_mount.yml
-  vars:
-    create_mount_path: ""{{ vault_kube_mount_path }}""
-    create_mount_default_lease_ttl: ""{{ vault_kube_default_lease_ttl }}""
-    create_mount_max_lease_ttl: ""{{ vault_kube_max_lease_ttl }}""
-    create_mount_description: ""Kubernetes Root CA""
-    create_mount_cert_dir: ""{{ vault_kube_cert_dir }}""
-    create_mount_config_ca_needed: false
+- include: create_mounts.yml
   when: inventory_hostname == groups.vault|first
 
 - include: ../shared/gen_ca.yml
   vars:
-    gen_ca_cert_dir: ""{{ vault_kube_cert_dir }}""
-    gen_ca_mount_path: ""{{ vault_kube_mount_path }}""
+    gen_ca_cert_dir: ""{{ vault_pki_mounts.kube.cert_dir }}""
+    gen_ca_mount_path: ""{{ vault_pki_mounts.kube.name }}""
+    gen_ca_vault_headers: ""{{ vault_headers }}""
+    gen_ca_vault_options: ""{{ vault_ca_options.kube }}""
   when: inventory_hostname in groups.vault
 
-## Vault Policies, Roles, and Auth Backends
+- include: ../shared/auth_backend.yml
+  vars:
+    auth_backend_description: A Username/Password Auth Backend primarily used for services needing to issue certificates
+    auth_backend_path: userpass
+    auth_backend_type: userpass
+  when: inventory_hostname == groups.vault|first
 
 - include: create_roles.yml
",move,other,move,includeするファイルに変更がある,59行目のファイルに移行。
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/kubernetes/secrets/tasks/gen_certs_vault.yml,MODIFY,None,"@@ -1,56 +1,23 @@
 ---
 - include: sync_kube_master_certs.yml
   when: inventory_hostname in groups['kube-master']
-  tags: k8s-secrets
 
 - include: sync_kube_node_certs.yml
   when: inventory_hostname in groups['k8s-cluster']
-  tags: k8s-secrets
 
-- name: gen_certs_vault | Read in the local credentials
-  command: cat /etc/vault/roles/kube/userpass
-  register: kube_vault_creds_cat
-  delegate_to: ""{{ groups['k8s-cluster'][0] }}""
-
-- name: gen_certs_vault | Set facts for read Vault Creds
-  set_fact:
-    kube_vault_creds: ""{{ kube_vault_creds_cat.stdout|from_json }}""
-  delegate_to: ""{{ groups['k8s-cluster'][0] }}""
-
-- name: gen_certs_vault | Log into Vault and obtain an token
-  uri:
-    url: ""{{ hostvars[groups['vault'][0]]['vault_leader_url'] }}/v1/auth/userpass/login/{{ kube_vault_creds.username }}""
-    headers:
-      Accept: application/json
-      Content-Type: application/json
-    method: POST
-    body_format: json
-    body:
-      password: ""{{ kube_vault_creds.password }}""
-  register: kube_vault_login_result
-  delegate_to: ""{{ groups['k8s-cluster'][0] }}""
-
-- name: gen_certs_vault | Set fact for Vault API token
-  set_fact:
-    kube_vault_headers:
-      Accept: application/json
-      Content-Type: application/json
-      X-Vault-Token: ""{{ kube_vault_login_result.get('json',{}).get('auth', {}).get('client_token') }}""
-  run_once: true
-
-# Issue certs to kube-master nodes
+# Issue admin certs to kube-master hosts
 - include: ../../../vault/tasks/shared/issue_cert.yml
   vars:
-    issue_cert_copy_ca: ""{{ item == kube_master_certs_needed|first }}""
+    issue_cert_common_name: ""admin:{{ item.rsplit('/', 1)[1].rsplit('.', 1)[0] }}""
+    issue_cert_copy_ca: ""{{ item == kube_admin_certs_needed|first }}""
     issue_cert_file_group: ""{{ kube_cert_group }}""
     issue_cert_file_owner: kube
-    issue_cert_headers: ""{{ kube_vault_headers }}""
     issue_cert_hosts: ""{{ groups['kube-master'] }}""
     issue_cert_path: ""{{ item }}""
-    issue_cert_role: kube
+    issue_cert_role: kube-master
     issue_cert_url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}""
     issue_cert_mount_path: ""{{ kube_vault_mount_path }}""
-  with_items: ""{{ kube_master_certs_needed|d([]) }}""
+  with_items: ""{{ kube_admin_certs_needed|d([]) }}""
   when: inventory_hostname in groups['kube-master']
 
 - name: gen_certs_vault | Set fact about certificate alt names
@@ -69,12 +36,13 @@
   when: loadbalancer_apiserver is defined and apiserver_loadbalancer_domain_name is defined
   run_once: true
 
+# Issue master components certs to kube-master hosts
 - include: ../../../vault/tasks/shared/issue_cert.yml
   vars:
+    issue_cert_common_name: ""kubernetes""
     issue_cert_alt_names: ""{{ kube_cert_alt_names }}""
     issue_cert_file_group: ""{{ kube_cert_group }}""
     issue_cert_file_owner: kube
-    issue_cert_headers: ""{{ kube_vault_headers }}""
     issue_cert_hosts: ""{{ groups['kube-master'] }}""
     issue_cert_ip_sans: >-
         [
@@ -87,7 +55,7 @@
         ""127.0.0.1"",""::1"",""{{ kube_apiserver_ip }}""
         ]
     issue_cert_path: ""{{ item }}""
-    issue_cert_role: kube
+    issue_cert_role: kube-master
     issue_cert_url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}""
     issue_cert_mount_path: ""{{ kube_vault_mount_path }}""
   with_items: ""{{ kube_master_components_certs_needed|d([]) }}""
@@ -97,27 +65,28 @@
 # Issue node certs to k8s-cluster nodes
 - include: ../../../vault/tasks/shared/issue_cert.yml
   vars:
+    issue_cert_common_name: ""system:node:{{ item.rsplit('/', 1)[1].rsplit('.', 1)[0] }}""
     issue_cert_copy_ca: ""{{ item == kube_node_certs_needed|first }}""
     issue_cert_file_group: ""{{ kube_cert_group }}""
     issue_cert_file_owner: kube
-    issue_cert_headers: ""{{ kube_vault_headers }}""
     issue_cert_hosts: ""{{ groups['k8s-cluster'] }}""
     issue_cert_path: ""{{ item }}""
-    issue_cert_role: kube
+    issue_cert_role: kube-node
     issue_cert_url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}""
     issue_cert_mount_path: ""{{ kube_vault_mount_path }}""
   with_items: ""{{ kube_node_certs_needed|d([]) }}""
   when: inventory_hostname in groups['k8s-cluster']
 
+# Issue proxy certs to k8s-cluster nodes
 - include: ../../../vault/tasks/shared/issue_cert.yml
   vars:
+    issue_cert_common_name: ""system:kube-proxy:{{ item.rsplit('/', 1)[1].rsplit('.', 1)[0] }}""
     issue_cert_copy_ca: ""{{ item == kube_proxy_certs_needed|first }}""
     issue_cert_file_group: ""{{ kube_cert_group }}""
     issue_cert_file_owner: kube
-    issue_cert_headers: ""{{ kube_vault_headers }}""
     issue_cert_hosts: ""{{ groups['k8s-cluster'] }}""
     issue_cert_path: ""{{ item }}""
-    issue_cert_role: kube
+    issue_cert_role: kube-proxy
     issue_cert_url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}""
     issue_cert_mount_path: ""{{ kube_vault_mount_path }}""
   with_items: ""{{ kube_proxy_certs_needed|d([]) }}""
",move,remove_task,move,certsを生成するタスクを削除している,44行目のファイルに移行された
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/kubespray-defaults/defaults/main.yaml,MODIFY,None,"@@ -135,3 +135,10 @@ rbac_enabled: ""{{ 'RBAC' in authorization_modes }}""
 ## List of key=value pairs that describe feature gates for
 ## the k8s cluster.
 kube_feature_gates: []
+
+# Vault data dirs.
+vault_base_dir: /etc/vault
+vault_cert_dir: ""{{ vault_base_dir }}/ssl""
+vault_config_dir: ""{{ vault_base_dir }}/config""
+vault_roles_dir: ""{{ vault_base_dir }}/roles""
+vault_secrets_dir: ""{{ vault_base_dir }}/secrets""
",edit_var,edit_var,edit_var,defaults/ディレクトリに含まれる変数の追加，削除が行われている,default値の定義
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/defaults/main.yml,MODIFY,None,"@@ -1,4 +1,6 @@
 ---
+vault_bootstrap: false
+vault_deployment_type: docker
 
 vault_adduser_vars:
   comment: ""Hashicorp Vault User""
@@ -6,41 +8,18 @@ vault_adduser_vars:
   name: vault
   shell: /sbin/nologin
   system: yes
+
+# This variables redefined in kubespray-defaults for using shared tasks
+# in etcd and kubernetes/secrets roles
 vault_base_dir: /etc/vault
-# https://releases.hashicorp.com/vault/0.6.4/vault_0.6.4_SHA256SUMS
-vault_version: 0.6.4
-vault_binary_checksum: 04d87dd553aed59f3fe316222217a8d8777f40115a115dac4d88fac1611c51a6
-vault_bootstrap: false
-vault_ca_options:
-  common_name: vault
-  format: pem
-  ttl: 87600h
 vault_cert_dir: ""{{ vault_base_dir }}/ssl""
-vault_client_headers:
-  Accept: ""application/json""
-  Content-Type: ""application/json""
-vault_config:
-  backend:
-    etcd:
-      address: ""{{ vault_etcd_url }}""
-      ha_enabled: ""true""
-      redirect_addr: ""https://{{ ansible_default_ipv4.address }}:{{ vault_port }}""
-      tls_ca_file: ""{{ vault_etcd_cert_dir }}/ca.pem""
-  cluster_name: ""kubernetes-vault""
-  default_lease_ttl: ""{{ vault_default_lease_ttl }}""
-  listener:
-    tcp:
-      address: ""0.0.0.0:{{ vault_port }}""
-      tls_cert_file: ""{{ vault_cert_dir }}/api.pem""
-      tls_key_file: ""{{ vault_cert_dir }}/api-key.pem""
-  max_lease_ttl: ""{{ vault_max_lease_ttl }}""
 vault_config_dir: ""{{ vault_base_dir }}/config""
-vault_container_name: kube-hashicorp-vault
-# This variable is meant to match the GID of vault inside Hashicorp's official Vault Container
-vault_default_lease_ttl: 720h
-vault_default_role_permissions:
-  allow_any_name: true
-vault_deployment_type: docker
+vault_roles_dir: ""{{ vault_base_dir }}/roles""
+vault_secrets_dir: ""{{ vault_base_dir }}/secrets""
+vault_log_dir: ""/var/log/vault""
+
+vault_version: 0.8.1
+vault_binary_checksum: 3c4d70ba71619a43229e65c67830e30e050eab7a81ac6b28325ff707e5914188
 vault_download_url: ""https://releases.hashicorp.com/vault/{{ vault_version }}/vault_{{ vault_version }}_linux_amd64.zip""
 vault_download_vars:
   container: ""{{ vault_deployment_type != 'host' }}""
@@ -55,17 +34,19 @@ vault_download_vars:
   unarchive: true
   url: ""{{ vault_download_url }}""
   version: ""{{ vault_version }}""
-vault_etcd_url: ""https://{{ hostvars[groups.etcd[0]]['ip']|d(hostvars[groups.etcd[0]]['ansible_default_ipv4']['address']) }}:2379""
+
+vault_container_name: kube-hashicorp-vault
+vault_temp_container_name: vault-temp
 vault_image_repo: ""vault""
 vault_image_tag: ""{{ vault_version }}""
-vault_log_dir: ""/var/log/vault""
-vault_max_lease_ttl: 87600h
-vault_needs_gen: false
+
+vault_address: 0.0.0.0
 vault_port: 8200
-vault_roles_dir: ""{{ vault_base_dir }}/roles""
-vault_secret_shares: 1
-vault_secret_threshold: 1
-vault_secrets_dir: ""{{ vault_base_dir }}/secrets""
+vault_etcd_url: ""https://{{ hostvars[groups.etcd[0]]['ip']|d(hostvars[groups.etcd[0]]['ansible_default_ipv4']['address']) }}:2379""
+
+vault_default_lease_ttl: 720h
+vault_max_lease_ttl: 87600h
+
 vault_temp_config:
   backend:
     file:
@@ -73,29 +54,109 @@ vault_temp_config:
   default_lease_ttl: ""{{ vault_default_lease_ttl }}""
   listener:
     tcp:
-      address: ""0.0.0.0:{{ vault_port }}""
+      address: ""{{ vault_address }}:{{ vault_port }}""
       tls_disable: ""true""
   max_lease_ttl: ""{{ vault_max_lease_ttl }}""
-vault_temp_container_name: vault-temp
-# etcd pki mount options
+
+vault_config:
+  backend:
+    etcd:
+      address: ""{{ vault_etcd_url }}""
+      ha_enabled: ""true""
+      redirect_addr: ""https://{{ ansible_default_ipv4.address }}:{{ vault_port }}""
+      tls_ca_file: ""{{ vault_etcd_cert_dir }}/ca.pem""
+  cluster_name: ""kubernetes-vault""
+  default_lease_ttl: ""{{ vault_default_lease_ttl }}""
+  max_lease_ttl: ""{{ vault_max_lease_ttl }}""
+  listener:
+    tcp:
+      address: ""{{ vault_address }}:{{ vault_port }}""
+      tls_cert_file: ""{{ vault_cert_dir }}/api.pem""
+      tls_key_file: ""{{ vault_cert_dir }}/api-key.pem""
+
+vault_secret_shares: 1
+vault_secret_threshold: 1
+
+vault_ca_options:
+  vault:
+    common_name: vault
+    format: pem
+    ttl: ""{{ vault_max_lease_ttl }}""
+    exclude_cn_from_sans: true
+  etcd:
+    common_name: etcd
+    format: pem
+    ttl: ""{{ vault_max_lease_ttl }}""
+    exclude_cn_from_sans: true
+  kube:
+    common_name: kube
+    format: pem
+    ttl: ""{{ vault_max_lease_ttl }}""
+    exclude_cn_from_sans: true
+
+vault_client_headers:
+  Accept: ""application/json""
+  Content-Type: ""application/json""
+
 vault_etcd_cert_dir: /etc/ssl/etcd/ssl
-vault_etcd_mount_path: etcd
-vault_etcd_default_lease_ttl: 720h
-vault_etcd_max_lease_ttl: 87600h
-vault_etcd_role:
-  name: etcd
-  group: etcd
-  policy_rules: default
-  role_options: default
-  mount_path: ""{{ vault_etcd_mount_path }}""
-# kubernetes pki mount options
-vault_kube_cert_dir: ""{{ kube_cert_dir }}""
-vault_kube_mount_path: kube
-vault_kube_default_lease_ttl: 720h
-vault_kube_max_lease_ttl: 87600h
-vault_kube_role:
-  name: kube
-  group: k8s-cluster
-  policy_rules: default
-  role_options: default
-  mount_path: ""{{ vault_kube_mount_path }}""
+vault_kube_cert_dir: /etc/kubernetes/ssl
+
+vault_pki_mounts:
+  vault:
+    name: vault
+    default_lease_ttl: ""{{ vault_default_lease_ttl }}""
+    max_lease_ttl: ""{{ vault_max_lease_ttl }}""
+    description: ""Vault Root CA""
+    cert_dir: ""{{ vault_cert_dir }}""
+    roles:
+      - name: vault
+        group: vault
+        password: ""{{ lookup('pipe','date +%Y%m%d%H%M%S' + cluster_name + 'vault') | to_uuid }}""
+        policy_rules: default
+        role_options: default
+  etcd:
+    name: etcd
+    default_lease_ttl: ""{{ vault_default_lease_ttl }}""
+    max_lease_ttl: ""{{ vault_max_lease_ttl }}""
+    description: ""Etcd Root CA""
+    cert_dir: ""{{ vault_etcd_cert_dir }}""
+    roles:
+      - name: etcd
+        group: etcd
+        password: ""{{ lookup('pipe','date +%Y%m%d%H%M%S' + cluster_name + 'etcd') | to_uuid }}""
+        policy_rules: default
+        role_options:
+          allow_any_name: true
+          enforce_hostnames: false
+          organization: ""kube:etcd""
+  kube:
+    name: kube
+    default_lease_ttl: ""{{ vault_default_lease_ttl }}""
+    max_lease_ttl: ""{{ vault_max_lease_ttl }}""
+    description: ""Kubernetes Root CA""
+    cert_dir: ""{{ vault_kube_cert_dir }}""
+    roles:
+      - name: kube-master
+        group: kube-master
+        password: ""{{ lookup('pipe','date +%Y%m%d%H%M%S' + cluster_name + 'kube-master') | to_uuid }}""
+        policy_rules: default
+        role_options:
+          allow_any_name: true
+          enforce_hostnames: false
+          organization: ""system:masters""
+      - name: kube-node
+        group: k8s-cluster
+        password: ""{{ lookup('pipe','date +%Y%m%d%H%M%S' + cluster_name + 'kube-node') | to_uuid }}""
+        policy_rules: default
+        role_options:
+          allow_any_name: true
+          enforce_hostnames: false
+          organization: ""system:nodes""
+      - name: kube-proxy
+        group: k8s-cluster
+        password: ""{{ lookup('pipe', 'date +%Y%m%d%H%M%S' + cluster_name + 'kube-proxy') | to_uuid }}""
+        policy_rules: default
+        role_options:
+          allow_any_name: true
+          enforce_hostnames: false
+          organization: ""system:node-proxier""
",edit_var,edit_var,edit_var,defaults/ディレクトリに含まれる変数の追加，削除が行われている,変数名の定義。でかい。
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/etcd/tasks/gen_certs_vault.yml,MODIFY,None,"@@ -7,51 +7,14 @@
   when: inventory_hostname in etcd_node_cert_hosts
   tags: etcd-secrets
 
-- name: gen_certs_vault | Read in the local credentials
-  command: cat /etc/vault/roles/etcd/userpass
-  register: etcd_vault_creds_cat
-  delegate_to: ""{{ groups['vault'][0] }}""
-
-- name: gen_certs_vault | Set facts for read Vault Creds
-  set_fact:
-    etcd_vault_creds: ""{{ etcd_vault_creds_cat.stdout|from_json }}""
-  delegate_to: ""{{ groups['vault'][0] }}""
-
-- name: gen_certs_vault | Log into Vault and obtain an token
-  uri:
-    url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}/v1/auth/userpass/login/{{ etcd_vault_creds.username }}""
-    headers:
-      Accept: application/json
-      Content-Type: application/json
-    method: POST
-    body_format: json
-    body:
-      password: ""{{ etcd_vault_creds.password }}""
-  register: etcd_vault_login_result
-  delegate_to: ""{{ groups['vault'][0] }}""
-
-- name: gen_certs_vault | Set fact for vault_client_token
-  set_fact:
-    vault_client_token: ""{{ etcd_vault_login_result.get('json', {}).get('auth', {}).get('client_token') }}""
-  run_once: true
-
-- name: gen_certs_vault | Set fact for Vault API token
-  set_fact:
-    etcd_vault_headers:
-      Accept: application/json
-      Content-Type: application/json
-      X-Vault-Token: ""{{ vault_client_token }}""
-  run_once: true
-  when: vault_client_token != """"
-
 # Issue master certs to Etcd nodes
 - include: ../../vault/tasks/shared/issue_cert.yml
   vars:
+    issue_cert_common_name: ""etcd:master:{{ item.rsplit('/', 1)[1].rsplit('.', 1)[0] }}""
     issue_cert_alt_names: ""{{ groups.etcd + ['localhost'] }}""
     issue_cert_copy_ca: ""{{ item == etcd_master_certs_needed|first }}""
     issue_cert_file_group: ""{{ etcd_cert_group }}""
     issue_cert_file_owner: kube
-    issue_cert_headers: ""{{ etcd_vault_headers }}""
     issue_cert_hosts: ""{{ groups.etcd }}""
     issue_cert_ip_sans: >-
         [
@@ -74,11 +37,11 @@
 # Issue node certs to everyone else
 - include: ../../vault/tasks/shared/issue_cert.yml
   vars:
+    issue_cert_common_name: ""etcd:node:{{ item.rsplit('/', 1)[1].rsplit('.', 1)[0] }}""
     issue_cert_alt_names: ""{{ etcd_node_cert_hosts }}""
     issue_cert_copy_ca: ""{{ item == etcd_node_certs_needed|first }}""
     issue_cert_file_group: ""{{ etcd_cert_group }}""
     issue_cert_file_owner: kube
-    issue_cert_headers: ""{{ etcd_vault_headers }}""
     issue_cert_hosts: ""{{ etcd_node_cert_hosts }}""
     issue_cert_ip_sans: >-
         [
",move,remove_task,move,certsを生成するタスクの削除,44行目のファイルに移行されたものが大半。
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/bootstrap/create_mounts.yml,ADD,None,"@@ -0,0 +1,12 @@
+---
+- include: ../shared/create_mount.yml
+  vars:
+    create_mount_path: ""{{ item.name }}""
+    create_mount_default_lease_ttl: ""{{ item.default_lease_ttl }}""
+    create_mount_max_lease_ttl: ""{{ item.max_lease_ttl }}""
+    create_mount_description: ""{{ item.description }}""
+    create_mount_cert_dir: ""{{ item.cert_dir }}""
+    create_mount_config_ca_needed: ""{{ item.config_ca }}""
+  with_items:
+    - ""{{ vault_pki_mounts.vault|combine({'config_ca': not vault_ca_cert_needed}) }}""
+    - ""{{ vault_pki_mounts.etcd|combine({'config_ca': not vault_etcd_ca_cert_needed}) }}""
",move,other,move,includeによって実行するタスクを追加している？,58行目から移行
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/bootstrap/create_roles.yml,ADD,None,"@@ -0,0 +1,10 @@
+---
+- include: ../shared/create_role.yml
+  vars:
+    create_role_name: ""{{ item.name }}""
+    create_role_group: ""{{ item.group }}""
+    create_role_policy_rules: ""{{ item.policy_rules }}""
+    create_role_password: ""{{ item.password }}""
+    create_role_options: ""{{ item.role_options }}""
+    create_role_mount_path: ""{{ mount.name }}""
+  with_items: ""{{ mount.roles }}""
",move,other,move,includeによって実行するタスクを追加している？,58行目のファイルから移行
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/bootstrap/gen_vault_certs.yml,MODIFY,None,"@@ -1,29 +1,21 @@
 ---
-
-- name: boostrap/gen_vault_certs | Add the vault role
-  uri:
-    url: ""{{ vault_leader_url }}/v1/{{ vault_ca_options.common_name }}/roles/vault""
-    headers: ""{{ vault_headers }}""
-    method: POST
-    body_format: json
-    body: ""{{ vault_default_role_permissions }}""
-    status_code: 204
-  when: inventory_hostname == groups.vault|first and vault_api_cert_needed
-
 - include: ../shared/issue_cert.yml
   vars:
+    issue_cert_common_name: ""{{ vault_pki_mounts.vault.roles[0].name }}""
     issue_cert_alt_names: ""{{ groups.vault + ['localhost'] }}""
     issue_cert_hosts: ""{{ groups.vault }}""
     issue_cert_ip_sans: >-
         [
         {%- for host in groups.vault -%}
         ""{{ hostvars[host]['ansible_default_ipv4']['address'] }}"",
+        {%- if hostvars[host]['ip'] is defined -%}
+        ""{{ hostvars[host]['ip'] }}"",
+        {%- endif -%}
         {%- endfor -%}
         ""127.0.0.1"",""::1""
         ]
-    issue_cert_mount_path: ""{{ vault_ca_options.common_name }}""
+    issue_cert_mount_path: ""{{ vault_pki_mounts.vault.name }}""
     issue_cert_path: ""{{ vault_cert_dir }}/api.pem""
-    issue_cert_headers: ""{{ hostvars[groups.vault|first]['vault_headers'] }}""
-    issue_cert_role: vault
+    issue_cert_role: ""{{ vault_pki_mounts.vault.roles[0].name }}""
     issue_cert_url: ""{{ vault_leader_url }}""
   when: vault_api_cert_needed
",remove_task,remove_task,remove_task,certsを生成するタスクの削除．それに伴う変数の変更,taskの削除と、jinja2で使う値の変更って感じ。
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/bootstrap/main.yml,MODIFY,None,"@@ -1,5 +1,4 @@
 ---
-
 - include: ../shared/check_vault.yml
   when: inventory_hostname in groups.vault
 
@@ -9,72 +8,57 @@
 - include: ../shared/find_leader.yml
   when: inventory_hostname in groups.vault and vault_cluster_is_initialized|d()
 
-## Sync Certs
-
 - include: sync_vault_certs.yml
   when: inventory_hostname in groups.vault
 
 - include: sync_etcd_certs.yml
   when: inventory_hostname in groups.etcd
 
-## Generate Certs
-
-# Start a temporary instance of Vault
 - include: start_vault_temp.yml
-  when: >-
-        inventory_hostname == groups.vault|first and
-        not vault_cluster_is_initialized
+  when: inventory_hostname == groups.vault|first and not vault_cluster_is_initialized
 
-# Set vault_leader_url for all nodes based on above
-- name: vault | bootstrap
+- name: vault | Set fact about vault leader url
   set_fact:
     vault_leader_url: ""{{ hostvars[groups.vault|first]['vault_leader_url'] }}""
   when: not vault_cluster_is_initialized
 
-# Ensure vault PKI mounts exists
-- include: ../shared/create_mount.yml
-  vars:
-    create_mount_path: ""{{ vault_ca_options.common_name }}""
-    create_mount_default_lease_ttl: ""{{ vault_default_lease_ttl }}""
-    create_mount_max_lease_ttl: ""{{ vault_max_lease_ttl }}""
-    create_mount_description: ""Vault Root CA""
-    create_mount_cert_dir: ""{{ vault_cert_dir }}""
-    create_mount_config_ca_needed: ""{{ not vault_ca_cert_needed }}""
+- include: create_mounts.yml
   when: inventory_hostname == groups.vault|first
 
-# Generate root CA certs for Vault if none exist
-- include: ../shared/gen_ca.yml
+- include: ../shared/auth_backend.yml
   vars:
-    gen_ca_cert_dir: ""{{ vault_cert_dir }}""
-    gen_ca_mount_path: ""{{ vault_ca_options.common_name }}""
-  when: >-
-        inventory_hostname in groups.vault and
-        not vault_cluster_is_initialized and
-        vault_ca_cert_needed
+    auth_backend_description: A Username/Password Auth Backend primarily used for services needing to issue certificates
+    auth_backend_path: userpass
+    auth_backend_type: userpass
+  when: inventory_hostname == groups.vault|first
 
-# Generate Vault API certs
-- include: gen_vault_certs.yml
-  when: inventory_hostname in groups.vault and vault_api_cert_needed
+- include: create_roles.yml
+  with_items:
+    - ""{{ vault_pki_mounts.vault }}""
+    - ""{{ vault_pki_mounts.etcd }}""
+  loop_control:
+    loop_var: mount
 
-# Ensure etcd PKI mounts exists
-- include: ../shared/create_mount.yml
+- include: ../shared/gen_ca.yml
   vars:
-    create_mount_path: ""{{ vault_etcd_mount_path }}""
-    create_mount_default_lease_ttl: ""{{ vault_etcd_default_lease_ttl }}""
-    create_mount_max_lease_ttl: ""{{ vault_etcd_max_lease_ttl }}""
-    create_mount_description: ""Etcd Root CA""
-    create_mount_cert_dir: ""{{ vault_etcd_cert_dir }}""
-    create_mount_config_ca_needed: ""{{ not vault_etcd_ca_cert_needed }}""
-  when: inventory_hostname == groups.vault|first
+    gen_ca_cert_dir: ""{{ vault_pki_mounts.vault.cert_dir }}""
+    gen_ca_mount_path: ""{{ vault_pki_mounts.vault.name }}""
+    gen_ca_vault_headers: ""{{ vault_headers }}""
+    gen_ca_vault_options: ""{{ vault_ca_options.vault }}""
+  when: >-
+        inventory_hostname in groups.vault
+        and not vault_cluster_is_initialized
+        and vault_ca_cert_needed
 
-# Generate root CA certs for etcd if none exist
 - include: ../shared/gen_ca.yml
   vars:
-    gen_ca_cert_dir: ""{{ vault_etcd_cert_dir }}""
-    gen_ca_mount_path: ""{{ vault_etcd_mount_path }}""
+    gen_ca_cert_dir: ""{{ vault_pki_mounts.etcd.cert_dir }}""
+    gen_ca_mount_path: ""{{ vault_pki_mounts.etcd.name }}""
+    gen_ca_vault_headers: ""{{ vault_headers }}""
+    gen_ca_vault_options: ""{{ vault_ca_options.etcd }}""
   when: inventory_hostname in groups.etcd and vault_etcd_ca_cert_needed
 
-- include: create_etcd_role.yml
+- include: gen_vault_certs.yml
+  when: inventory_hostname in groups.vault and vault_api_cert_needed
 
-# Update all host's CA bundle, etcd CA will be added in etcd role
 - include: ca_trust.yml
",move,other,move,"複数の変更が含まれる
- includeされるファイルが異なる
- includeで使われる変数が異なる","55, 56, 60行目のファイルへ移行。include: gen_vault_certs.ymlはファイル内で移動"
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/cluster/create_mounts.yml,ADD,None,"@@ -0,0 +1,13 @@
+---
+- include: ../shared/create_mount.yml
+  vars:
+    create_mount_path: ""{{ item.name }}""
+    create_mount_default_lease_ttl: ""{{ item.default_lease_ttl }}""
+    create_mount_max_lease_ttl: ""{{ item.max_lease_ttl }}""
+    create_mount_description: ""{{ item.description }}""
+    create_mount_cert_dir: ""{{ item.cert_dir }}""
+    create_mount_config_ca_needed: ""{{ item.name != vault_pki_mounts.kube.name }}""
+  with_items:
+    - ""{{ vault_pki_mounts.vault }}""
+    - ""{{ vault_pki_mounts.etcd }}""
+    - ""{{ vault_pki_mounts.kube }}""
",move,add_task,move,includeで実行されるタスクが追加される,50行目から移行。
bf0af1cd3d1c7648b4b731b9dc46aa4f053641fd,2017-09-05 06:07:35+00:00,Vault role updates: * using separated vault roles for generate certs with different `O` (Organization) subject field; * configure vault roles for issuing certificates with different `CN` (Common name) subject field; * set `CN` and `O` to `kubernetes` and `etcd` certificates; * vault/defaults vars definition was simplified; * vault dirs variables defined in kubernetes-defaults foles for using shared tasks in etcd and kubernetes/secrets roles; * upgrade vault to 0.8.1; * generate random vault user password for each role by default; * fix `serial` file name for vault certs; * move vault auth request to issue_cert tasks; * enable `RBAC` in vault CI;,roles/vault/tasks/bootstrap/create_etcd_role.yml,DELETE,None,"@@ -1,17 +0,0 @@
----
-- include: ../shared/auth_backend.yml
-  vars:
-    auth_backend_description: A Username/Password Auth Backend primarily used for services needing to issue certificates
-    auth_backend_path: userpass
-    auth_backend_type: userpass
-  delegate_to: ""{{ groups.vault|first }}""
-  run_once: true
-
-- include: ../shared/create_role.yml
-  vars:
-    create_role_name: ""{{ vault_etcd_role.name }}""
-    create_role_group: ""{{ vault_etcd_role.group }}""
-    create_role_policy_rules: ""{{ vault_etcd_role.policy_rules }}""
-    create_role_options: ""{{ vault_etcd_role.role_options }}""
-    create_role_mount_path: ""{{ vault_etcd_role.mount_path }}""
-  when: inventory_hostname in groups.etcd
",move,remove_task,move,includeで実行されるタスクが削除される,ファイルごと削除した代わりに、roles/vault/tasks/bootstrap/main.yml内に直接書くようになっている
5dc56df64e02ed3a920df66a5848922b6a18896d,2017-10-24 16:28:07+00:00,Fix ordering of kube-apiserver admission control plug-ins (#1841),roles/kubernetes/master/defaults/main.yml,MODIFY,1841,"@@ -39,13 +39,13 @@ kube_apiserver_cpu_requests: 100m
 
 # Admission control plug-ins
 kube_apiserver_admission_control:
+  - Initializers
   - NamespaceLifecycle
   - LimitRanger
   - ServiceAccount
   - DefaultStorageClass
-  - ResourceQuota
-  - Initializers
   - GenericAdmissionWebhook
+  - ResourceQuota
 
 # extra runtime config
 kube_api_runtime_config:
",move,move,edit_var,変数の宣言位置が変更される,順序が大事らしい。リストの変更という感じで。
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/kubernetes/preinstall/tasks/main.yml,MODIFY,2236,"@@ -89,10 +89,11 @@
     - ""/etc/cni/net.d""
     - ""/opt/cni/bin""
   when:
-    - kube_network_plugin in [""calico"", ""weave"", ""canal"", ""flannel"", ""contiv""]
+    - kube_network_plugin in [""calico"", ""weave"", ""canal"", ""flannel"", ""contiv"", ""cilium""]
     - inventory_hostname in groups['k8s-cluster']
   tags:
     - network
+    - cilium
     - calico
     - weave
     - canal
",condition,condition,condition,"whenディレクティブとtagディレクティブの変更
tagは実行時に明示的に指定するため影響が少ないと考えた",whenが本質っぽい
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/kubernetes/preinstall/tasks/verify-settings.yml,MODIFY,2236,"@@ -88,4 +88,10 @@
   assert:
     that: rbac_enabled and kube_api_anonymous_auth
   when: kube_apiserver_insecure_port == 0
+  ignore_errors: ""{{ ignore_assert_errors }}""
+
+- name: Stop if kernel version is too low
+  assert:
+    that: ansible_kernel.split('-')[0]|version_compare('4.8', '>=')
+  when: kube_network_plugin == 'cilium'
   ignore_errors: ""{{ ignore_assert_errors }}""
\ No newline at end of file
",add_task,add_task,add_task,バージョンチェックを行うタスクの追加,タスク追加
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/kubespray-defaults/defaults/main.yaml,MODIFY,2236,"@@ -82,7 +82,7 @@ kube_users:
     pass: ""{{kube_api_pwd}}""
     role: admin
 
-# Choose network plugin (calico, weave or flannel)
+# Choose network plugin (cilium, calico, weave or flannel)
 # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing
 kube_network_plugin: calico
 
",comment,other,other,コメントの修正だけ,コメントの修正
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/network_plugin/cilium/handlers/main.yml,ADD,2236,"@@ -0,0 +1,14 @@
+---
+- name: restart kubelet
+  command: /bin/true
+  notify:
+    - Kubelet | reload systemd
+    - Kubelet | reload kubelet
+
+- name: Kubelet | reload systemd
+  command: systemctl daemon-reload
+
+- name: Kubelet | reload kubelet
+  service:
+    name: kubelet
+    state: restarted
\ No newline at end of file
",add_task,add_task,add_task,新しく追加したファイルにタスクが記述されている,新しくハンドラが追加されている
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",tests/files/gce_centos7-cilium.yml,ADD,2236,"@@ -0,0 +1,11 @@
+# Instance settings
+cloud_image_family: centos-7
+cloud_region: us-central1-c
+cloud_machine_type: ""n1-standard-1""
+mode: default
+
+# Deployment settings
+kube_network_plugin: cilium
+deploy_netchecker: true
+kubedns_min_replicas: 1
+cloud_provider: gce
",edit_var,edit_var,add_task,テストに用いる変数が追加されている,テスト用環境の変数の定義っぽい。別スクリプトを実行して、Ansibleを実行してテストを行っているっぽい。
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/network_plugin/meta/main.yml,MODIFY,2236,"@@ -1,5 +1,10 @@
 ---
 dependencies:
+  - role: network_plugin/cilium
+    when: kube_network_plugin == 'cilium'
+    tags:
+      - cilium
+
   - role: network_plugin/calico
     when: kube_network_plugin == 'calico'
     tags:
",role,other,role,metaデータの追加,依存ロールの定義
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",tests/files/gce_coreos-cilium.yml,ADD,2236,"@@ -0,0 +1,13 @@
+# Instance settings
+cloud_image_family: coreos-stable
+cloud_region: us-central1-c
+mode: default
+startup_script: 'systemctl disable locksmithd && systemctl stop locksmithd'
+
+# Deployment settings
+kube_network_plugin: cilium
+bootstrap_os: coreos
+resolvconf_mode: host_resolvconf # this is required as long as the coreos stable channel uses docker < 1.12
+deploy_netchecker: true
+kubedns_min_replicas: 1
+cloud_provider: gce
",edit_var,edit_var,edit_var,テストに用いる変数が追加されている,テスト
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",tests/files/gce_rhel7-cilium.yml,ADD,2236,"@@ -0,0 +1,10 @@
+# Instance settings
+cloud_image_family: rhel-7
+cloud_region: us-central1-b
+mode: default
+
+# Deployment settings
+kube_network_plugin: cilium
+deploy_netchecker: true
+kubedns_min_replicas: 1
+cloud_provider: gce
",edit_var,edit_var,edit_var,テストに用いる変数が追加されている,テスト
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",tests/files/gce_ubuntu-cilium-sep.yml,ADD,2236,"@@ -0,0 +1,11 @@
+# Instance settings
+cloud_image_family: ubuntu-1604-lts
+cloud_region: us-central1-b
+mode: separate
+
+# Deployment settings
+kube_network_plugin: cilium
+deploy_netchecker: true
+kubedns_min_replicas: 1
+cloud_provider: gce
+
",edit_var,edit_var,edit_var,テストに用いる変数が追加されている,テスト
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/kubernetes-apps/network_plugin/meta/main.yml,MODIFY,2236,"@@ -1,5 +1,10 @@
 ---
 dependencies:
+  - role: kubernetes-apps/network_plugin/cilium
+    when: kube_network_plugin == 'cilium'
+    tags:
+      - cilium
+
   - role: kubernetes-apps/network_plugin/calico
     when: kube_network_plugin == 'calico'
     tags:
",role,other,role,metaデータの追加,依存ロールの定義
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/network_plugin/cilium/tasks/main.yml,ADD,2236,"@@ -0,0 +1,55 @@
+---
+- name: Cilium | Ensure BFPFS mounted
+  mount:
+    fstype: bpf
+    path: /sys/fs/bpf
+    src: bpffs
+    state: mounted
+
+- name: Cilium | Create Cilium certs directory
+  file:
+    dest: ""{{ cilium_cert_dir }}""
+    state: directory
+    mode: 0750
+    owner: root
+    group: root
+
+- name: Cilium | Link etcd certificates for cilium
+  file:
+    src: ""{{ etcd_cert_dir }}/{{ item.s }}""
+    dest: ""{{ cilium_cert_dir }}/{{ item.d }}""
+    state: hard
+    force: yes
+  with_items:
+    - {s: ""ca.pem"", d: ""ca_cert.crt""}
+    - {s: ""node-{{ inventory_hostname }}.pem"", d: ""cert.crt""}
+    - {s: ""node-{{ inventory_hostname }}-key.pem"", d: ""key.pem""}
+
+- name: Cilium | Create Cilium node manifests
+  template:
+    src: ""{{item.file}}.j2""
+    dest: ""{{kube_config_dir}}/{{item.file}}""
+  with_items:
+    - {name: cilium, file: cilium-config.yml, type: cm}
+    - {name: cilium, file: cilium-crb.yml, type: clusterrolebinding}
+    - {name: cilium, file: cilium-cr.yml, type: clusterrole}
+    - {name: cilium, file: cilium-ds.yml, type: ds}
+    - {name: cilium, file: cilium-sa.yml, type: sa}
+  register: cilium_node_manifests
+  when:
+    - inventory_hostname in groups['kube-master']
+    - rbac_enabled or item.type not in rbac_resources
+
+- name: Cilium | Set CNI directory permissions
+  file:
+    path: /opt/cni/bin
+    state: directory
+    owner: kube
+    recurse: true
+    mode: 0755
+  register: cni_bin_dir
+
+- name: Cilium | Create network policy directory
+  file:
+    path: ""{{ cilium_policy_dir }}""
+    state: directory
",add_task,add_task,add_task,新規ファイルにタスクが追加されている,タスク追加
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/kubernetes-apps/network_plugin/cilium/tasks/main.yml,ADD,2236,"@@ -0,0 +1,20 @@
+---
+- name: Cilium | Start Resources
+  kube:
+    name: ""{{item.item.name}}""
+    namespace: ""{{ system_namespace }}""
+    kubectl: ""{{bin_dir}}/kubectl""
+    resource: ""{{item.item.type}}""
+    filename: ""{{kube_config_dir}}/{{item.item.file}}""
+    state: ""latest""
+  with_items: ""{{ cilium_node_manifests.results }}""
+  when: inventory_hostname == groups['kube-master'][0] and not item|skipped
+
+- name: Cilium | Wait for pods to run
+  command: ""{{bin_dir}}/kubectl -n {{system_namespace}} get pods -l k8s-app=cilium -o jsonpath='{.items[?(@.status.containerStatuses[0].ready==false)].metadata.name}'""
+  register: pods_not_ready
+  until: pods_not_ready.stdout.find(""cilium"")==-1
+  retries: 30
+  delay: 10
+  ignore_errors: yes
+  when: inventory_hostname == groups['kube-master'][0]
",add_task,add_task,add_task,新規ファイルにタスクが追加されている,タスク追加
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/network_plugin/cilium/defaults/main.yml,ADD,2236,"@@ -0,0 +1,25 @@
+---
+# Log-level
+cilium_debug: false
+cilium_disable_ipv4: false
+
+# Etcd SSL dirs
+cilium_cert_dir: /etc/cilium/certs
+etcd_cert_dir: /etc/ssl/etcd/ssl
+
+# Cilium Network Policy directory
+cilium_policy_dir: /etc/kubernetes/policy
+
+# Limits for apps
+cilium_memory_limit: 500M
+cilium_cpu_limit: 200m
+cilium_memory_requests: 64M
+cilium_cpu_requests: 50m
+
+# Optional features
+cilium_enable_prometheus: false
+
+rbac_resources:
+  - sa
+  - clusterrole
+  - clusterrolebinding
",edit_var,edit_var,edit_var,defaultsディレクトリに新しくファイルが追加された,変数定義
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",inventory/sample/group_vars/k8s-cluster.yml,MODIFY,2236,"@@ -62,7 +62,7 @@ kube_users:
 # kube_oidc_groups_claim: groups
 
 
-# Choose network plugin (calico, contiv, weave or flannel)
+# Choose network plugin (cilium, calico, contiv, weave or flannel)
 # Can also be set to 'cloud', which lets the cloud provider setup appropriate routing
 kube_network_plugin: calico
 
",comment,other,other,コメントのみ編集,コメントの修正
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",.gitlab-ci.yml,MODIFY,2236,"@@ -257,6 +257,10 @@ before_script:
 # stage: deploy-special
   MOVED_TO_GROUP_VARS: ""true""
 
+.ubuntu_cilium_sep_variables: &ubuntu_cilium_sep_variables
+# stage: deploy-special
+  MOVED_TO_GROUP_VARS: ""true""
+
 .rhel7_weave_variables: &rhel7_weave_variables
 # stage: deploy-part1
   MOVED_TO_GROUP_VARS: ""true""
@@ -456,6 +460,17 @@ gce_ubuntu-contiv-sep:
   except: ['triggers']
   only: ['master', /^pr-.*$/]
 
+gce_ubuntu-cilium-sep:
+  stage: deploy-special
+  <<: *job
+  <<: *gce
+  variables:
+    <<: *gce_variables
+    <<: *ubuntu_cilium_sep_variables
+  when: manual
+  except: ['triggers']
+  only: ['master', /^pr-.*$/]
+
 gce_rhel7-weave:
   stage: deploy-part2
   <<: *job
",other,other,other,.gitlab-ci.yml,GitLabのCIの修正
f13e76d022d6143a3307a9a88573207ed6858330,2018-02-17 03:37:47+00:00,"Added cilium support (#2236) * Added cilium support * Fix typo in debian test config * Remove empty lines * Changed cilium version from <latest> to <v1.0.0-rc3> * Add missing changes for cilium * Add cilium to CI pipeline * Fix wrong file name * Check kernel version for cilium * fixed ci error * fixed cilium-ds.j2 template * added waiting for cilium pods to run * Fixed missing EOF * Fixed trailing spaces * Fixed trailing spaces * Fixed trailing spaces * Fixed too many blank lines * Updated tolerations,annotations in cilium DS template * Set cilium_version to iptables-1.9 to see if bug is fixed in CI * Update cilium image tag to v1.0.0-rc4 * Update Cilium test case CI vars filenames * Add optional prometheus flag, adjust initial readiness delay * Update README.md with cilium info",roles/download/defaults/main.yml,MODIFY,2236,"@@ -41,6 +41,7 @@ vault_version: 0.8.1
 weave_version: 2.2.0
 pod_infra_version: 3.0
 contiv_version: 1.1.7
+cilium_version: ""v1.0.0-rc4""
 
 # Download URLs
 istioctl_download_url: ""https://storage.googleapis.com/istio-release/releases/{{ istio_version }}/istioctl/istioctl-linux""
@@ -88,6 +89,8 @@ contiv_image_repo: ""contiv/netplugin""
 contiv_image_tag: ""{{ contiv_version }}""
 contiv_auth_proxy_image_repo: ""contiv/auth_proxy""
 contiv_auth_proxy_image_tag: ""{{ contiv_version }}""
+cilium_image_repo: ""docker.io/cilium/cilium""
+cilium_image_tag: ""{{ cilium_version }}""
 
 nginx_image_repo: nginx
 nginx_image_tag: 1.13
@@ -174,6 +177,12 @@ downloads:
     repo: ""{{ hyperkube_image_repo }}""
     tag: ""{{ hyperkube_image_tag }}""
     sha256: ""{{ hyperkube_digest_checksum|default(None) }}""
+  cilium:
+    enabled: ""{{ kube_network_plugin == 'cilium' }}""
+    container: true
+    repo: ""{{ cilium_image_repo }}""
+    tag: ""{{ cilium_image_tag }}""
+    sha256: ""{{ cilium_digest_checksum|default(None) }}""
   flannel:
     enabled: ""{{ kube_network_plugin == 'flannel' or kube_network_plugin == 'canal' }}""
     container: true
",edit_var,edit_var,edit_var,defaultsディレクトリに含まれるファイルに新しい変数が追加されている,ciliumの依存関係の記述もあるけど、全体的には変数定義かも
c874f16c02b1fce14b57dd244ee1b45fc3083d9b,2018-02-22 12:09:26+00:00,Fixing credential lookup for fe proxy and vault (#2361),roles/vault/defaults/main.yml,MODIFY,2361,"@@ -166,7 +166,7 @@ vault_pki_mounts:
           organization: ""system:node-proxier""
       - name: front-proxy-client
         group: k8s-cluster
-        password: ""{{ lookup('password', 'credentials/vault/kube-proxy length=15') }}""
+        password: ""{{ lookup('password', inventory_dir + '/credentials/vault/kube-proxy length=15') }}""
         policy_rules: default
         role_options:
           allow_any_name: true
",filepath,filepath,filepath,passwordのlookup先のファイルパスが変更される,パスワードを指定したファイルパスから読み込んでいる
31659efe13db773718fb1f08a697f36b4be830f9,2018-02-22 14:37:07+00:00,Fixing cert name in calico/canal for etcd check (#2358),roles/network_plugin/canal/tasks/main.yml,MODIFY,2358,"@@ -35,8 +35,8 @@
   changed_when: false
   run_once: true
   environment:
-    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/node-{{ inventory_hostname }}.pem""
-    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/node-{{ inventory_hostname }}-key.pem""
+    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/node-{{ groups['etcd'][0] }}.pem""
+    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/node-{{ groups['etcd'][0] }}-key.pem""
 
 - name: Canal | Create canal node manifests
   template:
",filepath,filepath,edit_var,機密情報に関するファイル名の変数がinventory_hostname -> groups['etcd'][0]に変更されている,変数の値の作り方の変更
31659efe13db773718fb1f08a697f36b4be830f9,2018-02-22 14:37:07+00:00,Fixing cert name in calico/canal for etcd check (#2358),roles/network_plugin/calico/tasks/main.yml,MODIFY,2358,"@@ -83,8 +83,8 @@
   uri:
     url: https://localhost:2379/health
     validate_certs: no
-    client_cert: ""{{ etcd_cert_dir }}/node-{{ inventory_hostname }}.pem""
-    client_key: ""{{ etcd_cert_dir }}/node-{{ inventory_hostname }}-key.pem""
+    client_cert: ""{{ etcd_cert_dir }}/node-{{ groups['etcd'][0] }}.pem""
+    client_key: ""{{ etcd_cert_dir }}/node-{{ groups['etcd'][0] }}-key.pem""
   register: result
   until: result.status == 200 or result.status == 401
   retries: 10
",filepath,filepath,edit_var,機密情報に関するファイル名の変数がinventory_hostname -> groups['etcd'][0]に変更されている,変数の値の作り方の変更
196995a1a75f768d52558459bd4aff76ed559454,2018-03-12 05:31:31+00:00,Fix issues#2451 Support docker-ce and docker-engine Support docker-ce and docker-engine include redhat/centos ubuntu debian,roles/docker/vars/ubuntu.yml,MODIFY,2451,"@@ -4,10 +4,12 @@ docker_kernel_min_version: '3.10'
 # https://download.docker.com/linux/ubuntu/
 docker_versioned_pkg:
   'latest': docker-ce
-  '1.11': docker-engine=1.11.1-0~{{ ansible_distribution_release|lower }}
+  '1.11': docker-engine=1.11.2-0~{{ ansible_distribution_release|lower }}
   '1.12': docker-engine=1.12.6-0~ubuntu-{{ ansible_distribution_release|lower }}
   '1.13': docker-engine=1.13.1-0~ubuntu-{{ ansible_distribution_release|lower }}
   '17.03': docker-ce=17.03.2~ce-0~ubuntu-{{ ansible_distribution_release|lower }}
+  'stable': docker-ce=17.03.2~ce-0~ubuntu-{{ ansible_distribution_release|lower }}
+  'edge': docker-ce=17.12.1~ce-0~ubuntu-{{ ansible_distribution_release|lower }}
 
 docker_package_info:
   pkg_mgr: apt
@@ -28,3 +30,17 @@ docker_repo_info:
        deb {{ docker_ubuntu_repo_base_url }}
        {{ ansible_distribution_release|lower }}
        stable
+
+dockerproject_repo_key_info:
+  pkg_key: apt_key
+  url: '{{ dockerproject_apt_repo_gpgkey }}'
+  repo_keys:
+    - 58118E89F3A912897C070ADBF76221572C52609D
+
+dockerproject_repo_info:
+  pkg_repo: apt_repository
+  repos:
+    - >
+       deb {{ dockerproject_apt_repo_base_url }}
+       {{ ansible_distribution|lower }}-{{ ansible_distribution_release|lower }}
+       main
",dependency,dependency,dependency,"docker-engineのバージョンを1.11.1-0 -> 1.11.2-0に変更している
stableとedgeのバージョン情報を追加している",全体的にも外部パッケージの依存に関する変更
196995a1a75f768d52558459bd4aff76ed559454,2018-03-12 05:31:31+00:00,Fix issues#2451 Support docker-ce and docker-engine Support docker-ce and docker-engine include redhat/centos ubuntu debian,roles/docker/vars/redhat.yml,MODIFY,2451,"@@ -3,6 +3,7 @@ docker_kernel_min_version: '0'
 
 # https://docs.docker.com/engine/installation/linux/centos/#install-from-a-package
 # https://download.docker.com/linux/centos/7/x86_64/stable/Packages/
+# https://yum.dockerproject.org/repo/main/centos/7
 # or do 'yum --showduplicates list docker-engine'
 docker_versioned_pkg:
   'latest': docker-ce
@@ -11,10 +12,13 @@ docker_versioned_pkg:
   '1.13': docker-engine-1.13.1-1.el7.centos
   '17.03': docker-ce-17.03.2.ce-1.el7.centos
   'stable': docker-ce-17.03.2.ce-1.el7.centos
-  'edge': docker-ce-17.03.2.ce-1.el7.centos
+  'edge': docker-ce-17.12.1.ce-1.el7.centos
 
 docker_selinux_versioned_pkg:
   'latest': docker-ce-selinux
+  '1.11': docker-engine-selinux-1.11.2-1.el7.centos
+  '1.12': docker-engine-selinux-1.12.6-1.el7.centos
+  '1.13': docker-engine-selinux-1.13.1-1.el7.centos
   '17.03': docker-ce-selinux-17.03.2.ce-1.el7.centos
   'stable': docker-ce-selinux-17.03.2.ce-1.el7.centos
   'edge': docker-ce-selinux-17.03.2.ce-1.el7.centos
",dependency,dependency,dependency,dockerのバージョン情報を追加している,全体的にも外部パッケージの依存に関する変更
196995a1a75f768d52558459bd4aff76ed559454,2018-03-12 05:31:31+00:00,Fix issues#2451 Support docker-ce and docker-engine Support docker-ce and docker-engine include redhat/centos ubuntu debian,roles/docker/vars/debian.yml,MODIFY,2451,"@@ -2,6 +2,7 @@
 docker_kernel_min_version: '3.10'
 
 # https://download.docker.com/linux/debian/
+# https://apt.dockerproject.org/repo/dists/debian-wheezy/main/filelist
 docker_versioned_pkg:
   'latest': docker-ce
   '1.11': docker-engine=1.11.2-0~{{ ansible_distribution_release|lower }}
@@ -30,3 +31,17 @@ docker_repo_info:
        deb {{ docker_debian_repo_base_url }}
        {{ ansible_distribution_release|lower }}
        stable
+
+dockerproject_repo_key_info:
+  pkg_key: apt_key
+  url: '{{ dockerproject_apt_repo_gpgkey }}'
+  repo_keys:
+    - 58118E89F3A912897C070ADBF76221572C52609D
+
+dockerproject_repo_info:
+  pkg_repo: apt_repository
+  repos:
+    - >
+       deb {{ dockerproject_apt_repo_base_url }}
+       {{ ansible_distribution|lower }}-{{ ansible_distribution_release|lower }}
+       main
",dependency,edit_var,dependency,dockerproject_repo_key_infoとdockerproject_repo_infoという名前の変数を追加している,全体的にも外部パッケージの依存に関する変更
196995a1a75f768d52558459bd4aff76ed559454,2018-03-12 05:31:31+00:00,Fix issues#2451 Support docker-ce and docker-engine Support docker-ce and docker-engine include redhat/centos ubuntu debian,roles/docker/tasks/main.yml,MODIFY,2451,"@@ -30,7 +30,7 @@
   tags:
     - facts
 
-- name: ensure docker repository public key is installed
+- name: ensure docker-ce repository public key is installed
   action: ""{{ docker_repo_key_info.pkg_key }}""
   args:
     id: ""{{item}}""
@@ -41,15 +41,36 @@
   retries: 4
   delay: ""{{ retry_stagger | random + 3 }}""
   with_items: ""{{ docker_repo_key_info.repo_keys }}""
-  when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS""] or is_atomic)
+  when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS"", ""RedHat""] or is_atomic)
 
-- name: ensure docker repository is enabled
+- name: ensure docker-ce repository is enabled
   action: ""{{ docker_repo_info.pkg_repo }}""
   args:
     repo: ""{{item}}""
     state: present
   with_items: ""{{ docker_repo_info.repos }}""
-  when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS""] or is_atomic) and (docker_repo_info.repos|length > 0)
+  when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS"", ""RedHat""] or is_atomic) and (docker_repo_info.repos|length > 0)
+
+- name: ensure docker-engine repository public key is installed
+  action: ""{{ dockerproject_repo_key_info.pkg_key }}""
+  args:
+    id: ""{{item}}""
+    url: ""{{dockerproject_repo_key_info.url}}""
+    state: present
+  register: keyserver_task_result
+  until: keyserver_task_result|succeeded
+  retries: 4
+  delay: ""{{ retry_stagger | random + 3 }}""
+  with_items: ""{{ dockerproject_repo_key_info.repo_keys }}""
+  when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS"", ""RedHat""] or is_atomic)
+
+- name: ensure docker-engine repository is enabled
+  action: ""{{ dockerproject_repo_info.pkg_repo }}""
+  args:
+    repo: ""{{item}}""
+    state: present
+  with_items: ""{{ dockerproject_repo_info.repos }}""
+  when: not (ansible_os_family in [""CoreOS"", ""Container Linux by CoreOS"", ""RedHat""] or is_atomic) and (dockerproject_repo_info.repos|length > 0)
 
 - name: Configure docker repository on RedHat/CentOS
   template:
",add_task,add_task,add_task,"変更のカテゴリに競合がある

- rename
docker -> docker-ce

- condition
RedHatの場合も実行するように変更

- add_task
今回の変更で追加したdockerの情報を元にdockerのインストールを確認するようなタスクが追加されている",whenの変更はあるけど、タスク追加の影響が大きい
196995a1a75f768d52558459bd4aff76ed559454,2018-03-12 05:31:31+00:00,Fix issues#2451 Support docker-ce and docker-engine Support docker-ce and docker-engine include redhat/centos ubuntu debian,roles/docker/defaults/main.yml,MODIFY,2451,"@@ -11,6 +11,12 @@ docker_repo_key_info:
 docker_repo_info:
   repos:
 
+dockerproject_repo_key_info:
+  repo_keys:
+
+dockerproject_repo_info:
+  repos:
+
 docker_dns_servers_strict: yes
 
 docker_container_storage_setup: false
@@ -24,3 +30,8 @@ docker_ubuntu_repo_gpgkey: 'https://download.docker.com/linux/ubuntu/gpg'
 #Debian docker-ce repo
 docker_debian_repo_base_url: ""https://download.docker.com/linux/debian""
 docker_debian_repo_gpgkey: 'https://download.docker.com/linux/debian/gpg'
+#dockerproject repo
+dockerproject_rh_repo_base_url: 'https://yum.dockerproject.org/repo/main/centos/7'
+dockerproject_rh_repo_gpgkey: 'https://yum.dockerproject.org/gpg'
+dockerproject_apt_repo_base_url: 'https://apt.dockerproject.org/repo'
+dockerproject_apt_repo_gpgkey: 'https://apt.dockerproject.org/gpg'
",edit_var,edit_var,edit_var,dockerprojectに関する変数の追加,デフォルトのリポジトリURLを追加
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/gen_certs_script.yml,MODIFY,None,"@@ -15,6 +15,7 @@
     owner: root
     mode: 0700
   run_once: yes
+  when: inventory_hostname == groups['etcd'][0]
   delegate_to: ""{{groups['etcd'][0]}}""
 
 - name: ""Gen_certs | create etcd cert dir (on {{groups['etcd'][0]}})""
@@ -26,6 +27,7 @@
     recurse: yes
     mode: 0700
   run_once: yes
+  when: inventory_hostname == groups['etcd'][0]
   delegate_to: ""{{groups['etcd'][0]}}""
 
 - name: Gen_certs | write openssl config
@@ -34,7 +36,9 @@
     dest: ""{{ etcd_config_dir }}/openssl.conf""
   run_once: yes
   delegate_to: ""{{groups['etcd'][0]}}""
-  when: gen_certs|default(false)
+  when:
+    - gen_certs|default(false)
+    - inventory_hostname == groups['etcd'][0]
 
 - name: Gen_certs | copy certs generation script
   copy:
@@ -43,8 +47,9 @@
     mode: 0700
   run_once: yes
   delegate_to: ""{{groups['etcd'][0]}}""
-  when: gen_certs|default(false)
-
+  when:
+    - gen_certs|default(false)
+    - inventory_hostname == groups['etcd'][0]
 
 - name: Gen_certs | run cert generation script
   command: ""bash -x {{ etcd_script_dir }}/make-ssl-etcd.sh -f {{ etcd_config_dir }}/openssl.conf -d {{ etcd_cert_dir }}""
@@ -61,7 +66,9 @@
               {% endfor %}""
   run_once: yes
   delegate_to: ""{{groups['etcd'][0]}}""
-  when: gen_certs|default(false)
+  when:
+    - gen_certs|default(false)
+    - inventory_hostname == groups['etcd'][0]
   notify: set etcd_secret_changed
 
 - set_fact:
@@ -160,5 +167,5 @@
     group: ""{{ etcd_cert_group }}""
     state: directory
     owner: kube
-    mode: ""u=rwX,g-rwx,o-rwx""
+    mode: ""640""
     recurse: yes
",condition,condition,condition,"when条件が新たに追加されたもの，
- inventory_hostname
とファイルのパーミッションについて表記が変更されている
when条件の変更が主な変更であるため",modeの変更もあるけど、whenの変更が多い
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/main.yml,MODIFY,None,"@@ -6,6 +6,7 @@
     - facts
 
 - include_tasks: ""gen_certs_{{ cert_management }}.yml""
+  when:
   tags:
     - etcd-secrets
 
@@ -29,47 +30,28 @@
   tags:
     - upgrade
 
-- include_tasks: set_cluster_health.yml
-  when: is_etcd_master and etcd_cluster_setup
-
 - include_tasks: configure.yml
-  when: is_etcd_master and etcd_cluster_setup
+  when: is_etcd_master
 
 - include_tasks: refresh_config.yml
-  when: is_etcd_master and etcd_cluster_setup
+  when: is_etcd_master
 
 - name: Restart etcd if certs changed
-  command: /bin/true
-  notify: restart etcd
-  when: is_etcd_master and etcd_secret_changed|default(false)
-
-- name: Restart etcd-events if certs changed
-  command: /bin/true
-  notify: restart etcd
-  when: is_etcd_master and etcd_events_cluster_setup and etcd_secret_changed|default(false)
-
-# reload-systemd
-- meta: flush_handlers
-
-- name: Ensure etcd is running
   service:
     name: etcd
-    state: started
+    state: restarted
     enabled: yes
-  when: is_etcd_master and etcd_cluster_setup
+  when: is_etcd_master and etcd_cluster_setup and etcd_secret_changed|default(false)
 
-- name: Ensure etcd-events is running
+- name: Restart etcd-events if certs changed
   service:
     name: etcd-events
-    state: started
+    state: restarted
     enabled: yes
-  when: is_etcd_master and etcd_events_cluster_setup
+  when: is_etcd_master and etcd_events_cluster_setup and etcd_secret_changed|default(false)
 
 # After etcd cluster is assembled, make sure that
 # initial state of the cluster is in `existing`
 # state insted of `new`.
-- include_tasks: set_cluster_health.yml
-  when: is_etcd_master and etcd_cluster_setup
-
 - include_tasks: refresh_config.yml
-  when: is_etcd_master and etcd_cluster_setup
+  when: is_etcd_master
",condition,condition,condition,"複数の変更カテゴリに該当するが，全体を通してwhenの条件に変更が加えられているため

- remove_task
ある条件の時だけnotifyだけを行うタスクの削除

name: Restart etcd-events...
command: /bin/true
notify: restart etcd
when: foo is bar

サービスの起動を保証するタスクを削除し，restartするタスクだけに変更

- edit_args 
serviceモジュールのstateを started -> restartedに変更
",タスクの削除に伴い、whenの条件の変更という感じかな。remove_taskかconditionか悩む。
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/refresh_config.yml,MODIFY,None,"@@ -4,7 +4,7 @@
     src: etcd.env.j2
     dest: /etc/etcd.env
   notify: restart etcd
-  when: is_etcd_master
+  when: is_etcd_master and etcd_cluster_setup
 
 - name: Refresh config | Create etcd-events config file
   template:
",condition,condition,condition,,条件追加
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/join_member.yml,DELETE,None,"@@ -1,47 +0,0 @@
----
-- name: Join Member | Add member to cluster
-  shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_access_addresses }} member add {{ etcd_member_name }} {{ etcd_peer_url }}""
-  register: member_add_result
-  until: member_add_result.rc == 0
-  retries: 4
-  delay: ""{{ retry_stagger | random + 3 }}""
-  when: target_node == inventory_hostname
-  environment:
-    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
-    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
-
-- include_tasks: refresh_config.yml
-  vars:
-    etcd_peer_addresses: >-
-      {% for host in groups['etcd'] -%}
-        {%- if hostvars[host]['etcd_member_in_cluster'].rc == 0 -%}
-          {{ ""etcd""+loop.index|string }}=https://{{ hostvars[host].access_ip | default(hostvars[host].ip | default(hostvars[host].ansible_default_ipv4['address'])) }}:2380,
-        {%- endif -%}
-        {%- if loop.last -%}
-          {{ etcd_member_name }}={{ etcd_peer_url }}
-        {%- endif -%}
-      {%- endfor -%}
-  when: target_node == inventory_hostname
-
-- name: Join Member | reload systemd
-  command: systemctl daemon-reload
-  when: target_node == inventory_hostname
-
-- name: Join Member | Ensure etcd is running
-  service:
-    name: etcd
-    state: started
-    enabled: yes
-  when: target_node == inventory_hostname
-
-- name: Join Member | Ensure member is in cluster
-  shell: ""{{ bin_dir }}/etcdctl --no-sync --endpoints={{ etcd_access_addresses }} member list | grep -q {{ etcd_access_address }}""
-  register: etcd_member_in_cluster
-  changed_when: false
-  check_mode: no
-  tags:
-    - facts
-  when: target_node == inventory_hostname
-  environment:
-    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
-    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
",remove_task,remove_task,remove_task,コミットメッセージにもあるように'join_members.yml'は使われないため削除された,コミットメッセージから、このファイルは使われないことが書かれているので。もしかしたら97行目のファイルに移行しているのかもしれない
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/join_etcd_member.yml,MODIFY,None,"@@ -1,5 +1,5 @@
 ---
-- name: Join Member | Add member to cluster
+- name: Join Member | Add member to etcd cluster
   shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_access_addresses }} member add {{ etcd_member_name }} {{ etcd_peer_url }}""
   register: member_add_result
   until: member_add_result.rc == 0
@@ -23,18 +23,7 @@
       {%- endfor -%}
   when: target_node == inventory_hostname
 
-- name: Join Member | reload systemd
-  command: systemctl daemon-reload
-  when: target_node == inventory_hostname
-
-- name: Join Member | Ensure etcd is running
-  service:
-    name: etcd
-    state: started
-    enabled: yes
-  when: target_node == inventory_hostname
-
-- name: Join Member | Ensure member is in cluster
+- name: Join Member | Ensure member is in etcd cluster
   shell: ""{{ bin_dir }}/etcdctl --no-sync --endpoints={{ etcd_access_addresses }} member list | grep -q {{ etcd_access_address }}""
   register: etcd_member_in_cluster
   changed_when: false
",move,remove_task,move,"Join Member | reload systemd, Join Member | Ensure etcd is runningが削除されている
ほか二つのタスクも名前が変更されている",97行目のファイルへ移行
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/join_etcd-events_member.yml,MODIFY,None,"@@ -1,5 +1,5 @@
 ---
-- name: Join Member | Add member to cluster
+- name: Join Member | Add member to etcd-events cluster
   shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_events_access_addresses }} member add {{ etcd_member_name }} {{ etcd_events_peer_url }}""
   register: member_add_result
   until: member_add_result.rc == 0
@@ -23,17 +23,6 @@
       {%- endfor -%}
   when: target_node == inventory_hostname
 
-- name: Join Member | reload systemd
-  command: systemctl daemon-reload
-  when: target_node == inventory_hostname
-
-- name: Join Member | Ensure etcd-events is running
-  service:
-    name: etcd-events
-    state: started
-    enabled: yes
-  when: target_node == inventory_hostname
-
 - name: Join Member | Ensure member is in etcd-events cluster
   shell: ""{{ bin_dir }}/etcdctl --no-sync --endpoints={{ etcd_events_access_addresses }} member list | grep -q {{ etcd_events_access_address }}""
   register: etcd_events_member_in_cluster
",move,remove_task,move,"Join Member | reload systemd, Join Member | Ensure etcd is runningが削除されている",97行目のファイルへ移行
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/install_rkt.yml,MODIFY,None,"@@ -11,6 +11,7 @@
   delay: ""{{ retry_stagger | random + 3 }}""
   changed_when: false
   environment: ""{{proxy_env}}""
+  when: etcd_cluster_setup
 
 - name: Install | Copy etcdctl binary from rkt container
   command: >-
@@ -26,3 +27,4 @@
   delay: ""{{ retry_stagger | random + 3 }}""
   changed_when: false
   environment: ""{{proxy_env}}""
+  when: etcd_cluster_setup
",condition,condition,condition,when: etcd_cluster_setupが追加されている,whenの追加
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/install_host.yml,MODIFY,None,"@@ -10,3 +10,4 @@
   retries: 4
   delay: ""{{ retry_stagger | random + 3 }}""
   changed_when: false
+  when: etcd_cluster_setup
",condition,condition,condition,when: etcd_cluster_setupが追加されている,whsnの追加
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/install_docker.yml,MODIFY,None,"@@ -9,22 +9,22 @@
   retries: 4
   delay: ""{{ retry_stagger | random + 3 }}""
   changed_when: false
+  when: etcd_cluster_setup
 
 - name: Install etcd launch script
   template:
     src: etcd.j2
     dest: ""{{ bin_dir }}/etcd""
     owner: 'root'
-    mode: 0755
+    mode: 0750
     backup: yes
-  notify: restart etcd
+  when: etcd_cluster_setup
 
 - name: Install etcd-events launch script
   template:
     src: etcd-events.j2
     dest: ""{{ bin_dir }}/etcd-events""
     owner: 'root'
-    mode: 0755
+    mode: 0750
     backup: yes
   when: etcd_events_cluster_setup
-  notify: restart etcd-events
",condition,permission,condition,ファイルmode: 0755 -> 0700に変更されている,fileのモードよりもwhenの方が影響大きいそう
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/set_cluster_health.yml,DELETE,None,"@@ -1,26 +0,0 @@
----
-- name: Configure | Check if etcd cluster is healthy
-  shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_access_addresses }} cluster-health | grep -q 'cluster is healthy'""
-  register: etcd_cluster_is_healthy
-  ignore_errors: true
-  changed_when: false
-  check_mode: no
-  when: is_etcd_master
-  tags:
-    - facts
-  environment:
-    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
-    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
-
-- name: Configure | Check if etcd-events cluster is healthy
-  shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_events_access_addresses }} cluster-health | grep -q 'cluster is healthy'""
-  register: etcd_events_cluster_is_healthy
-  ignore_errors: true
-  changed_when: false
-  check_mode: no
-  when: is_etcd_master and etcd_events_cluster_setup
-  tags:
-    - facts
-  environment:
-    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
-    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
",move,remove_task,move,ファイルそのものが削除されている,97行目のファイルに移行
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/handlers/main.yml,MODIFY,None,"@@ -10,7 +10,7 @@
 - name: restart etcd-events
   command: /bin/true
   notify:
-    - etcd-events | reload systemd
+    - etcd | reload systemd
     - reload etcd-events
     - wait for etcd-events up
 
@@ -19,9 +19,6 @@
 - name: etcd | reload systemd
   command: systemctl daemon-reload
 
-- name: etcd-events | reload systemd
-  command: systemctl daemon-reload
-
 - name: reload etcd
   service:
     name: etcd
",remove_task,remove_task,remove_task,重複するタスクが削除され，それに伴うnotify先の変更が行われている,タスク削除に伴うnotify先の変化なので、これはタスク削除
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/tasks/configure.yml,MODIFY,None,"@@ -1,20 +1,20 @@
 ---
-- name: Configure | Check if member is in etcd cluster
-  shell: ""{{ bin_dir }}/etcdctl --no-sync --endpoints={{ etcd_access_addresses }} member list | grep -q {{ etcd_access_address }}""
-  register: etcd_member_in_cluster
+- name: Configure | Check if etcd cluster is healthy
+  shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_access_addresses }} cluster-health | grep -q 'cluster is healthy'""
+  register: etcd_cluster_is_healthy
   ignore_errors: true
   changed_when: false
   check_mode: no
-  when: is_etcd_master
+  when: is_etcd_master and etcd_cluster_setup
   tags:
     - facts
   environment:
     ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
     ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
 
-- name: Configure | Check if member is in etcd-events cluster
-  shell: ""{{ bin_dir }}/etcdctl --no-sync --endpoints={{ etcd_events_access_addresses }} member list | grep -q {{ etcd_access_address }}""
-  register: etcd_events_member_in_cluster
+- name: Configure | Check if etcd-events cluster is healthy
+  shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_events_access_addresses }} cluster-health | grep -q 'cluster is healthy'""
+  register: etcd_events_cluster_is_healthy
   ignore_errors: true
   changed_when: false
   check_mode: no
@@ -25,44 +25,109 @@
     ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
     ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
 
+- include_tasks: refresh_config.yml
+  when: is_etcd_master
+
 - name: Configure | Copy etcd.service systemd file
   template:
     src: ""etcd-{{ etcd_deployment_type }}.service.j2""
     dest: /etc/systemd/system/etcd.service
     backup: yes
-  when: is_etcd_master
-  notify: restart etcd
+  when: is_etcd_master and etcd_cluster_setup
 
 - name: Configure | Copy etcd-events.service systemd file
   template:
-    src: ""etcd-events-host.service.j2""
+    src: ""etcd-events-{{ etcd_deployment_type }}.service.j2""
     dest: /etc/systemd/system/etcd-events.service
     backup: yes
-  when: is_etcd_master and etcd_deployment_type == ""host"" and etcd_events_cluster_setup
-  notify: restart etcd-events
+  when: is_etcd_master and etcd_events_cluster_setup
 
-- name: Configure | Copy etcd-events.service systemd file
-  template:
-    src: ""etcd-events-docker.service.j2""
-    dest: /etc/systemd/system/etcd-events.service
-    backup: yes
-  when: is_etcd_master and etcd_deployment_type == ""docker"" and etcd_events_cluster_setup
-  notify: restart etcd-events
+- name: Configure | reload systemd
+  command: systemctl daemon-reload
+  when: is_etcd_master
+
+- name: Configure | Ensure etcd is running
+  service:
+    name: etcd
+    state: started
+    enabled: yes
+  when: is_etcd_master and etcd_cluster_setup
+
+- name: Configure | Ensure etcd-events is running
+  service:
+    name: etcd-events
+    state: started
+    enabled: yes
+  when: is_etcd_master and etcd_events_cluster_setup
+
+- name: Configure | Check if etcd cluster is healthy
+  shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_access_addresses }} cluster-health | grep -q 'cluster is healthy'""
+  register: etcd_cluster_is_healthy
+  until: etcd_cluster_is_healthy.rc == 0
+  retries: 4
+  delay: ""{{ retry_stagger | random + 3 }}""
+  ignore_errors: false
+  changed_when: false
+  check_mode: no
+  when: is_etcd_master and etcd_cluster_setup
+  tags:
+    - facts
+  environment:
+    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
+    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
+
+- name: Configure | Check if etcd-events cluster is healthy
+  shell: ""{{ bin_dir }}/etcdctl --endpoints={{ etcd_events_access_addresses }} cluster-health | grep -q 'cluster is healthy'""
+  register: etcd_events_cluster_is_healthy
+  until: etcd_events_cluster_is_healthy.rc == 0
+  retries: 4
+  delay: ""{{ retry_stagger | random + 3 }}""
+  ignore_errors: false
+  changed_when: false
+  check_mode: no
+  when: is_etcd_master and etcd_events_cluster_setup
+  tags:
+    - facts
+  environment:
+    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
+    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
+
+- name: Configure | Check if member is in etcd cluster
+  shell: ""{{ bin_dir }}/etcdctl --no-sync --endpoints={{ etcd_access_addresses }} member list | grep -q {{ etcd_access_address }}""
+  register: etcd_member_in_cluster
+  ignore_errors: true
+  changed_when: false
+  check_mode: no
+  when: is_etcd_master and etcd_cluster_setup
+  tags:
+    - facts
+  environment:
+    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
+    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
+
+- name: Configure | Check if member is in etcd-events cluster
+  shell: ""{{ bin_dir }}/etcdctl --no-sync --endpoints={{ etcd_events_access_addresses }} member list | grep -q {{ etcd_access_address }}""
+  register: etcd_events_member_in_cluster
+  ignore_errors: true
+  changed_when: false
+  check_mode: no
+  when: is_etcd_master and etcd_events_cluster_setup
+  tags:
+    - facts
+  environment:
+    ETCDCTL_CERT_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}.pem""
+    ETCDCTL_KEY_FILE: ""{{ etcd_cert_dir }}/admin-{{ inventory_hostname }}-key.pem""
 
 - name: Configure | Join member(s) to etcd cluster one at a time
   include_tasks: join_etcd_member.yml
   vars:
     target_node: ""{{ item }}""
-  loop_control:
-    pause: 10
   with_items: ""{{ groups['etcd'] }}""
-  when: inventory_hostname == item and etcd_member_in_cluster.rc != 0 and etcd_cluster_is_healthy.rc == 0
+  when: inventory_hostname == item and etcd_cluster_setup and etcd_member_in_cluster.rc != 0 and etcd_cluster_is_healthy.rc == 0
 
 - name: Configure | Join member(s) to etcd-events cluster one at a time
-  include_tasks: join_etcd-evetns_member.yml
+  include_tasks: join_etcd-events_member.yml
   vars:
     target_node: ""{{ item }}""
-  loop_control:
-    pause: 10
   with_items: ""{{ groups['etcd'] }}""
   when: inventory_hostname == item and etcd_events_cluster_setup and etcd_events_member_in_cluster.rc != 0 and etcd_events_cluster_is_healthy.rc == 0
",move,command,move,"タスク名の変更に伴いetcdコマンドの引数に変更がある
- --no-sync
- member list
+ cluster-health
挙動が変更されるのでconditionを適応した","90,91,95行目のファイルから移行されてきたものがある"
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",cluster.yml,MODIFY,None,"@@ -51,13 +51,13 @@
   any_errors_fatal: ""{{ any_errors_fatal | default(true) }}""
   roles:
     - { role: kubespray-defaults}
-    - { role: etcd, tags: etcd, etcd_cluster_setup: true }
+    - { role: etcd, tags: etcd }
 
 - hosts: k8s-cluster:calico-rr
   any_errors_fatal: ""{{ any_errors_fatal | default(true) }}""
   roles:
     - { role: kubespray-defaults}
-    - { role: etcd, tags: etcd, etcd_cluster_setup: false }
+    - { role: etcd, tags: etcd }
 
 - hosts: etcd:k8s-cluster:vault:calico-rr
   any_errors_fatal: ""{{ any_errors_fatal | default(true) }}""
",edit_args,tags,edit_args,rolesで指定されるtag情報に変更があるため,roleを実行する際のetcd_cluster_setupの変数を変更している。var:を省略できるっぽい。
86e3506ae637e2050f953cd85cda5ca16a12ce6f,2018-04-01 19:38:33+00:00,"Etcd cluster setup makeover The current way to setup the etc cluster is messy and buggy. - It checks for cluster is healthy before the cluster is even created. - The unit files are started on handlers, not in the task, so you mess with ""flush handlers"". - The join_member.yml is not used. - etcd events cluster is not configured for kubeadm - remove duplicate runs between running the role on etcd nodes and k8s nodes",roles/etcd/defaults/main.yml,MODIFY,None,"@@ -1,6 +1,7 @@
 ---
 # Set to false to only do certificate management
 etcd_cluster_setup: true
+etcd_events_cluster_setup: false
 
 etcd_backup_prefix: ""/var/backups""
 etcd_data_dir: ""/var/lib/etcd""
",edit_var,edit_var,edit_var,新しいデフォルト変数が追加される,変数定義
7df5edef52db25ad8e6997ef83c7cbc767a109ee,2018-05-11 14:01:52+00:00,Fix path for pip and python,roles/bootstrap-os/tasks/bootstrap-coreos.yml,MODIFY,None,"@@ -12,7 +12,6 @@
     bin_dir: ""/opt/bin""
   tags:
     - facts
-  when: need_bootstrap.rc != 0
 
 - name: Bootstrap | Run bootstrap.sh
   script: bootstrap.sh
",condition,condition,condition,whenディレクティブが削除される,whenの削除
fefa1670a6899034feffde7cd7db7d5b06e6ec59,2018-11-12 15:35:21+00:00,fix calico_version wrong get (#3694) the ':' makes wrong return of calico_version after the calicoctl downloaded && before the cluster is up,roles/kubernetes/preinstall/tasks/0020-verify-settings.yml,MODIFY,3694,"@@ -137,7 +137,7 @@
     - facts
 
 - name: ""Get current version of calico cluster version""
-  shell: ""{{ bin_dir }}/calicoctl version  | grep 'Cluster Version' | awk '{ print $3}'""
+  shell: ""{{ bin_dir }}/calicoctl version  | grep 'Cluster Version:' | awk '{ print $3}'""
   register: calico_version_on_server
   run_once: yes
   delegate_to: ""{{ groups['kube-master'][0] }}""
",command,command,command,grepで検索する文字列の変更,:が追加されている
5834e609a6a9dda82e6cf1d52a02f7530fb1cd5f,2018-12-28 07:27:27+00:00,Add scale master features (#3946) * Add scale master features * Add certificate management with kubeadm * Add kubeadm kubeconfig * Fix ymalroles error * fix upgrade cluster fialed * force update cert and keys when you reconfigure cluster,roles/download/tasks/kubeadm_images.yml,MODIFY,3946,"@@ -1,3 +1,4 @@
+---
 - name: kubeadm | Create kubeadm config
   template:
     src: ""kubeadm-images.yaml.j2""
",no_effect,directive,other,blockディレクティブの追加,yml的な修正
5834e609a6a9dda82e6cf1d52a02f7530fb1cd5f,2018-12-28 07:27:27+00:00,Add scale master features (#3946) * Add scale master features * Add certificate management with kubeadm * Add kubeadm kubeconfig * Fix ymalroles error * fix upgrade cluster fialed * force update cert and keys when you reconfigure cluster,roles/kubernetes/master/tasks/kubeadm-certificate.yml,ADD,3946,"@@ -0,0 +1,42 @@
+---
+- name: Backup old certs and keys
+  copy:
+    src: ""{{ kube_cert_dir }}/{{ item.src }}""
+    dest: ""{{ kube_cert_dir }}/{{ item.dest }}""
+    remote_src: yes
+  with_items:
+    - {src: apiserver.crt, dest: apiserver.crt.old}
+    - {src: apiserver.key, dest: apiserver.key.old}
+    - {src: apiserver-kubelet-client.crt, dest: apiserver-kubelet-client.crt.old}
+    - {src: apiserver-kubelet-client.key, dest: apiserver-kubelet-client.key.old}
+    - {src: front-proxy-client.crt, dest: front-proxy-client.crt.old}
+    - {src: front-proxy-client.key, dest: front-proxy-client.key.old}
+  ignore_errors: yes
+
+- name: Remove old certs and keys
+  file:
+    path: ""{{ kube_cert_dir }}/{{ item }}""
+    state: absent
+  with_items:
+    - apiserver.crt
+    - apiserver.key
+    - apiserver-kubelet-client.crt
+    - apiserver-kubelet-client.key
+    - front-proxy-client.crt
+    - front-proxy-client.key
+
+- name: Generate new certs and keys
+  command: ""{{ bin_dir }}/kubeadm init phase certs {{ item }} --config={{ kube_config_dir }}/kubeadm-config.yaml""
+  with_items:
+    - apiserver
+    - apiserver-kubelet-client
+    - front-proxy-client
+  when: inventory_hostname == groups['kube-master']|first and kubeadm_version is version('v1.13.0', '>=')
+
+- name: Generate new certs and keys
+  command: ""{{ bin_dir }}/kubeadm alpha phase certs {{ item }} --config={{ kube_config_dir }}/kubeadm-config.yaml""
+  with_items:
+    - apiserver
+    - apiserver-kubelet-client
+    - front-proxy-client
+  when: inventory_hostname == groups['kube-master']|first and kubeadm_version is version('v1.13.0', '<')
",add_task,add_task,add_task,新規ファイルとそのタスク,タスク追加
5834e609a6a9dda82e6cf1d52a02f7530fb1cd5f,2018-12-28 07:27:27+00:00,Add scale master features (#3946) * Add scale master features * Add certificate management with kubeadm * Add kubeadm kubeconfig * Fix ymalroles error * fix upgrade cluster fialed * force update cert and keys when you reconfigure cluster,roles/kubernetes/master/tasks/kubeadm-kubeconfig.yml,ADD,3946,"@@ -0,0 +1,32 @@
+---
+- name: Backup old configuration files
+  copy:
+    src: ""{{ kube_config_dir }}/{{ item.src }}""
+    dest: ""{{ kube_config_dir }}/{{ item.dest }}""
+    remote_src: yes
+  with_items:
+    - {src: admin.conf, dest: admin.conf.old}
+    - {src: kubelet.conf, dest: kubelet.conf.old}
+    - {src: controller-manager.conf, dest: controller-manager.conf.old}
+    - {src: scheduler.conf, dest: scheduler.conf.old}
+  ignore_errors: yes
+
+- name: Remove old configuration files
+  file:
+    path: ""{{ kube_config_dir }}/{{ item }}""
+    state: absent
+  with_items:
+    - admin.conf
+    - kubelet.conf
+    - controller-manager.conf
+    - scheduler.conf
+
+- name: Generate new configuration files
+  command: ""{{ bin_dir }}/kubeadm init phase kubeconfig all --config={{ kube_config_dir }}/kubeadm-config.yaml""
+  when: kubeadm_version is version('v1.13.0', '>=')
+  ignore_errors: yes
+
+- name: Generate new configuration files
+  command: ""{{ bin_dir }}/kubeadm alpha phase kubeconfig all --config={{ kube_config_dir }}/kubeadm-config.yaml""
+  when: kubeadm_version is version('v1.13.0', '<')
+  ignore_errors: yes
",add_task,add_task,add_task,新規ファイルとそのタスク,タスク追加
5834e609a6a9dda82e6cf1d52a02f7530fb1cd5f,2018-12-28 07:27:27+00:00,Add scale master features (#3946) * Add scale master features * Add certificate management with kubeadm * Add kubeadm kubeconfig * Fix ymalroles error * fix upgrade cluster fialed * force update cert and keys when you reconfigure cluster,roles/kubernetes/master/tasks/kubeadm-setup.yml,MODIFY,3946,"@@ -10,10 +10,10 @@
   import_tasks: kubeadm-migrate-certs.yml
   when: old_apiserver_cert.stat.exists
 
-- name: kubeadm | Check service account key
+- name: kubeadm | Check apiserver key
   stat:
-    path: ""{{ kube_cert_dir }}/sa.key""
-  register: sa_key_before
+    path: ""{{ kube_cert_dir }}/apiserver.key""
+  register: apiserver_key_before
   delegate_to: ""{{groups['kube-master']|first}}""
   run_once: true
 
@@ -95,6 +95,12 @@
 - name: kubeadm | set kubeadm version
   import_tasks: kubeadm-version.yml
 
+- name: kubeadm | Certificate management with kubeadm
+  import_tasks: kubeadm-certificate.yml
+  when:
+    - not upgrade_cluster_setup
+    - kubeadm_already_run.stat.exists
+
 - name: kubeadm | Initialize first master
   command: timeout -k 600s 600s {{ bin_dir }}/kubeadm init --config={{ kube_config_dir }}/kubeadm-config.yaml --ignore-preflight-errors=all
   register: kubeadm_init
@@ -136,6 +142,12 @@
   with_items: ""{{ kubeadm_certs.results }}""
   when: inventory_hostname != groups['kube-master']|first
 
+- name: kubeadm | Kubeconfig management with kubeadm
+  import_tasks: kubeadm-kubeconfig.yml
+  when:
+    - not upgrade_cluster_setup
+    - kubeadm_already_run.stat.exists
+
 - name: kubeadm | Init other uninitialized masters
   command: timeout -k 600s 600s {{ bin_dir }}/kubeadm init --config={{ kube_config_dir }}/kubeadm-config.yaml --ignore-preflight-errors=all
   register: kubeadm_init
@@ -149,17 +161,17 @@
   import_tasks: kubeadm-upgrade.yml
   when: upgrade_cluster_setup
 
-- name: kubeadm | Check service account key again
+- name: kubeadm | Check apiserver key again
   stat:
-    path: ""{{ kube_cert_dir }}/sa.key""
-  register: sa_key_after
+    path: ""{{ kube_cert_dir }}/apiserver.key""
+  register: apiserver_key_after
   delegate_to: ""{{groups['kube-master']|first}}""
   run_once: true
 
 - name: kubeadm | Set secret_changed if service account key was updated
   command: /bin/true
   notify: Master | set secret_changed
-  when: sa_key_before.stat.checksum|default("""") != sa_key_after.stat.checksum
+  when: apiserver_key_before.stat.checksum|default("""") != apiserver_key_after.stat.checksum
 
 - name: kubeadm | cleanup old certs if necessary
   import_tasks: kubeadm-cleanup-old-certs.yml
",include,filepath,,"service account -> apiserver へのアップグレード？による変更が行われている
変更の原因として管理対象がsa(service accout).keyからapiserver.keyに変更されたことだと考えられる","ファイルパスの修正と、103,104行目のファイルのタスクをimportしている"
eecaba6b844987645703a926df5f4432a7ede1c7,2019-01-16 13:30:50+00:00,Generate external admin.conf with kubeadm (#4056) * Generate external admin.conf with kubeadm * Fix apiserver sans,roles/kubernetes/master/tasks/kubeadm-setup.yml,MODIFY,4056,"@@ -56,11 +56,11 @@
       {{ ' '.join(groups['kube-master']) }}
       {%- if loadbalancer_apiserver is defined %}
       {{ apiserver_loadbalancer_domain_name }}
-      {%- endif %}
+      {% endif %}
       {% for host in groups['kube-master'] -%}
       {%- if hostvars[host]['access_ip'] is defined -%}
       {{ hostvars[host]['access_ip'] }}
-      {%- endif %}
+      {% endif %}
       {{ hostvars[host]['ip'] | default(hostvars[host]['ansible_default_ipv4']['address']) }}
       {%- endfor %}
       {%- if supplementary_addresses_in_ssl_keys is defined -%}
",jinja2,jinja2,other,"-' によってwhitespaceの処理が異なる
https://jinja.palletsprojects.com/en/3.1.x/templates/#whitespace-control
",jinjaのtemplateの出力結果の空白の取り除き
eecaba6b844987645703a926df5f4432a7ede1c7,2019-01-16 13:30:50+00:00,Generate external admin.conf with kubeadm (#4056) * Generate external admin.conf with kubeadm * Fix apiserver sans,roles/kubernetes/client/tasks/main.yml,MODIFY,4056,"@@ -1,11 +1,17 @@
 ---
 - name: Set external kube-apiserver endpoint
   set_fact:
-    external_apiserver_endpoint: >-
+    external_apiserver_address: >-
       {%- if loadbalancer_apiserver is defined and loadbalancer_apiserver.port is defined -%}
-      https://{{ apiserver_loadbalancer_domain_name }}:{{ loadbalancer_apiserver.port|default(kube_apiserver_port) }}
+      {{ apiserver_loadbalancer_domain_name }}
       {%- else -%}
-      https://{{ kube_apiserver_access_address }}:{{ kube_apiserver_port }}
+      {{ kube_apiserver_access_address }}
+      {%- endif -%}
+    external_apiserver_port: >-
+      {%- if loadbalancer_apiserver is defined and loadbalancer_apiserver.port is defined -%}
+      {{ loadbalancer_apiserver.port|default(kube_apiserver_port) }}
+      {%- else -%}
+      {{ kube_apiserver_port }}
       {%- endif -%}
   tags:
     - facts
@@ -24,12 +30,28 @@
     mode: ""0600""
     backup: yes
 
-- name: Copy admin kubeconfig to ansible host
-  fetch:
-    src: ""{{ kube_config_dir }}/admin.conf""
+- name: Generate admin kubeconfig with external api endpoint
+  shell: >-
+    {{ bin_dir }}/kubeadm alpha
+    {% if kubeadm_version is version('v1.13.0', '<') %}
+    phase
+    {% endif %}
+    kubeconfig user
+    --client-name kubernetes-admin
+    --org system:masters
+    --cert-dir {{ kube_config_dir }}/ssl
+    --apiserver-advertise-address {{ external_apiserver_address }}
+    --apiserver-bind-port {{ external_apiserver_port }}
+  run_once: yes
+  register: admin_kubeconfig
+
+- name: Write admin kubeconfig on ansible host
+  copy:
+    content: ""{{ admin_kubeconfig.stdout }}""
     dest: ""{{ artifacts_dir }}/admin.conf""
-    flat: yes
-    validate_checksum: no
+    mode: 0640
+  delegate_to: localhost
+  become: no
   run_once: yes
   when: kubeconfig_localhost|default(false)
 
",add_task,add_task,add_task,"- ハードコードされていたURLを変数に置き換え
- 新しい変数external_apiserver_portの追加
- 新しいタスクGenerate admin kubeconfig with external api endpointの追加",jinja2っぽいけど、元々ある変数をjnja2で制御しているわけではなく、タスクの内容も変更されているので
e552be76ce85bfaaf0480260d279ad86d1b37147,2019-02-14 18:19:19+00:00,Docker apt repo name fix (again) (#4246) For some reason 18.09 packages are now prefixed with `5:` in the download.docker.com apt repos Followup to #4236,roles/container-engine/docker/vars/ubuntu-arm64.yml,MODIFY,4246,"@@ -7,9 +7,9 @@ docker_versioned_pkg:
   '17.09': docker-ce=17.09.1~ce-0~ubuntu
   '17.12': docker-ce=17.12.1~ce-0~ubuntu-{{ ansible_distribution_release|lower }}
   '18.06': docker-ce=18.06.2~ce~3-0~ubuntu
-  '18.09': docker-ce=18.09.2~3-0~ubuntu-{{ ansible_distribution_release|lower }}
+  '18.09': docker-ce=5:18.09.2~3-0~ubuntu-{{ ansible_distribution_release|lower }}
   'stable': docker-ce=18.06.2~ce~3-0~ubuntu
-  'edge': docker-ce=18.09.2~3-0~ubuntu-{{ ansible_distribution_release|lower }}
+  'edge': docker-ce=5:18.09.2~3-0~ubuntu-{{ ansible_distribution_release|lower }}
 
 docker_package_info:
   pkg_mgr: apt
",filepath,dependency,filepath,docker-ce 18.09およびedgeのバージョン情報の修正,ファイル名の変更はファイルパスの変更になりますか？
e552be76ce85bfaaf0480260d279ad86d1b37147,2019-02-14 18:19:19+00:00,Docker apt repo name fix (again) (#4246) For some reason 18.09 packages are now prefixed with `5:` in the download.docker.com apt repos Followup to #4236,roles/container-engine/docker/vars/debian.yml,MODIFY,4246,"@@ -14,7 +14,7 @@ docker_versioned_pkg:
   '17.12': docker-ce=17.12.1~ce-0~debian
   '18.03': docker-ce=18.03.1~ce-0~debian
   '18.06': docker-ce=18.06.2~ce~3-0~debian
-  '18.09': docker-ce=18.09.2~3-0~debian-{{ ansible_distribution_release|lower }}
+  '18.09': docker-ce=5:18.09.2~3-0~debian-{{ ansible_distribution_release|lower }}
   'stable': docker-ce=18.06.2~ce~3-0~debian
   'edge': docker-ce=17.12.1~ce-0~debian
 
",filepath,dependency,filepath,docker-ce 18.09およびedgeのバージョン情報の修正,ファイル名の変更はファイルパスの変更になりますか？
e552be76ce85bfaaf0480260d279ad86d1b37147,2019-02-14 18:19:19+00:00,Docker apt repo name fix (again) (#4246) For some reason 18.09 packages are now prefixed with `5:` in the download.docker.com apt repos Followup to #4236,roles/container-engine/docker/vars/ubuntu-amd64.yml,MODIFY,4246,"@@ -11,9 +11,9 @@ docker_versioned_pkg:
   '17.09': docker-ce=17.09.0~ce-0~ubuntu-{{ ansible_distribution_release|lower }}
   '17.12': docker-ce=17.12.1~ce-0~ubuntu-{{ ansible_distribution_release|lower }}
   '18.06': docker-ce=18.06.2~ce~3-0~ubuntu
-  '18.09': docker-ce=18.09.2~3-0~ubuntu-{{ ansible_distribution_release|lower }}
+  '18.09': docker-ce=5:18.09.2~3-0~ubuntu-{{ ansible_distribution_release|lower }}
   'stable': docker-ce=18.06.2~ce~3-0~ubuntu
-  'edge': docker-ce=18.09.2~ce~3-0~ubuntu
+  'edge': docker-ce=5:18.09.2~ce~3-0~ubuntu
 
 docker_package_info:
   pkg_mgr: apt
",filepath,dependency,filepath,docker-ce 18.09およびedgeのバージョン情報の修正,ファイル名の変更はファイルパスの変更になりますか？
d588532c9ba4e6e70c7328ce8672e44cc26cd6b3,2019-04-23 21:46:02+00:00,"Update probe timeouts, delays etc. (#4612) * Fix merge conflict * Add check delay * Add more liveness and readiness options to metrics-server",tests/files/packet_centos7-flannel-addons.yml,MODIFY,4612,"@@ -19,8 +19,7 @@ ingress_nginx_enabled: true
 cert_manager_enabled: true
 # Disabled temporarily
 metrics_server_enabled: false
+metrics_server_kubelet_insecure_tls: true
 kube_token_auth: true
 kube_basic_auth: true
 enable_nodelocaldns: false
-
-vm_memory: 6144Mi
",edit_var,edit_var,edit_var,変数が追加，削除される,テスト
d588532c9ba4e6e70c7328ce8672e44cc26cd6b3,2019-04-23 21:46:02+00:00,"Update probe timeouts, delays etc. (#4612) * Fix merge conflict * Add check delay * Add more liveness and readiness options to metrics-server",tests/files/gce_centos7-flannel-addons.yml,MODIFY,4612,"@@ -22,6 +22,7 @@ kube_encrypt_secret_data: true
 cert_manager_enabled: true
 # Disabled temporarily
 metrics_server_enabled: false
+metrics_server_kubelet_insecure_tls: true
 kube_token_auth: true
 kube_basic_auth: true
 enable_nodelocaldns: false
",edit_var,edit_var,edit_var,metrics_server...変数が追加される,テスト
53e3463b5a872afdc279a7d7b595737ffbf9922c,2019-04-25 11:20:47+00:00,Fix GCE tests with undefined CI_PLATFORM (#4650),.gitlab-ci/gce.yml,MODIFY,4650,"@@ -59,7 +59,7 @@ gce_centos7-flannel-addons:
 
 gce_centos-weave-kubeadm-sep:
   stage: deploy-gce
-  <<: *gce
+  extends: .gce
   variables:
     <<: *centos_weave_kubeadm_variables
   when: on_success
@@ -126,7 +126,7 @@ gce_ubuntu-flannel-ha:
 
 gce_centos-weave-kubeadm-triggers:
   stage: deploy-gce
-  <<: *gce
+  extends: .gce
   variables:
     <<: *centos_weave_kubeadm_variables
   when: on_success
@@ -227,7 +227,7 @@ gce_centos7-kube-router:
 
 gce_centos7-multus-calico:
   stage: deploy-gce
-  <<: *gce
+  extends: .gce
   variables:
     <<: *centos7_multus_calico_variables
   when: manual
",other,other,other,,GitLab用
741de6051c6d411f697ae419a80a162157066d00,2019-04-29 06:36:19+00:00,Fix nodeselectors for contiv and nginx-ingress (#4662) * Fix nodeselectors for contiv and nginx-ingress Change-Id: Ib3eb6bd87193c69a90ee944c9164a0b6792c79ba * Set kube proxy mode to iptables for addons task Change-Id: Iff71a71f672405c74b4708c71db15ddc4391a53a,roles/kubernetes-apps/ingress_controller/ingress_nginx/defaults/main.yml,MODIFY,4662,"@@ -2,7 +2,7 @@
 ingress_nginx_namespace: ""ingress-nginx""
 ingress_nginx_host_network: false
 ingress_nginx_nodeselector:
-  node-role.kubernetes.io/node: """"
+  beta.kubernetes.io/os: ""linux""
 ingress_nginx_tolerations: []
 ingress_nginx_insecure_port: 80
 ingress_nginx_secure_port: 443
",edit_var,edit_var,edit_var,変数の追加．削除,defaultの変数の変更
741de6051c6d411f697ae419a80a162157066d00,2019-04-29 06:36:19+00:00,Fix nodeselectors for contiv and nginx-ingress (#4662) * Fix nodeselectors for contiv and nginx-ingress Change-Id: Ib3eb6bd87193c69a90ee944c9164a0b6792c79ba * Set kube proxy mode to iptables for addons task Change-Id: Iff71a71f672405c74b4708c71db15ddc4391a53a,tests/files/packet_centos7-flannel-addons.yml,MODIFY,4662,"@@ -6,6 +6,7 @@ mode: ha
 # Kubespray settings
 kubeadm_control_plane: true
 kubeadm_certificate_key: 3998c58db6497dd17d909394e62d515368c06ec617710d02edea31c06d741085
+kube_proxy_mode: iptables
 kube_network_plugin: flannel
 helm_enabled: true
 kubernetes_audit: true
",edit_var,edit_var,edit_var,変数の追加,テスト
741de6051c6d411f697ae419a80a162157066d00,2019-04-29 06:36:19+00:00,Fix nodeselectors for contiv and nginx-ingress (#4662) * Fix nodeselectors for contiv and nginx-ingress Change-Id: Ib3eb6bd87193c69a90ee944c9164a0b6792c79ba * Set kube proxy mode to iptables for addons task Change-Id: Iff71a71f672405c74b4708c71db15ddc4391a53a,inventory/sample/group_vars/k8s-cluster/addons.yml,MODIFY,4662,"@@ -79,7 +79,7 @@ rbd_provisioner_enabled: false
 ingress_nginx_enabled: false
 # ingress_nginx_host_network: false
 # ingress_nginx_nodeselector:
-#   node-role.kubernetes.io/node: """"
+#   beta.kubernetes.io/os: ""linux"": """"
 # ingress_nginx_tolerations:
 #   - key: ""node-role.kubernetes.io/master""
 #     operator: ""Equal""
",comment,other,other,コメントのみの変更,コメントの部分の変更
dc16ab92f45d3310358ae3e803be24a881e9f2e4,2019-07-08 09:20:03+00:00,fix for calico with kdd datastore (#4922) * fix for calico with kdd datastore * remove AS number from daemonset * revert changes to canal * additionnal fixes for kdd datastore in calico,roles/network_plugin/calico/tasks/install.yml,MODIFY,4922,"@@ -35,7 +35,7 @@
 
 - name: Calico | Install calicoctl wrapper script
   template:
-    src: calicoctl.sh.j2
+    src: ""calicoctl.{{ calico_datastore }}.sh.j2""
     dest: ""{{ bin_dir }}/calicoctl.sh""
     mode: 0755
     owner: root
@@ -73,6 +73,32 @@
     - 'calico_conf.stdout == ""0""'
     - calico_pool_cidr is defined
 
+- name: Calico | Create calico manifests for kdd
+  template:
+    src: ""{{ item.file }}.j2""
+    dest: ""{{ kube_config_dir }}/{{ item.file }}""
+  with_items:
+    - {name: calico, file: kdd-crds.yml, type: kdd}
+  register: calico_node_kdd_manifest
+  when:
+    - inventory_hostname in groups['kube-master']
+    - calico_datastore == ""kdd""
+
+- name: Start Calico resources
+  kube:
+    name: ""{{ item.item.name }}""
+    namespace: ""kube-system""
+    kubectl: ""{{ bin_dir }}/kubectl""
+    resource: ""{{ item.item.type }}""
+    filename: ""{{ kube_config_dir }}/{{ item.item.file }}""
+    state: ""latest""
+  with_items:
+    - ""{{ calico_node_kdd_manifest.results }}""
+  when:
+    - inventory_hostname == groups['kube-master'][0] and not item is skipped
+  loop_control:
+    label: ""{{ item.item.file }}""
+
 - name: Calico | Configure calico network pool (v3.0.0 <= version < v3.3.0)
   shell: >
     echo ""
@@ -180,9 +206,10 @@
         ""asNumber"": ""{{ local_as }}""
       },
       ""orchRefs"":[{""nodeName"":""{{ inventory_hostname }}"",""orchestrator"":""k8s""}]
-   }}' | {{ bin_dir }}/calicoctl.sh create --skip-exists -f -
+   }}' | {{ bin_dir }}/calicoctl.sh {{ 'apply -f -' if calico_datastore == ""kdd"" else 'create --skip-exists -f -' }}
   retries: 4
   delay: ""{{ retry_stagger | random + 3 }}""
+  delegate_to: ""{{ groups['kube-master'][0] }}""
   when:
     - calico_version is version('v3.0.0', '>=')
     - peer_with_router|default(false)
@@ -230,6 +257,7 @@
   delay: ""{{ retry_stagger | random + 3 }}""
   with_items:
     - ""{{ peers|selectattr('scope','undefined')|list|default([]) | union(peers|selectattr('scope','defined')|selectattr('scope','equalto', 'node')|list|default([])) }}""
+  delegate_to: ""{{ groups['kube-master'][0] }}""
   when:
     - calico_version is version('v3.0.0', '>=')
     - peer_with_router|default(false)
@@ -269,6 +297,7 @@
   with_items:
     - ""{{ peers|selectattr('scope','defined')|selectattr('scope','equalto', 'global')|list|default([]) }}""
   run_once: true
+  delegate_to: ""{{ groups['kube-master'][0] }}""
   when:
     - calico_version | version_compare('v3.0.0', '>=')
     - peer_with_router|default(false)
@@ -309,6 +338,7 @@
   delay: ""{{ retry_stagger | random + 3 }}""
   with_items:
     - ""{{ groups['calico-rr'] | default([]) }}""
+  delegate_to: ""{{ groups['kube-master'][0] }}""
   when:
     - calico_version is version('v3.0.0', '>=')
     - peer_with_calico_rr|default(false)
@@ -351,17 +381,6 @@
     - inventory_hostname in groups['kube-master']
     - rbac_enabled or item.type not in rbac_resources
 
-- name: Calico | Create calico manifests for kdd
-  template:
-    src: ""{{ item.file }}.j2""
-    dest: ""{{ kube_config_dir }}/{{ item.file }}""
-  with_items:
-    - {name: calico, file: kdd-crds.yml, type: kdd}
-  register: calico_node_kdd_manifest
-  when:
-    - inventory_hostname in groups['kube-master']
-    - calico_datastore == ""kdd""
-
 - name: Calico | Create calico manifests for typha
   template:
     src: ""{{ item.file }}.j2""
",add_task,add_task,add_task,"複数の変更

- Filepath: src: のファイルパス
- Move: name: Calico | Create calico manifests for kdd 
- add_task: name: Start Calico resources
- directive: delegate_toの追加
- command: echoで出力する文字列の変更

add_task以外は軽微な変更に留まると考えた",タスクの順序が変更されて、新しくタスクが追加されている。delegete_toも増えているのでどれにしようかな。startするようにしたのが大きいかな？
dc16ab92f45d3310358ae3e803be24a881e9f2e4,2019-07-08 09:20:03+00:00,fix for calico with kdd datastore (#4922) * fix for calico with kdd datastore * remove AS number from daemonset * revert changes to canal * additionnal fixes for kdd datastore in calico,roles/network_plugin/calico/tasks/check.yml,MODIFY,4922,"@@ -10,16 +10,10 @@
   run_once: yes
 
 - name: ""Get current version of calico cluster version""
-  shell: ""{{ bin_dir }}/calicoctl version  | grep 'Cluster Version:' | awk '{ print $3}'""
+  shell: ""{{ bin_dir }}/calicoctl.sh version  | grep 'Cluster Version:' | awk '{ print $3}'""
   register: calico_version_on_server
   run_once: yes
   changed_when: false
-  environment:
-    ETCD_ENDPOINTS: ""{{ etcd_access_addresses }}""
-    ETCD_CA_CERT_FILE: ""{{ calico_cert_dir }}/ca_cert.crt""
-    ETCD_CERT_FILE: ""{{ calico_cert_dir }}/cert.crt""
-    ETCD_KEY_FILE: ""{{ calico_cert_dir }}/key.pem""
-
 
 - name: ""Determine if calico upgrade is needed""
   block:
",edit_args,command,edit_args,"実行する対象がシェルスクリプトに変更されている
environmentが削除されている",enviromentの削除が大きそう
fbbfff3795daffa8d9cd69244781f768d1c99d59,2019-07-30 18:58:11+00:00,fix broken ubuntu containerd engine (#5002),roles/container-engine/containerd/tasks/main.yml,MODIFY,5002,"@@ -101,7 +101,7 @@
 
 - name: Check if runc is installed
   stat:
-    path: /usr/sbin/runc
+    path: ""{{ runc_binary }}""
   register: runc_stat
 
 - name: Install runc package if necessary
",hard_coded,filepath,edit_args,ハードコードされていた値が変数に置き換えられている,変数の値の変更
fbbfff3795daffa8d9cd69244781f768d1c99d59,2019-07-30 18:58:11+00:00,fix broken ubuntu containerd engine (#5002),roles/container-engine/containerd/vars/ubuntu-amd64.yml,MODIFY,5002,"@@ -3,6 +3,7 @@
 containerd_versioned_pkg:
   'latest': ""{{ containerd_package }}""
   '1.2.4': ""{{ containerd_package }}=1.2.4-1""
+  '1.2.6': ""{{ containerd_package }}=1.2.6-3""
   'stable': ""{{ containerd_package }}=1.2.4-1""
   'edge': ""{{ containerd_package }}=1.2.4-1""
 
@@ -25,3 +26,5 @@ containerd_repo_info:
        deb {{ containerd_ubuntu_repo_base_url }}
        {{ ansible_distribution_release|lower }}
        {{ containerd_ubuntu_repo_component }}
+
+runc_binary: /usr/bin/runc
",dependency,dependency,dependency,containered_versionに新しいバージョン情報が追加されている,1.2.6の追加
07ecef86e3f81e17221d89f8ea64ce54328ebfea,2019-08-22 09:40:32+00:00,"Replace fetch with synchronize due to memory error (#5084) Fix for Kubespray Issue #5059 (https://github.com/kubernetes-sigs/kubespray/issues/5059). There is a known issue with the 'fetch' module that will sometimes lead to it failing with a memory error. See ansible/ansible#11702 (https://github.com/ansible/ansible/issues/11702). I encountered this issue with the ""Copy kubectl binary to ansible host"" task in kubespray/roles/kubernetes/client/tasks/main.yml, and it caused my entire deployment to error out (see ""Output of ansible run"" above). Replacing 'fetch' with 'synchronize' fixes this issue.",roles/kubernetes/client/tasks/main.yml,MODIFY,5084,"@@ -90,11 +90,9 @@
   when: kubeconfig_localhost|default(false)
 
 - name: Copy kubectl binary to ansible host
-  fetch:
+  synchronize:
     src: ""{{ bin_dir }}/kubectl""
     dest: ""{{ artifacts_dir }}/kubectl""
-    flat: yes
-    validate_checksum: no
   become: no
   run_once: yes
   when: kubectl_localhost|default(false)
",other,other,directive,ansibleのfetchモジュールに問題があり，synchronizeモジュールに置き換えられている,fetchをsynchronizeに変更している
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,.gitlab-ci/packet.yml,MODIFY,5189,"@@ -61,10 +61,11 @@ packet_ubuntu-flannel-ha:
   <<: *packet
   when: manual
 
-packet_ubuntu-contiv-sep:
-  stage: deploy-part2
-  <<: *packet
-  when: on_success
+# Contiv does not work in k8s v1.16
+# packet_ubuntu-contiv-sep:
+#   stage: deploy-part2
+#   <<: *packet
+#   when: on_success
 
 packet_ubuntu18-cilium-sep:
   stage: deploy-special
",other,other,other,ci,GitLab用
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/download/tasks/download_file.yml,MODIFY,5189,"@@ -7,7 +7,7 @@
 
   - name: download_file | Set pathname of cached file
     set_fact:
-      file_path_cached: ""{{ download_cache_dir }}/{{ download.dest | regex_replace('^\\/', '') }}""
+      file_path_cached: ""{{ download_cache_dir }}/{{ download.dest | basename }}""
     tags:
     - facts
 
",filter,filepath,edit_var,basenameフィルタを使うように変更,filterを通して、変数の変更
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/master/tasks/kubeadm-secondary-experimental.yml,MODIFY,5189,"@@ -15,14 +15,7 @@
     {{ bin_dir }}/kubeadm init phase
     --config {{ kube_config_dir }}/kubeadm-config.yaml
     upload-certs
-    {% if kubeadm_version is version('v1.15.0', '<') %}
-    --experimental-upload-certs
-    {% else %}
     --upload-certs
-    {% endif %}
-    {% if kubeadm_certificate_key is defined and kubeadm_version is version('v1.15.0', '<') %}
-    --certificate-key={{ kubeadm_certificate_key }}
-    {% endif %}
   register: kubeadm_upload_cert
   when:
     - inventory_hostname == groups['kube-master']|first
@@ -60,9 +53,6 @@
     {{ bin_dir }}/kubeadm join
     --config {{ kube_config_dir }}/kubeadm-controlplane.yaml
     --ignore-preflight-errors=all
-    {% if kubeadm_certificate_key is defined and kubeadm_version is version('v1.15.0', '<') %}
-    --certificate-key={{ kubeadm_certificate_key }}
-    {% endif %}
   register: kubeadm_join_control_plane
   retries: 3
   until: kubeadm_join_control_plane is succeeded
",jinja2,command,jinja2,"コマンド引数の変更
バージョンによる引数の違いを削除",jinja2の制御部分を削除
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/kubeadm/tasks/main.yml,MODIFY,5189,"@@ -42,7 +42,6 @@
     kubeadm_token: ""{{ temp_token.stdout }}""
   when: kubeadm_token is not defined
 
-
 - name: gets the kubeadm version
   command: ""{{ bin_dir }}/kubeadm version -o short""
   register: kubeadm_output
",no_effect,other,other,refactor,空白行の削除
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/kubeadm/tasks/kubeadm_etcd_node.yml,MODIFY,5189,"@@ -4,14 +4,7 @@
     {{ bin_dir }}/kubeadm init phase
     --config {{ kube_config_dir }}/kubeadm-config.yaml
     upload-certs
-    {% if kubeadm_version is version('v1.15.0', '<') %}
-    --experimental-upload-certs
-    {% else %}
     --upload-certs
-    {% endif %}
-    {% if kubeadm_certificate_key is defined and kubeadm_version is version('v1.15.0', '<') %}
-    --certificate-key={{ kubeadm_certificate_key }}
-    {% endif %}
   register: kubeadm_upload_cert
   delegate_to: ""{{ groups['kube-master'][0] }}""
   when: kubeadm_etcd_refresh_cert_key
@@ -27,14 +20,14 @@
     {{ bin_dir }}/kubeadm join phase
     control-plane-prepare download-certs
     --certificate-key {{ kubeadm_certificate_key }}
-    --experimental-control-plane
+    --control-plane
     --token {{ kubeadm_token }}
     --discovery-token-unsafe-skip-ca-verification
     {{ kubeadm_discovery_address }}
     &&
     {{ bin_dir }}/kubeadm join phase
     control-plane-prepare certs
-    --experimental-control-plane
+    --control-plane
     --token {{ kubeadm_token }}
     --discovery-token-unsafe-skip-ca-verification
     {{ kubeadm_discovery_address }}
",jinja2,command,jinja2,"コマンド引数の変更
バージョンによる引数の違いを削除",jinja2のテンプレート部分の削除、引数の変更
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/client/tasks/main.yml,MODIFY,5189,"@@ -49,32 +49,16 @@
 # NOTE(mattymo): Please forgive this workaround
 - name: Generate admin kubeconfig with external api endpoint
   shell: >-
-    {% if kubeadm_version is version('v1.14.0', '>=') %}
     mkdir -p {{ kube_config_dir }}/external_kubeconfig &&
-    {% endif %}
     {{ bin_dir }}/kubeadm
-    {% if kubeadm_version is version('v1.14.0', '>=') %}
     init phase
-    {% elif kubeadm_version is version('v1.13.0', '>=') %}
-    alpha
-    {% else %}
-    alpha phase
-    {% endif %}
-    {% if kubeadm_version is version('v1.14.0', '>=') %}
     kubeconfig admin
     --kubeconfig-dir {{ kube_config_dir }}/external_kubeconfig
-    {% else %}
-    kubeconfig user
-    --client-name kubernetes-admin
-    --org system:masters
-    {% endif %}
     --cert-dir {{ kube_cert_dir }}
     --apiserver-advertise-address {{ external_apiserver_address }}
     --apiserver-bind-port {{ external_apiserver_port }}
-    {% if kubeadm_version is version('v1.14.0', '>=') %}
     >/dev/null && cat {{ kube_config_dir }}/external_kubeconfig/admin.conf &&
     rm -rf {{ kube_config_dir }}/external_kubeconfig
-    {% endif %}
   environment: ""{{ proxy_env }}""
   run_once: yes
   register: admin_kubeconfig
",jinja2,command,jinja2,"コマンド引数の変更
バージョンによる引数の違いを削除",jinja2のテンプレート部分の削除
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/download/tasks/sync_file.yml,MODIFY,5189,"@@ -6,7 +6,7 @@
 
   - name: download_file | Set pathname of cached file
     set_fact:
-      file_path_cached: ""{{ download_cache_dir }}/{{ download.dest | regex_replace('^\\/', '') }}""
+      file_path_cached: ""{{ download_cache_dir }}/{{ download.dest | basename }}""
     tags:
     - facts
 
",filter,filepath,edit_var,basenameフィルタを使うように変更,filterを通して、変数の変更
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/node/tasks/kubelet.yml,MODIFY,5189,"@@ -57,4 +57,4 @@
     enabled: yes
     state: started
   tags:
-    - kubelet
\ No newline at end of file
+    - kubelet
",no_effect,other,other,?,最終行に空行を追加
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/master/tasks/kubeadm-setup.yml,MODIFY,5189,"@@ -110,16 +110,7 @@
     --config={{ kube_config_dir }}/kubeadm-config.yaml
     --ignore-preflight-errors=all
     --skip-phases=addon/coredns
-    {% if kubeadm_version is version('v1.14.0', '>=') %}
-    {% if kubeadm_version is version('v1.15.0', '<') %}
-    --experimental-upload-certs
-    {% if kubeadm_certificate_key is defined and kubeadm_version is version('v1.15.0', '<') %}
-    --certificate-key={{ kubeadm_certificate_key }}
-    {% endif %}
-    {% else %}
     --upload-certs
-    {% endif %}
-    {% endif %}
   register: kubeadm_init
   # Retry is because upload config sometimes fails
   retries: 3
@@ -135,7 +126,6 @@
     kubeadm_certificate_key: ""{{ item | regex_search('--certificate-key ([^ ]+)','\\1') | first }}""
   with_items: ""{{ hostvars[groups['kube-master'][0]]['kubeadm_init'].stdout_lines | default([]) }}""
   when:
-    - kubeadm_version is version('v1.14.0', '>=')
     - kubeadm_certificate_key is not defined
     - item | trim | match('.*--certificate-key .*')
 
",jinja2,command,jinja2,"コマンド引数の変更
バージョンによる引数の違いを削除",jinja2のテンプレート部分の削除
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/master/tasks/kubeadm-version.yml,MODIFY,5189,"@@ -3,11 +3,6 @@
   command: ""{{ bin_dir }}/kubeadm version -o short""
   register: kubeadm_output
 
-- name: sets kubeadm api version to v1beta1
-  set_fact:
-    kubeadmConfig_api_version: v1beta1
-  when: kubeadm_output.stdout is version('v1.13.0', '>=')
-
 - name: sets kubeadm api version to v1beta2
   set_fact:
     kubeadmConfig_api_version: v1beta2
",edit_var,edit_var,remove_task,kubeadmのバージョンアップに伴い変数が利用されなくなった,タスクの削除
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/download/defaults/main.yml,MODIFY,5189,"@@ -49,7 +49,7 @@ download_delegate: ""{% if download_localhost %}localhost{% else %}{{ groups['kub
 image_arch: ""{{host_architecture | default('amd64')}}""
 
 # Versions
-kube_version: v1.15.3
+kube_version: v1.16.0
 kubeadm_version: ""{{ kube_version }}""
 etcd_version: v3.3.10
 
@@ -85,7 +85,7 @@ kube_ovn_version: ""v0.6.0""
 kube_router_version: ""v0.2.5""
 multus_version: ""v3.2.1""
 
-crictl_version: ""v1.15.0""
+crictl_version: ""v1.16.0""
 
 # Download URLs
 kubeadm_download_url: ""https://storage.googleapis.com/kubernetes-release/release/{{ kubeadm_version }}/bin/linux/{{ image_arch }}/kubeadm""
@@ -97,99 +97,57 @@ crictl_download_url: ""https://github.com/kubernetes-sigs/cri-tools/releases/down
 
 crictl_checksums:
   arm:
+    v1.16.0: 331c49bd9196009b8230f7a36ec272924a7bcf4c1614ecddf0eb9598c787da0e
     v1.15.0: f31f8c3b4791608a48d030d1aa1a694a73849ae057b23a90ce4ef17e5afde9e8
     v1.14.0: 9910cecfd6558239ba015323066c7233d8371af359b9ddd0b2a35d5223bcf945
-    v1.13.0: 2e478ebed85f9d70d49fd8f1d1089c8fba6e37d3461aeef91813f1ab0f0df586
   arm64:
+    v1.16.0: aa118c31d6f6fd2d24bb2de4a33598a14a5952e1d01f93d5c3267c2b5334743b
     v1.15.0: 785c3da7e058f6fd00b0a48de24b9199eb6bae940d13f509c44ea6dd7ad9ffcd
     v1.14.0: f76b3d00a272c8d210e9a45f77d07d3770bee310d99c4fd9a72d6f55278882e5
-    v1.13.0: 68949c0cb5a37e7604c145d189cf1e109c08c93d9c710ba663db026b9c6f2746
   amd64:
+    v1.16.0: a3eefa10a483c643ad85aee3d7832a720976ef7e80dde46b212eaaacd7d09512
     v1.15.0: c3b71be1f363e16078b51334967348aab4f72f46ef64a61fe7754e029779d45a
     v1.14.0: 483c90a9fe679590df4332ba807991c49232e8cd326c307c575ecef7fe22327b
-    v1.13.0: 9bdbea7a2b382494aff2ff014da328a042c5aba9096a7772e57fdf487e5a1d51
 
 # Checksums
 hyperkube_checksums:
   arm:
-    v1.16.0-beta.2: 2b64ef9e8e2f92b72352cc7ab95b416407f4fca9ed1a5020aeeb6a3777bd16ed
+    v1.16.0: 4f2e4ffcf7b7f40c70c637c7be9dd51cd1ee29763696011149f315e90339330b
     v1.15.3: 100d8bddb29e77397b90e6dfbcf0af2d901a90ea4bde90b83b5a39f394c3900b
     v1.15.2: eeaa8e071541c7bcaa186ff1d2919d076b27ef70c9e9df70f910756eba55dc99
     v1.15.1: fc5af96fd9341776d84c38675be7b8045dee20af327af9331972c422a4109918
     v1.15.0: d923c781031bfd97d0fbe50311e4d7c3616aa5b6d466b99049931f09d73d07b9
-    v1.14.6: 93d35b16785e71f6a38e9a54ddf1aca08c924b0f49e5f99ea8ccaff59bd9721b
-    v1.14.5: 860b84dd32611a6008fe20fb998a2fc0a25ff44067eae556224827d05429c91e
-    v1.14.4: 429a10369b2ef35a9c2d662347277339d53fa66ef55ffeabcc7d9b850e31056d
-    v1.14.3: 3fac785261bcf79f7a80b12c4a1dda893ce8c0879caf57b36d4701730671b574
-    v1.14.2: 6929a59850c8702c04d62cd343d1143b17456da040f32317e09f8c25a08d2346
-    v1.14.1: 839a4abfeafbd5f5ab057ad0e8a0b0b488b3cde14a646eba040a7f579875f565
-    v1.14.0: d090b1da23564a7e9bb8f1f4264f2116536c52611ae203fe2ca13eaad0a8003e
   arm64:
-    v1.16.0-beta.2: 0f1e694db5c75cff526c6c066e618c43e83384c36d4e38cf1ac6e9baf71b38d4
+    v1.16.0: 0431fff5b32042369a3f233afb7b3adcda7ae5446f31700819ed986f3624bc69
     v1.15.3: 1e3e70b8d1e8ebc642f2801d9c7938a27764dfb2f5aea432ab4326d43c04a1f5
     v1.15.2: c4cf69f52c7013faee9d54e0f376e0732a4a7b0f7ffc7241e9b7e28bad0ac77f
     v1.15.1: 80ed372c5f6c5178df88616175310057c06bdc9d0905953814a1927eb3aaa657
     v1.15.0: 824af7d925b87a5ade63575b98b59ee81005fc76eac1dc399602308d7a60bc3c
-    v1.14.6: 97646bffe61e54a0c6f61d68b5625ec2e98d8b9d04cec2c8382266e437835e93
-    v1.14.5: 90c77847d64eb857c8e686e8593fe7a9e505bcbf960b0407217255827a9da59a
-    v1.14.4: 9e0b4fde88a07c705e0937cd3161392684e3ca08535d14a99ae3b86bbf4c56b3
-    v1.14.3: f29211d668cbcf1aa415dfa64aad95ffc53b5410482a23cddb680caec4e907a3
-    v1.14.2: 959fb7d9c17fc8f7cb1a69920aaf08aefd62c0fbf6b5bdc46250f147ea6a5cd4
-    v1.14.1: d5236efc2547fd07c7cc2ed9345dfbcd1204385847ca686cf1c62d15056de399
-    v1.14.0: 708e00a41f6516d525dee00c91ebe3c3bf2feaf9b7f0af7689487e3e17e356c2
   amd64:
-    v1.16.0-beta.2: 2f05aba15c163883a610681a563d89fd7adf10cb70b90cdb6760f00f8d023a4b
+    v1.16.0: 00b54ca779db1749ed714b19bb2b9a0333b39048af134f9199e4a5441c1b8324
     v1.15.3: 3685c65b4fb85d552f77346900affc2e9a1bc997b4cd3dde0e705fd8c1d9be7a
     v1.15.2: ab885606438748eb89a7738e219f5353d94c40c63a4935a539ce89760280f065
     v1.15.1: 22b7b1e7f5f2a452d62e0ca4c2cba67119c51e04219aaeaf8452825f9177069e
     v1.15.0: 3cc72cc58517b97c608c7a59a20255675bc70f07217c9e11e58cac7746139283
-    v1.14.6: 4f9a8984985786797fa3353961ba2b58f50235581c9b5978130fbb4199005538
-    v1.14.5: 2c3410518980b8705ba9b7b708076a206f2bde37cb8bf5ba8f15c32c697f4d97
-    v1.14.4: 5f31434f3a884257a7b0e3178fc869720a7526c8637af5713d23433ddf2592dd
-    v1.14.3: 6c6cb5c118b2129ba4e56697f42567be3587eb636a477cd342b69f87b3b049d1
-    v1.14.2: 05546057f2053e085fa8387ab82581c95fe4195cd783408ccbb4fc3487c50176
-    v1.14.1: fb34b98da9325feca8daa09bb934dbe6a533aad69c2a5599bbed81b99bb9c267
-    v1.14.0: af8b04504365dbe4ce6a1772f42eb390d4221a21149b522fc8a0c4b1cd3d97aa
 kubeadm_checksums:
   arm:
-    v1.16.0-beta.2: 6cf8b364b40aba09e1aaa4ed873d90df2b17725dafa78252470777df9404a736
+    v1.16.0: 6c666958e11b7d4513adecb3107c885c98bdc79f38d369c9f80eaaeae4ddfe66
     v1.15.3: 6c6fa56810908b5be83882094ea199844edc94b7e969160623c86512d9251c06
     v1.15.2: 4b35ad0031c08a83de7c8d9f9bbed6a30d93a5c74e16ea9e6211ad2e0e12bdd1
     v1.15.1: 855abd520291dcef0577a1a2ef87a70f522fd2b22603a12abcd86c2f7ec9c022
     v1.15.0: 9464030a1d4e101de5f47348f3514d5a9eb95cbce2e5e31f53ada1ca485cf75e
-    v1.14.6: 6283ac962d02714e962e4f206c6bc8d6be58f5c9a12d2918aaa2fac7f73add09
-    v1.14.5: 0bb551f7468de2fa6f98ce60653495327be052364ac9f9e8917a4d1ad864412b
-    v1.14.4: 36835488d7187406690ee6aa4b3c9c54855cb5c55d786d0574a508b955fe3a46
-    v1.14.3: 270b8c346aeaa309d11d65695c4a90f6bff5b1ea14bdec3c417ca2dfb3de0db3
-    v1.14.2: d2a59269aa68a4bace2a80b247b6f9a82f0542ec3004185fb0ba86e181fdfb29
-    v1.14.1: 4bd111411208f1270ed3af8780b87d24a3c17c9fdbe4b0f8c7a9a21cd765543e
-    v1.14.0: 11f2cfa8bf7ee177dbac8073ab0f039dc265536baaa8dc0c4dea699f981f6fd1
   arm64:
-    v1.16.0-beta.2: 0e3ae66f2f57a18eb363af1d49a22b35a24e32bf36af5ef630aa5ceeedc9feed
+    v1.16.0: 9a1d21bfb6bd15697ac010665e5917a5364b340d5b60f2f0302c179d75da0f3f
     v1.15.3: 6f472bc8ab1ba3d76448bd45b200edef96741e5affde8dc1429300af3a4904d8
     v1.15.2: d3b6ee2048b366726ca366d2db4c46b2cacc38e8ec09cc35781d16593753d930
     v1.15.1: 44fbfad0f1026d249fc4f365f1e9562cd52d75360d4d1032731122ba5a4d57dc
     v1.15.0: fe3c79070814fe847a23209b1027672fe5c5e7e5c9611e329225058926836f96
-    v1.14.6: d935de033e7442ce5f8a35294fa890b884454d0482a9cf136c4abacd8c6ec165
-    v1.14.5: 7dd1195d16980c4c888d13e49d97c3513f668e192bf2778bc0f0516e0f7fe2ac
-    v1.14.4: 60745b3ac761d3aa55ab9a24677ecf4e7f48b5abed34c725047a174456e5a79b
-    v1.14.3: 8edcc07c65f81eea3fc47cd237dd6560c6907c5e0ca52d71eab53ca1164e7d01
-    v1.14.2: bff0712b87796509129aa802ad3ac25b8cc83af01762b22b4dcca8dbdb26b520
-    v1.14.1: 5cf05464168e45ee4719264a267c65f9319fae1ceb9923fedab97a9d6a629e0b
-    v1.14.0: 7ed9d706e50cd6d3fc618a7af3d19b691b8a5343ddedaeccb4ea09af3ecfae2c
   amd64:
-    v1.16.0-beta.2: bba224360cfb4e6471f84523fcc954951c05c0fef0a4311a07e76f306cadebf1
+    v1.16.0: 18f30d65fb05148c73cc07c77a83f4a2427379af493ca9f60eda42239409e7ef
     v1.15.3: ec56a00bc8d9ec4ac2b081a3b2127d8593daf3b2c86560cf9e6cba5ada2d5a80
     v1.15.2: fe2a13a1dea73249560ea44ab54c0359a9722e9c66832f6bcad86798438cba2f
     v1.15.1: 3d42441ae177826f1181e559cd2a729464ca8efadef196cfa0e8053a615333b5
     v1.15.0: fc4aa44b96dc143d7c3062124e25fed671cab884ebb8b2446edd10abb45e88c2
-    v1.14.6: 4ef6030ab059ed434702c003975273dc855c370c4fcdae1109a3bb137c16ecb9
-    v1.14.5: b3e840f7816f64e071d25f8a90b984eecd6251b68e568b420d85ef0a4dd514bb
-    v1.14.4: 291790a1cef82c4de28cc3338a199ca8356838ca26f775f2c2acba165b633d9f
-    v1.14.3: 026700dfff3c78be1295417e96d882136e5e1f095eb843e6575e57ef9930b5d3
-    v1.14.2: 77510f61352bb6e537e70730b670627963f2c314fbd36a644b0c435b97e9705a
-    v1.14.1: c4fc478572b5623857f5d820e1c107ae02049ca02cf2993e512a091a0196957b
-    v1.14.0: 03678f49ee4737f8b8c4f59ace0d140a36ffbc4f6035c59561f59f45b57d0c93
 
 etcd_binary_checksums:
   # Etcd does not have arm32 builds at the moment, having some dummy value is
",check_sum,check_sum,check_sum,"kubernetes, crictlのバージョンアップに伴うcheck_sumの追加",バージョン情報とチェックサムの追加
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,tests/testcases/100_check-k8s-conformance.yml,MODIFY,5189,"@@ -1,7 +1,7 @@
 ---
 - hosts: kube-master[0]
   vars:
-    sonobuoy_version: 0.15.0
+    sonobuoy_version: 0.16.0
     sonobuoy_arch: amd64
     sonobuoy_parallel: 30
     sonobuoy_path: /usr/local/bin/sonobuoy
",dependency,dependency,dependency,コミットメッセージにある通り（bump sonobuoy version）,semverのバージョン変更
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubespray-defaults/defaults/main.yaml,MODIFY,5189,"@@ -12,10 +12,10 @@ is_atomic: false
 disable_swap: true
 
 ## Change this to use another Kubernetes version, e.g. a current beta release
-kube_version: v1.15.3
+kube_version: v1.16.0
 
 ## The minimum version working
-kube_version_min_required: v1.14.0
+kube_version_min_required: v1.15.0
 
 # use HyperKube image to control plane containers
 kubeadm_use_hyperkube_image: False
",dependency,dependency,dependency,kubernetesの要求バージョンの変更,semverのバージョン変更
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,roles/kubernetes/preinstall/tasks/0020-verify-settings.yml,MODIFY,5189,"@@ -241,12 +241,6 @@
   when: resolvconf_mode is defined
   run_once: true
 
-- name: Stop if k8s version is too low for kubeadm etcd mode
-  assert:
-    that: kube_version is version('v1.14.0', '>=')
-    msg: ""kubeadm etcd mode requires k8s version >= v1.14.0""
-  when: etcd_kubeadm_enabled
-
 - name: Stop if kubeadm etcd mode is enabled but experimental control plane is not
   assert:
     that: kubeadm_control_plane
",remove_task,remove_task,remove_task,kubernetesの要求バージョン引き上げに伴う不要なタスクの削除,タスクの削除
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,.gitlab-ci/terraform.yml,MODIFY,5189,"@@ -66,33 +66,33 @@ tf-validate-aws:
     PROVIDER: aws
     CLUSTER: $CI_COMMIT_REF_NAME
 
-tf-packet-ubuntu16-default:
-  extends: .terraform_apply
-  variables:
-    TF_VERSION: 0.11.11
-    PROVIDER: packet
-    CLUSTER: $CI_COMMIT_REF_NAME
-    TF_VAR_number_of_k8s_masters: ""1""
-    TF_VAR_number_of_k8s_nodes: ""1""
-    TF_VAR_plan_k8s_masters: t1.small.x86
-    TF_VAR_plan_k8s_nodes: t1.small.x86
-    TF_VAR_facility: ewr1
-    TF_VAR_public_key_path: """"
-    TF_VAR_operating_system: ubuntu_16_04
-
-tf-packet-ubuntu18-default:
-  extends: .terraform_apply
-  variables:
-    TF_VERSION: 0.11.11
-    PROVIDER: packet
-    CLUSTER: $CI_COMMIT_REF_NAME
-    TF_VAR_number_of_k8s_masters: ""1""
-    TF_VAR_number_of_k8s_nodes: ""1""
-    TF_VAR_plan_k8s_masters: t1.small.x86
-    TF_VAR_plan_k8s_nodes: t1.small.x86
-    TF_VAR_facility: ams1
-    TF_VAR_public_key_path: """"
-    TF_VAR_operating_system: ubuntu_18_04
+# tf-packet-ubuntu16-default:
+#   extends: .terraform_apply
+#   variables:
+#     TF_VERSION: 0.11.11
+#     PROVIDER: packet
+#     CLUSTER: $CI_COMMIT_REF_NAME
+#     TF_VAR_number_of_k8s_masters: ""1""
+#     TF_VAR_number_of_k8s_nodes: ""1""
+#     TF_VAR_plan_k8s_masters: t1.small.x86
+#     TF_VAR_plan_k8s_nodes: t1.small.x86
+#     TF_VAR_facility: ewr1
+#     TF_VAR_public_key_path: """"
+#     TF_VAR_operating_system: ubuntu_16_04
+#
+# tf-packet-ubuntu18-default:
+#   extends: .terraform_apply
+#   variables:
+#     TF_VERSION: 0.11.11
+#     PROVIDER: packet
+#     CLUSTER: $CI_COMMIT_REF_NAME
+#     TF_VAR_number_of_k8s_masters: ""1""
+#     TF_VAR_number_of_k8s_nodes: ""1""
+#     TF_VAR_plan_k8s_masters: t1.small.x86
+#     TF_VAR_plan_k8s_nodes: t1.small.x86
+#     TF_VAR_facility: ams1
+#     TF_VAR_public_key_path: """"
+#     TF_VAR_operating_system: ubuntu_18_04
 
 .ovh_variables: &ovh_variables
   OS_AUTH_URL: https://auth.cloud.ovh.net/v3
",other,other,other,gitlab-ci,GitLab用
a43e0d3f95d3e3b1eaf66b125b1f5603f896606b,2019-10-02 09:21:07+00:00,Switch to Kubernetes v1.16.0 (#5189) * Switch to Kubernetes v1.16.0 Change-Id: I5d6a9528b2d443750fc5e031aff15ad3ffead158 * Fix download localhost cached file path Change-Id: I65e79b70e3d1b37265ebc60f41b460cf4b0a0d47 * fix kubeadm etcd for v1.16 Change-Id: I6888a00fd48b530a38b0b31c4095492476af42d2 * disable tf packet jobs Change-Id: I075c4666547fdea4c50ec04864f38e2cfaa79154 * Disable contiv packet jobs. Fix kube-router Change-Id: I3170e8789e60711d4cee8faf65f2094480b79b8d * bump sonobuoy version Change-Id: Ib946905629c7c53ed88f08fb2f41c454457a0097,inventory/sample/group_vars/k8s-cluster/k8s-cluster.yml,MODIFY,5189,"@@ -20,7 +20,7 @@ kube_users_dir: ""{{ kube_config_dir }}/users""
 kube_api_anonymous_auth: true
 
 ## Change this to use another Kubernetes version, e.g. a current beta release
-kube_version: v1.15.3
+kube_version: v1.16.0
 
 # kubernetes image repo define
 kube_image_repo: ""gcr.io/google-containers""
",dependency,dependency,dependency,kubernetesの要求バージョンの引き上げ,semverのバージョン変更
8a5434419bea08771ad73c65447729fe51bacbd6,2019-11-11 11:27:41+00:00,fix useradd etcd (#5281),roles/adduser/defaults/main.yml,MODIFY,5281,"@@ -6,8 +6,7 @@ addusers:
   etcd:
     name: etcd
     comment: ""Etcd user""
-    createhome: yes
-    home: ""{{ etcd_data_dir }}""
+    createhome: no
     system: yes
     shell: /bin/nologin
   kube:
",edit_args,edit_args,edit_args,etcdモジュールの引数変更,createhomeをnoにするのが本質
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,inventory/sample/group_vars/all/gcp.yml,ADD,5857,"@@ -0,0 +1,10 @@
+## GCP compute Persistent Disk CSI Driver credentials and parameters
+## See docs/gcp-pd-csi.md for information about the implementation
+
+## Specify the path to the file containing the service account credentials
+# gcp_pd_csi_sa_cred_file: ""/my/safe/credentials/directory/cloud-sa.json""
+
+## To enable GCP Persistent Disk CSI driver, uncomment below
+# gcp_pd_csi_enabled: true
+# gcp_pd_csi_controller_replicas: 1
+# gcp_pd_csi_driver_image_tag: ""v0.7.0-gke.0""
",comment,other,other,コメントのみ変更,コメントの追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,inventory/sample/group_vars/k8s-cluster/k8s-cluster.yml,MODIFY,5857,"@@ -255,7 +255,7 @@ podsecuritypolicy_enabled: false
 ## See https://github.com/kubernetes-sigs/kubespray/issues/2141
 ## Set this variable to true to get rid of this issue
 volume_cross_zone_attachment: false
-# Add Persistent Volumes Storage Class for corresponding cloud provider (supported: in-tree OpenStack, Cinder CSI, AWS EBS CSI)
+# Add Persistent Volumes Storage Class for corresponding cloud provider (supported: in-tree OpenStack, Cinder CSI, AWS EBS CSI, GCP Persistent Disk CSI)
 persistent_volumes_enabled: false
 
 ## Container Engine Acceleration
",comment,other,other,コメントのみ変更,コメントの追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/kubernetes-apps/persistent_volumes/meta/main.yml,MODIFY,5857,"@@ -20,3 +20,10 @@ dependencies:
     tags:
       - persistent_volumes_aws_ebs_csi
       - aws-ebs-csi-driver
+
+  - role: kubernetes-apps/persistent_volumes/gcp-pd-csi
+    when:
+      - gcp_pd_csi_enabled
+    tags:
+      - persistent_volumes_gcp_pd_csi
+      - gcp-pd-csi-driver
",role,role,role,実行するロールの追加,roleの追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/kubernetes-apps/persistent_volumes/gcp-pd-csi/tasks/main.yml,ADD,5857,"@@ -0,0 +1,19 @@
+---
+- name: Kubernetes Persistent Volumes | Copy GCP PD CSI Storage Class template
+  template:
+    src: ""gcp-pd-csi-storage-class.yml.j2""
+    dest: ""{{ kube_config_dir }}/gcp-pd-csi-storage-class.yml""
+  register: manifests
+  when:
+    - inventory_hostname == groups['kube-master'][0]
+
+- name: Kubernetes Persistent Volumes | Add GCP PD CSI Storage Class
+  kube:
+    name: gcp-pd-csi
+    kubectl: ""{{ bin_dir }}/kubectl""
+    resource: StorageClass
+    filename: ""{{ kube_config_dir }}/gcp-pd-csi-storage-class.yml""
+    state: ""latest""
+  when:
+    - inventory_hostname == groups['kube-master'][0]
+    - manifests.changed
",add_task,add_task,add_task,新規ファイルとそのタスク,タスクの追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/kubernetes-apps/meta/main.yml,MODIFY,5857,"@@ -45,6 +45,14 @@ dependencies:
       - aws-ebs-csi-driver
       - csi-driver
 
+  - role: kubernetes-apps/csi_driver/gcp_pd
+    when:
+      - gcp_pd_csi_enabled
+    tags:
+      - apps
+      - gcp-pd-csi-driver
+      - csi-driver
+
   - role: kubernetes-apps/persistent_volumes
     when:
       - persistent_volumes_enabled
",role,role,role,実行するロールの追加,ロールの追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/kubernetes-apps/csi_driver/gcp_pd/tasks/main.yml,ADD,5857,"@@ -0,0 +1,49 @@
+---
+- name: GCP PD CSI Driver | Check if cloud-sa.json exists
+  fail:
+    msg: ""Credentials file cloud-sa.json is mandatory""
+  when: gcp_pd_csi_sa_cred_file is not defined or not gcp_pd_csi_sa_cred_file
+  tags: gcp-pd-csi-driver
+
+- name: GCP PD CSI Driver | Copy GCP credentials file
+  copy:
+    src: ""{{ gcp_pd_csi_sa_cred_file }}""
+    dest: ""{{ kube_config_dir }}/cloud-sa.json""
+    group: ""{{ kube_cert_group }}""
+    mode: 0640
+  when: inventory_hostname == groups['kube-master'][0]
+  tags: gcp-pd-csi-driver
+
+- name: GCP PD CSI Driver | Get base64 cloud-sa.json
+  slurp:
+    src: ""{{ kube_config_dir }}/cloud-sa.json""
+  register: gcp_cred_secret
+  when: inventory_hostname == groups['kube-master'][0]
+  tags: gcp-pd-csi-driver
+
+- name: GCP PD CSI Driver | Generate Manifests
+  template:
+    src: ""{{ item.file }}.j2""
+    dest: ""{{ kube_config_dir }}/{{ item.file }}""
+  with_items:
+    - {name: gcp-pd-csi-cred-secret, file: gcp-pd-csi-cred-secret.yml}
+    - {name: gcp-pd-csi-setup, file: gcp-pd-csi-setup.yml}
+    - {name: gcp-pd-csi-controller, file: gcp-pd-csi-controller.yml}
+    - {name: gcp-pd-csi-node, file: gcp-pd-csi-node.yml}
+  register: gcp_pd_csi_manifests
+  when: inventory_hostname == groups['kube-master'][0]
+  tags: gcp-pd-csi-driver
+
+- name: GCP PD CSI Driver | Apply Manifests
+  kube:
+    kubectl: ""{{ bin_dir }}/kubectl""
+    filename: ""{{ kube_config_dir }}/{{ item.item.file }}""
+    state: ""latest""
+  with_items:
+    - ""{{ gcp_pd_csi_manifests.results }}""
+  when:
+    - inventory_hostname == groups['kube-master'][0]
+    - not item is skipped
+  loop_control:
+    label: ""{{ item.item.file }}""
+  tags: gcp-pd-csi-driver
",add_task,add_task,add_task,新規ファイルとそのタスク,タスクの追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/kubernetes-apps/csi_driver/gcp_pd/defaults/main.yml,ADD,5857,"@@ -0,0 +1,3 @@
+---
+gcp_pd_csi_controller_replicas: 1
+gcp_pd_csi_driver_image_tag: ""v0.7.0-gke.0""
",edit_var,edit_var,edit_var,デフォルト変数の追加,変数の追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/download/defaults/main.yml,MODIFY,5857,"@@ -531,6 +531,13 @@ cinder_csi_plugin_image_tag: ""latest""
 aws_ebs_csi_plugin_image_repo: ""{{ docker_image_repo }}/amazon/aws-ebs-csi-driver""
 aws_ebs_csi_plugin_image_tag: ""latest""
 
+gcp_pd_csi_image_repo: ""gke.gcr.io""
+gcp_pd_csi_driver_image_tag: ""v0.7.0-gke.0""
+gcp_pd_csi_provisioner_image_tag: ""v1.5.0-gke.0""
+gcp_pd_csi_attacher_image_tag: ""v2.1.1-gke.0""
+gcp_pd_csi_resizer_image_tag: ""v0.4.0-gke.0""
+gcp_pd_csi_registrar_image_tag: ""v1.2.0-gke.0""
+
 dashboard_image_repo: ""{{ gcr_image_repo }}/google_containers/kubernetes-dashboard-{{ image_arch }}""
 dashboard_image_tag: ""v1.10.1""
 
",dependency,dependency,dependency,gcpのcsiドライバのタグ情報,依存バージョン情報の追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/kubespray-defaults/defaults/main.yaml,MODIFY,5857,"@@ -305,6 +305,7 @@ local_volume_provisioner_enabled: ""{{ local_volumes_enabled | default('false') }
 local_volume_provisioner_directory_mode: 0700
 cinder_csi_enabled: false
 aws_ebs_csi_enabled: false
+gcp_pd_csi_enabled: false
 persistent_volumes_enabled: false
 cephfs_provisioner_enabled: false
 rbd_provisioner_enabled: false
",edit_var,edit_var,edit_var,デフォルト変数の追加,変数の追加
484df62c5a506af8f6a7681a76fbf8e8a60298e3,2020-03-31 07:06:40+00:00,GCP Persistent Disk CSI Driver deployment (#5857) * GCP Persistent Disk CSI Driver deployment * Fix MD lint * Fix Yaml lint,roles/kubernetes-apps/persistent_volumes/gcp-pd-csi/defaults/main.yml,ADD,5857,"@@ -0,0 +1,8 @@
+---
+# Choose between pd-standard and pd-ssd
+gcp_pd_csi_volume_type: pd-standard
+gcp_pd_regional_replication_enabled: false
+gcp_pd_restrict_zone_replication: false
+gcp_pd_restricted_zones:
+  - europe-west1-b
+  - europe-west1-c
",edit_var,edit_var,edit_var,デフォルト変数の追加,変数の追加
98e7a07fbae671c3651e3c687398356362ebb5cd,2020-04-25 10:55:28+00:00,bump to dashboard 2.0.0 with metrics scrapper support (#5821) * bump to dashboard 2.0 rc6 with metrics scrapper * fix missing yaml seperator making Replicaset complaining about missing ServiceAccount * unwanted legay gross hack forgot to remove before * no need namespace on CrBinding * bump to 2.0.0 release * remove dashboard_metrics_scrapper_enabled,roles/download/defaults/main.yml,MODIFY,5821,"@@ -597,8 +597,10 @@ gcp_pd_csi_attacher_image_tag: ""v2.1.1-gke.0""
 gcp_pd_csi_resizer_image_tag: ""v0.4.0-gke.0""
 gcp_pd_csi_registrar_image_tag: ""v1.2.0-gke.0""
 
-dashboard_image_repo: ""{{ gcr_image_repo }}/google_containers/kubernetes-dashboard-{{ image_arch }}""
-dashboard_image_tag: ""v1.10.1""
+dashboard_image_repo: ""{{ docker_image_repo }}/kubernetesui/dashboard-{{ image_arch }}""
+dashboard_image_tag: ""v2.0.0""
+dashboard_metrics_scraper_repo: ""{{ docker_image_repo }}/kubernetesui/metrics-scraper""
+dashboard_metrics_scraper_tag: ""v1.0.4""
 
 image_pull_command: ""{{ docker_bin_dir }}/docker pull""
 image_save_command: ""{{ docker_bin_dir }}/docker save {{ image_reponame }} | gzip -{{ download_compress }} > {{ image_path_final }}""
@@ -1137,6 +1139,15 @@ downloads:
     groups:
       - kube-master
 
+  dashboard_metrics_scrapper:
+    enabled: ""{{ dashboard_enabled }}""
+    container: true
+    repo: ""{{ dashboard_metrics_scraper_repo }}""
+    tag: ""{{ dashboard_metrics_scraper_tag }}""
+    sha256: ""{{ dashboard_digest_checksum|default(None) }}""
+    groups:
+      - kube-master
+
 download_defaults:
   container: false
   file: false
",dependency,dependency,edit_var,"コミットメッセージより，バージョンのbumpが目的であること
（bump to dashboard 2.0.0 with metrics scrapper support）

また，dashboard_metrics_scrapper　変数は新規に追加されたバージョン情報に関する変数 dashboard_metrics_scrapper_repo，dashboard_metrics_scraper_tagを利用しているため",変数の追加と変更
98e7a07fbae671c3651e3c687398356362ebb5cd,2020-04-25 10:55:28+00:00,bump to dashboard 2.0.0 with metrics scrapper support (#5821) * bump to dashboard 2.0 rc6 with metrics scrapper * fix missing yaml seperator making Replicaset complaining about missing ServiceAccount * unwanted legay gross hack forgot to remove before * no need namespace on CrBinding * bump to 2.0.0 release * remove dashboard_metrics_scrapper_enabled,roles/kubernetes-apps/ansible/defaults/main.yml,MODIFY,5821,"@@ -41,6 +41,9 @@ netchecker_server_group: 1000
 dashboard_enabled: true
 dashboard_replicas: 1
 
+# Namespace for dashboad
+dashboard_namespace: kube-system
+
 # Limits for dashboard
 dashboard_cpu_limit: 100m
 dashboard_memory_limit: 256M
",edit_var,edit_var,edit_var,変数の追加,変数の追加
98e7a07fbae671c3651e3c687398356362ebb5cd,2020-04-25 10:55:28+00:00,bump to dashboard 2.0.0 with metrics scrapper support (#5821) * bump to dashboard 2.0 rc6 with metrics scrapper * fix missing yaml seperator making Replicaset complaining about missing ServiceAccount * unwanted legay gross hack forgot to remove before * no need namespace on CrBinding * bump to 2.0.0 release * remove dashboard_metrics_scrapper_enabled,roles/kubernetes-apps/ansible/tasks/dashboard.yml,MODIFY,5821,"@@ -11,7 +11,7 @@
 - name: Kubernetes Apps | Start dashboard
   kube:
     name: ""{{ item.item.name }}""
-    namespace: ""kube-system""
+    namespace: ""{{ dashboard_namespace }}""
     kubectl: ""{{ bin_dir }}/kubectl""
     resource: ""{{ item.item.type }}""
     filename: ""{{ kube_config_dir }}/{{ item.item.file }}""
",edit_args,edit_args,edit_args,ハードコードされたパラメータを変数に置き換え，変更できるようにしている,引数の変更
ef7076e36f2528d11620602be7731db0637fe235,2020-05-08 12:57:42+00:00,"fix expected str instance, float found #6078 (#6103)",roles/download/defaults/main.yml,MODIFY,6078,"@@ -80,7 +80,7 @@ flannel_version: ""v0.12.0""
 cni_version: ""v0.8.5""
 
 weave_version: 2.6.2
-pod_infra_version: 3.1
+pod_infra_version: ""3.1""
 contiv_version: 1.2.1
 cilium_version: ""v1.7.3""
 kube_ovn_version: ""v1.1.1""
",edit_var,edit_var,edit_var,文字列を与えるべき箇所で浮動小数が使われていた,float型ではなく、str型にする
10e54eca26faa6f3e19c825e068ff54e6b6a0e3a,2020-06-16 07:34:06+00:00,make better condition for applying nf_conntrack kernel tweak (#6267) * MINOR: Check kernel version before enable modprobe nf_conntrack * CLEANUP: no more need to ignore error of this task * MINOR: Fixing yaml and ansible lint error - remove trailling-space,roles/kubernetes/node/tasks/main.yml,MODIFY,6267,"@@ -108,8 +108,9 @@
     name: nf_conntrack_ipv4
     state: present
   register: enable_nf_conntrack
-  ignore_errors: yes
-  when: kube_proxy_mode == 'ipvs'
+  when:
+    - ansible_kernel.split('.')[0:3] | join('.')  < '4.19'
+    - kube_proxy_mode == 'ipvs'
   tags:
     - kube-proxy
 
@@ -118,7 +119,7 @@
     name: nf_conntrack
     state: present
   when:
-    - enable_nf_conntrack is failed
+    - ansible_kernel.split('.')[0:3] | join('.')  >= '4.19'
     - kube_proxy_mode == 'ipvs'
   tags:
     - kube-proxy
",condition,condition,condition,"条件に"" - ansible_kernel.split('.')[0:3] | join('.')  < '4.19'""が追加されている",whenの内容の変更
6bb47d8adb1cd7e06480fc10824b3fbb1692f04c,2020-07-04 09:02:48+00:00,Fix can't remove etcd node (#6363) * add remove_node_ip * move remove_node_ip to remove etcd part * fix: remove tail space * fix: handle ubuntu: focal,roles/remove-node/post-remove/tasks/main.yml,MODIFY,6363,"@@ -1,16 +1,4 @@
 ---
-- name: Lookup node IP in kubernetes
-  shell: >-
-    {{ bin_dir }}/kubectl get nodes {{ node }}
-    -o jsonpath='{range.status.addresses[?(@.type==""InternalIP"")]}{.address}{""\n""}{end}'
-  register: remove_node_ip
-  when:
-    - inventory_hostname in groups['etcd']
-    - ip is not defined
-    - access_ip is not defined
-  delegate_to: ""{{ groups['etcd']|first }}""
-  failed_when: false
-
 - name: Delete node
   command: ""{{ bin_dir }}/kubectl delete node {{ inventory_hostname }}""
   delegate_to: ""{{ groups['kube-master']|first }}""
",move,move,move,roles/remove-node/remove-etcd-node/tasks/main.ymlに移動,155行目のファイルに移行している
6bb47d8adb1cd7e06480fc10824b3fbb1692f04c,2020-07-04 09:02:48+00:00,Fix can't remove etcd node (#6363) * add remove_node_ip * move remove_node_ip to remove etcd part * fix: remove tail space * fix: handle ubuntu: focal,roles/remove-node/remove-etcd-node/tasks/main.yml,MODIFY,6363,"@@ -1,4 +1,16 @@
 ---
+- name: Lookup node IP in kubernetes
+  shell: >-
+    {{ bin_dir }}/kubectl get nodes {{ node }}
+    -o jsonpath='{range.status.addresses[?(@.type==""InternalIP"")]}{.address}{""\n""}{end}'
+  register: remove_node_ip
+  when:
+    - inventory_hostname in groups['etcd']
+    - ip is not defined
+    - access_ip is not defined
+  delegate_to: ""{{ groups['etcd']|first }}""
+  failed_when: false
+
 - name: Set node IP
   set_fact:
     node_ip: ""{{ ip | default(access_ip | default(remove_node_ip.stdout)) | trim }}""
",move,move,move,roles/remove-node/post-remove/tasks/main.ymlから移動,154行目のファイルから移行
6bb47d8adb1cd7e06480fc10824b3fbb1692f04c,2020-07-04 09:02:48+00:00,Fix can't remove etcd node (#6363) * add remove_node_ip * move remove_node_ip to remove etcd part * fix: remove tail space * fix: handle ubuntu: focal,roles/reset/tasks/main.yml,MODIFY,6363,"@@ -1,5 +1,4 @@
 ---
-
 - name: reset | include file with pre-reset tasks specific to the network_plugin if exists
   include_tasks: ""{{ (role_path + '/../network_plugin/' + kube_network_plugin + '/tasks/pre-reset.yml') | realpath  }}""
   when:
@@ -142,7 +141,7 @@
 
 - name: reset | unmount kubelet dirs
   command: umount -f {{ item }}
-  with_items: '{{ mounted_dirs.stdout_lines }}'
+  with_items: ""{{ mounted_dirs.stdout_lines }}""
   register: umount_dir
   retries: 4
   until: umount_dir.rc == 0
@@ -296,7 +295,7 @@
       {%- else -%}
       network
       {%- endif -%}
-      {%- elif ansible_distribution == ""Ubuntu"" and ansible_distribution_release == ""bionic"" -%}
+      {%- elif ansible_distribution == ""Ubuntu"" and ansible_distribution_release in [""bionic"", ""focal""] -%}
       systemd-networkd
       {%- elif ansible_os_family == ""Debian"" -%}
       networking
",jinja2,jinja2,jinja2,条件に'focal'が追加されている,jinja2のelifの中身の変更が本質
3eefb5f2ad40df94158eb4ac4aad911759166c92,2020-10-21 14:32:32+00:00,"fix scaling in kubeadm etcd mode (#6822) 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'kubeadm_upload_cert' kubeadm_upload_cert will never be found as a hostvar for the first master since the task is executed for a worker. Fix by executing the upload task for the first master and register the needed key. After that, workers can read hostvars for the master Var kubeadm_etcd_refresh_cert_key removed since it no longer has any use.",roles/kubernetes/kubeadm/defaults/main.yml,MODIFY,6822,"@@ -11,8 +11,5 @@ kube_override_hostname: >-
   {{ inventory_hostname }}
   {%- endif -%}
 
-# Requests a fresh upload of certificates from first master
-kubeadm_etcd_refresh_cert_key: true
-
 # Experimental kubeadm etcd deployment mode. Available only for new deployment
 etcd_kubeadm_enabled: false
",edit_var,edit_var,edit_var,使われなくなった変数の削除,変数の追加
3eefb5f2ad40df94158eb4ac4aad911759166c92,2020-10-21 14:32:32+00:00,"fix scaling in kubeadm etcd mode (#6822) 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'kubeadm_upload_cert' kubeadm_upload_cert will never be found as a hostvar for the first master since the task is executed for a worker. Fix by executing the upload task for the first master and register the needed key. After that, workers can read hostvars for the master Var kubeadm_etcd_refresh_cert_key removed since it no longer has any use.",scale.yml,MODIFY,6822,"@@ -74,6 +74,25 @@
     - { role: kubernetes/node, tags: node }
   environment: ""{{ proxy_env }}""
 
+- name: Upload control plane certs and retrieve encryption key
+  hosts: kube-master | first
+  tags: kubeadm
+  tasks:
+    - name: include needed vars
+      include_vars: roles/kubespray-defaults/defaults/main.yaml
+    - name: Upload control plane certificates
+      command: >-
+        {{ bin_dir }}/kubeadm init phase
+        --config {{ kube_config_dir }}/kubeadm-config.yaml
+        upload-certs
+        --upload-certs
+      register: kubeadm_upload_cert
+      changed_when: false
+    - name: set fact 'kubeadm_certificate_key' for later use
+      set_fact:
+        kubeadm_certificate_key: ""{{ kubeadm_upload_cert.stdout_lines[-1] | trim }}""
+      when: kubeadm_certificate_key is not defined
+
 - name: Target only workers to get kubelet installed and checking in on any new nodes(network)
   hosts: kube-node
   gather_facts: False
",move,move,add_task,roles/kubernetes/kubeadm/tasks/kubeadm_etcd_node.ymlからタスクが移動している,タスクの追加
3eefb5f2ad40df94158eb4ac4aad911759166c92,2020-10-21 14:32:32+00:00,"fix scaling in kubeadm etcd mode (#6822) 'ansible.vars.hostvars.HostVarsVars object' has no attribute 'kubeadm_upload_cert' kubeadm_upload_cert will never be found as a hostvar for the first master since the task is executed for a worker. Fix by executing the upload task for the first master and register the needed key. After that, workers can read hostvars for the master Var kubeadm_etcd_refresh_cert_key removed since it no longer has any use.",roles/kubernetes/kubeadm/tasks/kubeadm_etcd_node.yml,MODIFY,6822,"@@ -1,18 +1,7 @@
 ---
-- name: Refresh certificates so they are fresh and not expired
-  command: >-
-    {{ bin_dir }}/kubeadm init phase
-    --config {{ kube_config_dir }}/kubeadm-config.yaml
-    upload-certs
-    --upload-certs
-  register: kubeadm_upload_cert
-  delegate_to: ""{{ groups['kube-master'][0] }}""
-  when: kubeadm_etcd_refresh_cert_key
-  run_once: yes
-
 - name: Parse certificate key if not set
   set_fact:
-    kubeadm_certificate_key: ""{{ hostvars[groups['kube-master'][0]]['kubeadm_upload_cert'].stdout_lines[-1] | trim }}""
+    kubeadm_certificate_key: ""{{ hostvars[groups['kube-master'][0]]['kubeadm_certificate_key'] }}""
   when: kubeadm_certificate_key is undefined
 
 - name: Pull control plane certs down
",move,move,remove_task,scale.ymlに 'upload control plane certificates'という名前で移動している,タスクの削除の方が大きそう
95b329b64d571dc7e4aba9696db998ee95cbd1cb,2021-02-11 16:14:16+00:00,bootstrap-os: match on os-release ID / VARIANT_ID (#7269) This fixes deployment with CentOS 8 Streams and make detection more reliable Signed-off-by: Etienne Champetier <e.champetier@ateme.com>,roles/bootstrap-os/tasks/bootstrap-debian.yml,MODIFY,7269,"@@ -51,20 +51,20 @@
   # This command should always run, even in check mode
   check_mode: false
   when:
-    - '""bionic"" in os_release.stdout'
+    - '''UBUNTU_CODENAME=bionic'' in os_release.stdout_lines'
 
 - name: Change Network Name Resolution configuration
   raw: sed -i 's/^DNSSEC=yes/DNSSEC=allow-downgrade/g' /etc/systemd/resolved.conf
   become: true
   when:
-    - '""bionic"" in os_release.stdout'
+    - '''UBUNTU_CODENAME=bionic'' in os_release.stdout_lines'
     - need_dnssec_allow_downgrade.rc
 
 - name: Restart systemd-resolved service
   raw: systemctl restart systemd-resolved
   become: true
   when:
-    - '""bionic"" in os_release.stdout'
+    - '''UBUNTU_CODENAME=bionic'' in os_release.stdout_lines'
     - need_dnssec_allow_downgrade.rc
 
 - name: Install python3
",condition,condition,condition,判定の内容は変わらないがその方法が少し異なる,whenの内容の変更
95b329b64d571dc7e4aba9696db998ee95cbd1cb,2021-02-11 16:14:16+00:00,bootstrap-os: match on os-release ID / VARIANT_ID (#7269) This fixes deployment with CentOS 8 Streams and make detection more reliable Signed-off-by: Etienne Champetier <e.champetier@ateme.com>,roles/bootstrap-os/tasks/main.yml,MODIFY,7269,"@@ -7,32 +7,34 @@
   check_mode: false
 
 - include_tasks: bootstrap-centos.yml
-  when: '""CentOS"" in os_release.stdout or ""Oracle"" in os_release.stdout'
+  when: '''ID=""centos""'' in os_release.stdout_lines or ''ID=""ol""'' in os_release.stdout_lines'
 
 - include_tasks: bootstrap-redhat.yml
-  when: '""Red Hat Enterprise Linux"" in os_release.stdout and ""CentOS"" not in os_release.stdout'
+  when: '''ID=""rhel""'' in os_release.stdout_lines'
 
 - include_tasks: bootstrap-clearlinux.yml
-  when: '""Clear Linux OS"" in os_release.stdout'
+  when: '''ID=clear-linux-os'' in os_release.stdout_lines'
 
+# Fedora CoreOS
 - include_tasks: bootstrap-fedora-coreos.yml
-  when: '""ID=fedora"" in os_release.stdout and ""VARIANT_ID=coreos"" in os_release.stdout'
+  when:
+    - '''ID=fedora'' in os_release.stdout_lines'
+    - '''VARIANT_ID=coreos'' in os_release.stdout_lines'
 
 - include_tasks: bootstrap-flatcar.yml
-  when:
-    - '""Flatcar"" in os_release.stdout'
-    - '""ID=fedora"" not in os_release.stdout'
+  when: '''ID=flatcar'' in os_release.stdout_lines'
 
 - include_tasks: bootstrap-debian.yml
-  when: '""Debian"" in os_release.stdout or ""Ubuntu"" in os_release.stdout'
+  when: '''ID=debian'' in os_release.stdout_lines or ''ID=ubuntu'' in os_release.stdout_lines'
 
+# Fedora ""classic""
 - include_tasks: bootstrap-fedora.yml
   when:
-    - '""Fedora"" in os_release.stdout'
-    - '""VARIANT_ID=coreos"" not in os_release.stdout'
+    - '''ID=fedora'' in os_release.stdout_lines'
+    - '''VARIANT_ID=coreos'' not in os_release.stdout_lines'
 
 - include_tasks: bootstrap-opensuse.yml
-  when: '""openSUSE"" in os_release.stdout'
+  when: '''ID=""opensuse-leap""'' in os_release.stdout_lines or ''ID=""opensuse-tumbleweed""'' in os_release.stdout_lines'
 
 - name: Create remote_tmp for it is used by another module
   file:
",condition,condition,condition,判定の内容は変わらないがその方法が少し異なる,whenの内容の変更
95b329b64d571dc7e4aba9696db998ee95cbd1cb,2021-02-11 16:14:16+00:00,bootstrap-os: match on os-release ID / VARIANT_ID (#7269) This fixes deployment with CentOS 8 Streams and make detection more reliable Signed-off-by: Etienne Champetier <e.champetier@ateme.com>,roles/bootstrap-os/tasks/bootstrap-centos.yml,MODIFY,7269,"@@ -22,7 +22,7 @@
     dest: /etc/yum.repos.d/public-yum-ol7.repo
   when:
     - use_oracle_public_repo|default(true)
-    - '""Oracle"" in os_release.stdout'
+    - '''ID=""ol""'' in os_release.stdout_lines'
     - (ansible_distribution_version | float) < 7.6
   environment: ""{{ proxy_env }}""
 
@@ -38,7 +38,7 @@
     - ol7_developer_EPEL
   when:
     - use_oracle_public_repo|default(true)
-    - '""Oracle"" in os_release.stdout'
+    - '''ID=""ol""'' in os_release.stdout_lines'
     - (ansible_distribution_version | float) < 7.6
 
 - name: Enable Oracle Linux repo
@@ -52,7 +52,7 @@
     - { option: ""baseurl"", value: ""http://yum.oracle.com/repo/OracleLinux/OL{{ ansible_distribution_major_version }}/addons/x86_64/"" }
   when:
     - use_oracle_public_repo|default(true)
-    - '""Oracle"" in os_release.stdout'
+    - '''ID=""ol""'' in os_release.stdout_lines'
     - (ansible_distribution_version | float) >= 7.6
 
 - name: Install EPEL for Oracle Linux repo package
@@ -61,7 +61,7 @@
     state: present
   when:
     - use_oracle_public_repo|default(true)
-    - '""Oracle"" in os_release.stdout'
+    - '''ID=""ol""'' in os_release.stdout_lines'
     - (ansible_distribution_version | float) >= 7.6
 
 # CentOS ships with python installed
",condition,condition,condition,判定の内容は変わらないがその方法が少し異なる,whenの内容の変更
d53fd29e34f46b453c28f5400549ff83b91cf284,2021-03-23 20:46:06+00:00,Add support for cilium ipsec (#7342) * Add support for cilium ipsec * Fix typo for bpffs,roles/network_plugin/cilium/tasks/main.yml,MODIFY,7342,"@@ -1,47 +1,4 @@
 ---
-- name: Cilium | Ensure BFPFS mounted
-  mount:
-    fstype: bpf
-    path: /sys/fs/bpf
-    src: bpffs
-    state: mounted
+- import_tasks: check.yml
 
-- name: Cilium | Create Cilium certs directory
-  file:
-    dest: ""{{ cilium_cert_dir }}""
-    state: directory
-    mode: 0750
-    owner: root
-    group: root
-
-- name: Cilium | Link etcd certificates for cilium
-  file:
-    src: ""{{ etcd_cert_dir }}/{{ item.s }}""
-    dest: ""{{ cilium_cert_dir }}/{{ item.d }}""
-    state: hard
-    force: yes
-  with_items:
-    - {s: ""{{ kube_etcd_cacert_file }}"", d: ""ca_cert.crt""}
-    - {s: ""{{ kube_etcd_cert_file }}"", d: ""cert.crt""}
-    - {s: ""{{ kube_etcd_key_file }}"", d: ""key.pem""}
-
-- name: Cilium | Create Cilium node manifests
-  template:
-    src: ""{{ item.file }}.j2""
-    dest: ""{{ kube_config_dir }}/{{ item.file }}""
-  with_items:
-    - {name: cilium, file: cilium-config.yml, type: cm}
-    - {name: cilium, file: cilium-crb.yml, type: clusterrolebinding}
-    - {name: cilium, file: cilium-cr.yml, type: clusterrole}
-    - {name: cilium, file: cilium-ds.yml, type: ds}
-    - {name: cilium, file: cilium-deploy.yml, type: deploy}
-    - {name: cilium, file: cilium-sa.yml, type: sa}
-  register: cilium_node_manifests
-  when:
-    - inventory_hostname in groups['kube-master']
-
-- name: Cilium | Enable portmap addon
-  template:
-    src: 000-cilium-portmap.conflist.j2
-    dest: /etc/cni/net.d/000-cilium-portmap.conflist
-  when: cilium_enable_portmap
+- include_tasks: install.yml
\ No newline at end of file
",move,move,move,task/mainではinclude.ymlだけを担うよう変更,166行目のファイルに移行
d53fd29e34f46b453c28f5400549ff83b91cf284,2021-03-23 20:46:06+00:00,Add support for cilium ipsec (#7342) * Add support for cilium ipsec * Fix typo for bpffs,roles/network_plugin/cilium/defaults/main.yml,MODIFY,7342,"@@ -51,3 +51,6 @@ cilium_deploy_additionally: false
 # information about this kind of setups.
 cilium_auto_direct_node_routes: false
 cilium_native_routing_cidr: """"
+
+# IPsec based transparent encryption between nodes
+cilium_ipsec_enabled: false
\ No newline at end of file
",edit_var,edit_var,edit_var,新たな変数'+cilium_ipsec_enabled: false'を追加,変数の定義
d53fd29e34f46b453c28f5400549ff83b91cf284,2021-03-23 20:46:06+00:00,Add support for cilium ipsec (#7342) * Add support for cilium ipsec * Fix typo for bpffs,roles/network_plugin/cilium/tasks/check.yml,ADD,7342,"@@ -0,0 +1,9 @@
+---
+- name: Cilium | Check cilium_ipsec_enabled variables
+  assert:
+    that:
+      - ""cilium_ipsec_key is defined""
+    msg: ""cilium_ipsec_key should be defined to use cilium_ipsec_enabled""
+  when:
+    - cilium_ipsec_enabled
+    - cilium_tunnel_mode in ['vxlan']
\ No newline at end of file
",add_task,add_task,add_task,assertを行うタスクの追加,新しくタスクが追加されている
d53fd29e34f46b453c28f5400549ff83b91cf284,2021-03-23 20:46:06+00:00,Add support for cilium ipsec (#7342) * Add support for cilium ipsec * Fix typo for bpffs,roles/network_plugin/cilium/tasks/install.yml,ADD,7342,"@@ -0,0 +1,48 @@
+---
+- name: Cilium | Ensure BPFFS mounted
+  mount:
+    fstype: bpf
+    path: /sys/fs/bpf
+    src: bpffs
+    state: mounted
+
+- name: Cilium | Create Cilium certs directory
+  file:
+    dest: ""{{ cilium_cert_dir }}""
+    state: directory
+    mode: 0750
+    owner: root
+    group: root
+
+- name: Cilium | Link etcd certificates for cilium
+  file:
+    src: ""{{ etcd_cert_dir }}/{{ item.s }}""
+    dest: ""{{ cilium_cert_dir }}/{{ item.d }}""
+    state: hard
+    force: yes
+  with_items:
+    - {s: ""{{ kube_etcd_cacert_file }}"", d: ""ca_cert.crt""}
+    - {s: ""{{ kube_etcd_cert_file }}"", d: ""cert.crt""}
+    - {s: ""{{ kube_etcd_key_file }}"", d: ""key.pem""}
+
+- name: Cilium | Create Cilium node manifests
+  template:
+    src: ""{{ item.file }}.j2""
+    dest: ""{{ kube_config_dir }}/{{ item.file }}""
+  with_items:
+    - {name: cilium, file: cilium-config.yml, type: cm}
+    - {name: cilium, file: cilium-crb.yml, type: clusterrolebinding}
+    - {name: cilium, file: cilium-cr.yml, type: clusterrole}
+    - {name: cilium, file: cilium-secret.yml, type: secret, when: cilium_ipsec_enabled}
+    - {name: cilium, file: cilium-ds.yml, type: ds}
+    - {name: cilium, file: cilium-deploy.yml, type: deploy}
+    - {name: cilium, file: cilium-sa.yml, type: sa}
+  register: cilium_node_manifests
+  when:
+    - inventory_hostname in groups['kube-master']
+
+- name: Cilium | Enable portmap addon
+  template:
+    src: 000-cilium-portmap.conflist.j2
+    dest: /etc/cni/net.d/000-cilium-portmap.conflist
+  when: cilium_enable_portmap
",move,move,move,task/main.ymlに記述された内容の移動,163行目のファイルから移行
844ebb7838dc2c4419550bbc68e323a615d47bfc,2021-04-13 07:46:50+00:00,fix offline mode (#7493) * fix offline mode * add offline messages,inventory/sample/group_vars/all/offline.yml,RENAME,7493,"@@ -35,6 +35,15 @@
 # [Optional] Calico with kdd: If using Calico network plugin with kdd datastore
 # calico_crds_download_url: ""{{ files_repo }}/kubernetes/calico/{{ calico_version }}.tar.gz""
 
+# [Optional] helm: only if you set helm_enabled: true
+helm_download_url: ""{{ files_repo }}/helm-{{ helm_version }}-linux-{{ image_arch }}.tar.gz""
+
+# [Optional] crun: only if you set crun_enabled: true
+crun_download_url: ""{{ files_repo }}/containers/crun/releases/download/{{ crun_version }}/crun-{{ crun_version }}-linux-{{ image_arch }}""
+
+# [Optional] kata: only if you set kata_containers_enabled: true
+kata_containers_download_url: ""{{ files_repo }}/kata-containers/runtime/releases/download/{{ kata_containers_version }}/kata-static-{{ kata_containers_version }}-{{ ansible_architecture }}.tar.xz""
+
 ## CentOS/Redhat
 ### For EL7, base and extras repo must be available, for EL8, baseos and appstream
 ### By default we enable those repo automatically
",edit_var,edit_var,edit_var,変数の追加,変数の追加
649f962ac6d1aa44625a6c337d4579cfe99e37c9,2021-10-27 22:15:11+00:00,Metrics-server Deployment has incongruencies in resources requests/limits (#8088) * fix(metrics-server): update defaults * fix(metrics-server): typo error,roles/kubernetes-apps/metrics_server/defaults/main.yml,MODIFY,8088,"@@ -3,8 +3,8 @@ metrics_server_resizer: false
 metrics_server_kubelet_insecure_tls: true
 metrics_server_kubelet_preferred_address_types: ""InternalIP""
 metrics_server_metric_resolution: 15s
-metrics_server_cpu: 20m
-metrics_server_memory: 15Mi
+metrics_server_cpu: 190m
+metrics_server_memory: 180Mi
 metrics_server_memory_per_node: 2Mi
 metrics_server_min_cluster_size: 10
 metrics_server_limits_cpu: 100m
",edit_var,edit_var,edit_var,cpu及びメモリーの値の変更,変数の変更
c94291558d93ebf5106a85157fde827edbe0c09c,2021-11-05 14:53:53+00:00,Fix containerd install for fcos (#8107) * Fix containerd install for fcos * rm orphaned runc and containerd binaries,roles/container-engine/runc/defaults/main.yml,MODIFY,8107,"@@ -1,5 +1,5 @@
 ---
 
-runc_bin_dir: /usr/bin/
+runc_bin_dir: ""{{ bin_dir }}""
 
 runc_package_name: runc
",hard_coded,edit_var,filepath,ハードコードを変数化,ファイルパスをテンプレートで置き換える
c94291558d93ebf5106a85157fde827edbe0c09c,2021-11-05 14:53:53+00:00,Fix containerd install for fcos (#8107) * Fix containerd install for fcos * rm orphaned runc and containerd binaries,roles/container-engine/runc/tasks/main.yml,MODIFY,8107,"@@ -1,8 +1,14 @@
 ---
+- name: runc | set is_ostree
+  set_fact:
+    is_ostree: ""{{ ostree.stat.exists }}""
+
 - name: runc | Uninstall runc package managed by package manager
   package:
     name: ""{{ runc_package_name }}""
     state: absent
+  when:
+    - not (is_ostree or (ansible_distribution == ""Flatcar Container Linux by Kinvolk"") or (ansible_distribution == ""Flatcar""))
 
 - name: runc | Download runc binary
   include_tasks: ""../../../download/tasks/download_file.yml""
@@ -15,3 +21,10 @@
     dest: ""{{ runc_bin_dir }}/runc""
     mode: 0755
     remote_src: true
+
+- name: runc | Remove orphaned binary
+  file:
+    path: /usr/bin/runc
+    state: absent
+  when: runc_bin_dir != ""/usr/bin""
+  ignore_errors: true  # noqa ignore-errors
",add_task,add_task,add_task,"コミットメッセージ（rm orphaned runc and containerd binaries）から変更の目的をorphaned binaryとして判断した
カテゴリの競合
- edit_var: set_factによる登録
- condition: when条件の追加
- add_task: 'runc | Remove orphaned binary' タスクの追加",タスク追加
c94291558d93ebf5106a85157fde827edbe0c09c,2021-11-05 14:53:53+00:00,Fix containerd install for fcos (#8107) * Fix containerd install for fcos * rm orphaned runc and containerd binaries,roles/kubespray-defaults/defaults/main.yaml,MODIFY,8107,"@@ -67,7 +67,7 @@ haproxy_config_dir: ""/etc/haproxy""
 # Directory where the binaries will be installed
 bin_dir: /usr/local/bin
 docker_bin_dir: /usr/bin
-containerd_bin_dir: /usr/bin
+containerd_bin_dir: ""{{ bin_dir }}""
 etcd_data_dir: /var/lib/etcd
 # Where the binaries will be downloaded.
 # Note: ensure that you've enough disk space (about 1G)
",hard_coded,edit_var,filepath,ハードコードを変数化,ファイルパスをテンプレートで置き換える
c94291558d93ebf5106a85157fde827edbe0c09c,2021-11-05 14:53:53+00:00,Fix containerd install for fcos (#8107) * Fix containerd install for fcos * rm orphaned runc and containerd binaries,roles/container-engine/containerd/tasks/main.yml,MODIFY,8107,"@@ -57,6 +57,19 @@
       - --strip-components=1
   notify: restart containerd
 
+- name: containerd | Remove orphaned binary
+  file:
+    path: ""/usr/bin/{{ item }}""
+    state: absent
+  when: containerd_bin_dir != ""/usr/bin""
+  ignore_errors: true  # noqa ignore-errors
+  with_items:
+    - containerd
+    - containerd-shim
+    - containerd-shim-runc-v1
+    - containerd-shim-runc-v2
+    - ctr
+
 - name: containerd | Generate systemd service for containerd
   template:
     src: containerd.service.j2
",add_task,add_task,add_task,orphaned binaryの削除を行うタスク,タスク追加
be9de6b9d9f1036656d59bcaeb8e6776d0e03240,2021-11-19 17:02:51+00:00,Fix debian 9 check for apt cache update (#8215),roles/bootstrap-os/tasks/bootstrap-debian.yml,MODIFY,8215,"@@ -56,10 +56,7 @@
   become: true
   when:
     - '''ID=debian'' in os_release.stdout_lines'
-    - (
-        '''VERSION=""10'' in os_release.stdout_lines' or
-        '''VERSION=""11'' in os_release.stdout_lines'
-      )
+    - '''VERSION_ID=""10""'' in os_release.stdout_lines or ''VERSION_ID=""11""'' in os_release.stdout_lines'
   register: bootstrap_update_apt_result
   changed_when:
     - '""changed its"" in bootstrap_update_apt_result.stdout'
",condition,condition,condition,VERSION -> VERSION_IDに変更,whenの記述が適切ではなかった？内容には変更は無さそう
2f44b40d6846cee88c985d43ec87472397c5363b,2021-11-29 21:39:21+00:00,OEL7: Fix CentOS7 Extras for OEL7 (#8219) * OEL7: Fix CentOS7 Extras for OEL7 * Molecule: add logs collection for jobs,.gitlab-ci/vagrant.yml,MODIFY,8219,"@@ -16,6 +16,12 @@ molecule_tests:
     - ./tests/scripts/vagrant_clean.sh
   script:
     - ./tests/scripts/molecule_run.sh
+  after_script:
+    - chronic ./tests/scripts/molecule_logs.sh
+  artifacts:
+    when: always
+    paths:
+      - molecule_logs/
 
 .vagrant:
   extends: .testcases
",,other,directive,,directiveの追加
2f44b40d6846cee88c985d43ec87472397c5363b,2021-11-29 21:39:21+00:00,OEL7: Fix CentOS7 Extras for OEL7 (#8219) * OEL7: Fix CentOS7 Extras for OEL7 * Molecule: add logs collection for jobs,roles/bootstrap-os/tasks/bootstrap-centos.yml,MODIFY,8219,"@@ -79,7 +79,7 @@
     - { option: ""name"", value: ""CentOS-{{ ansible_distribution_major_version }} - Extras"" }
     - { option: ""enabled"", value: ""1"" }
     - { option: ""gpgcheck"", value: ""0"" }
-    - { option: ""baseurl"", value: ""http://mirror.centos.org/centos/{{ ansible_distribution_major_version }}/extras/$basearch/os/"" }
+    - { option: ""baseurl"", value: ""http://mirror.centos.org/centos/{{ ansible_distribution_major_version }}/extras/$basearch/{% if ansible_distribution_major_version|int > 7 %}os/{% endif %}"" }
   when:
     - use_oracle_public_repo|default(true)
     - '''ID=""ol""'' in os_release.stdout_lines'
",jinja2,jinja2,jinja2,条件の付与,jinja2の制御が利用されている
3c6fa6e583aeadf8d0f2f7e3b167e8ce4b976d60,2021-11-30 09:06:56+00:00,"offline install using containerd runtime (#8254) install containerd on centos need to binary download it but offline.yml has no that value binary download url default in roles/download/defaults/main.yml:runc_download_url: ""https://github.com/opencontainers/runc/releases/download/{{ runc_version }}/runc.{{ image_arch }}"" roles/download/defaults/main.yml:containerd_download_url: ""https://github.com/containerd/containerd/releases/download/v{{ containerd_version }}/containerd-{{ containerd_version }}-linux-{{ image_arch }}.tar.gz"" if i use default offlie.yml, it's error from task download files because runc,containerd down url is none offline i want fix this just add 2 new line",inventory/sample/group_vars/all/offline.yml,MODIFY,8254,"@@ -49,6 +49,10 @@
 # crio_download_base: ""download.opensuse.org/repositories/devel:kubic:libcontainers:stable""
 # crio_download_crio: ""http://{{ crio_download_base }}:/cri-o:/""
 
+# [Optional] runc,containerd: only if you set container_runtime: containerd
+# runc_download_url: ""{{ files_repo }}/{{ runc_version }}/runc.{{ image_arch }}""
+# containerd_download_url: ""{{ files_repo }}/containerd/v{{ containerd_version }}/containerd-{{ containerd_version }}-linux-{{ image_arch }}.tar.gz""
+
 ## CentOS/Redhat/AlmaLinux
 ### For EL7, base and extras repo must be available, for EL8, baseos and appstream
 ### By default we enable those repo automatically
",comment,other,other,コメントの追記のみ,コメントの追加
39acb2b84d5a4d39c108020d1047ed7048d91d80,2022-03-07 13:35:55+00:00,Update ansible-lint to 5.4.0 (#8607) (#8608) * Update ansible-lint to 5.4.0 (#8607) It seems that the Rich version 11.0.0 has a breaking change. So need to update ansible-lint to 5.3.2 or later. * Fix for ansible-lint no-changed-when rule (#8607),roles/kubernetes/kubeadm/tasks/main.yml,MODIFY,8607,"@@ -78,6 +78,7 @@
         --ignore-preflight-errors=DirAvailable--etc-kubernetes-manifests
         --skip-phases={{ kubeadm_join_phases_skip | join(',') }}
       register: kubeadm_join
+      changed_when: kubeadm_join is success
 
   rescue:
 
@@ -89,6 +90,7 @@
         --ignore-preflight-errors=all
         --skip-phases={{ kubeadm_join_phases_skip | join(',') }}
       register: kubeadm_join
+      changed_when: kubeadm_join is success
 
   always:
 
",lint,lint,directive,"* Fix for ansible-lint no-changed-when rule (#8607)

コマンドモジュールを用いているため，changed_whenを追加",changed_whenの追加
39acb2b84d5a4d39c108020d1047ed7048d91d80,2022-03-07 13:35:55+00:00,Update ansible-lint to 5.4.0 (#8607) (#8608) * Update ansible-lint to 5.4.0 (#8607) It seems that the Rich version 11.0.0 has a breaking change. So need to update ansible-lint to 5.3.2 or later. * Fix for ansible-lint no-changed-when rule (#8607),roles/upgrade/pre-upgrade/tasks/main.yml,MODIFY,8607,"@@ -50,6 +50,7 @@
     - name: Cordon node
       command: ""{{ kubectl }} cordon {{ kube_override_hostname|default(inventory_hostname) }}""
       delegate_to: ""{{ groups['kube_control_plane'][0] }}""
+      changed_when: true
 
     - name: Check kubectl version
       command: ""{{ kubectl }} version --client --short""
@@ -110,6 +111,7 @@
           until: drain_fallback_result.rc == 0
           retries: ""{{ drain_fallback_retries }}""
           delay: ""{{ drain_fallback_retry_delay_seconds }}""
+          changed_when: drain_fallback_result.rc == 0
       when:
         - drain_nodes
         - drain_fallback_enabled
",lint,lint,directive,"* Fix for ansible-lint no-changed-when rule (#8607)

コマンドモジュールを用いているため，changed_whenを追加",changed_whenの追加
39acb2b84d5a4d39c108020d1047ed7048d91d80,2022-03-07 13:35:55+00:00,Update ansible-lint to 5.4.0 (#8607) (#8608) * Update ansible-lint to 5.4.0 (#8607) It seems that the Rich version 11.0.0 has a breaking change. So need to update ansible-lint to 5.3.2 or later. * Fix for ansible-lint no-changed-when rule (#8607),roles/reset/tasks/main.yml,MODIFY,8607,"@@ -148,6 +148,7 @@
     - name: reset | force remove all cri pods (rescue)
       shell: ""ip netns list | cut -d' ' -f 1 | xargs -n1 ip netns delete && {{ bin_dir }}/crictl rmp -a -f""
       ignore_errors: true  # noqa ignore-errors
+      changed_when: true
 
 - name: reset | stop etcd services
   service:
",lint,lint,directive,"* Fix for ansible-lint no-changed-when rule (#8607)

コマンドモジュールを用いているため，changed_whenを追加",changed_whenの追加
889454f2bc35dbfc69da8824d2e8bc658ba08412,2022-06-13 21:10:12+00:00,Fix typo in calico check (#8969),roles/network_plugin/calico/tasks/check.yml,MODIFY,8969,"@@ -174,7 +174,7 @@
   run_once: True
   delegate_to: ""{{ groups['kube_control_plane'][0] }}""
 
-- name: ""Check ipip mode is Nerver for calco ipv6""
+- name: ""Check ipip mode is Never for calico ipv6""
   assert:
     that:
       - ""calico_ipip_mode_ipv6 in ['Never']""
",rename_task,rename_task,rename_task,typo,タスク名のtypoの修正
4b3db07cdb6b08c78045a12a1697e341b9b08adc,2022-07-26 17:29:10+00:00,Fix calicoctl version to v3.23.3 (#9121) Signed-off-by: bo.jiang <bo.jiang@daocloud.io>,roles/download/defaults/main.yml,MODIFY,9121,"@@ -557,11 +557,11 @@ calicoctl_binary_checksums:
     v3.22.3: a9e5f6bad4ad8c543f6bdcd21d3665cdd23edc780860d8e52a87881a7b3e203c
     v3.21.5: 98407b1c608fec0896004767c72cd4b6cf939976d67d3eca121f1f02137c92a7
   arm64:
-    v3.23.2: 741b222f9bb10b7b5e268e5362796061c8862d4f785bb6b9c4f623ea143f4682
+    v3.23.3: 741b222f9bb10b7b5e268e5362796061c8862d4f785bb6b9c4f623ea143f4682
     v3.22.3: 3a3e70828c020efd911181102d21cb4390b7b68669898bd40c0c69b64d11bb63
     v3.21.5: cc73e2b8f5b695b6ab06e7856cd516c1e9ec3e903abb510ef465ca6b530e18e6
   ppc64le:
-    v3.23.2: f83efcd8d3d7c96dfe8e596dc9739eb5d9616626a6afba29b0af97e5c222575a
+    v3.23.3: f83efcd8d3d7c96dfe8e596dc9739eb5d9616626a6afba29b0af97e5c222575a
     v3.22.3: 7c2fe391f2a18eccff65c64bf93133dc5c58c7322cbd31ea207bbfef5b563947
     v3.21.5: 1ebb615b18f9c3fe2d41281d1bc9e3909048b56d2bc76c18431cbeb7a653d24d
 
",check_sum,check_sum,check_sum,,チェックサムの対応の修正
1baabb3c059860d1223848ecf8d1085941069e6b,2022-08-30 15:03:02+00:00,Fix cloud_init files for different distros (#9232),tests/cloud_playbooks/roles/packet-ci/defaults/main.yml,MODIFY,9232,"@@ -18,21 +18,25 @@ inventory_path: ""/tmp/{{ test_name }}/inventory""
 mode: aio
 
 # Cloud init config for each os type
+# distro: fedora -> I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IGZlZG9yYQp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIGdyb3Vwczogd2hlZWwKICAgc3VkbzogJ0FMTD0oQUxMKSBOT1BBU1NXRDpBTEwnCiAgIHNoZWxsOiAvYmluL2Jhc2gKICAgbG9ja19wYXNzd2Q6IEZhbHNlCiAgIGhvbWU6IC9ob21lL2t1YmVzcHJheQogICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgIC0gc3NoLXJzYSBBQUFBQjNOemFDMXljMkVBQUFBREFRQUJBQUFCQVFDYW5UaS9lS3gwK3RIWUpBZURocStzRlMyT2JVUDEvSTY5ZjdpVjNVdGtLbFQyMEpmVzFmNkZlWHQvMDRWZjI3V1FxK05xczZ2R0JxRDlRWFNZdWYrdDAvczdFUExqVGVpOW1lMW1wcXIrdVRlK0tEdFRQMzlwZkQzL2VWQ2FlQjcyNkdQMkZrYUQwRnpwbUViNjZPM05xaHhPUTk2R3gvOVhUdXcvSzNsbGo0T1ZENkdyalIzQjdjNFh0RUJzWmNacHBNSi9vSDFtR3lHWGRoMzFtV1FTcUFSTy9QOFU4R3d0MCtIR3BVd2gvaGR5M3QrU1lvVEIyR3dWYjB6b3lWd3RWdmZEUXpzbThmcTNhdjRLdmV6OGtZdU5ESnYwNXg0bHZVWmdSMTVaRFJYc0FuZGhReXFvWGRDTEFlMCtlYUtYcTlCa1d4S0ZiOWhQZTBBVWpqYTU=
+# distro: rhel: -> I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=
+# distro: rhel (+ sudo and hostname packages): -> I2Nsb3VkLWNvbmZpZwpwYWNrYWdlczoKIC0gc3VkbwogLSBob3N0bmFtZQpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=
+# generic one -> I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1
 cloud_init:
   centos-7: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
   centos-8: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
   almalinux-8: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
   rockylinux-8: ""I2Nsb3VkLWNvbmZpZwpwYWNrYWdlczoKIC0gc3VkbwogLSBob3N0bmFtZQpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
-  debian-9: ""I2Nsb3VkLWNvbmZpZwp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgc2hlbGw6IC9iaW4vYmFzaAogICBob21lOiAvaG9tZS91YnVudHUKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
-  debian-10: ""I2Nsb3VkLWNvbmZpZwp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgc2hlbGw6IC9iaW4vYmFzaAogICBob21lOiAvaG9tZS91YnVudHUKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
-  debian-11: ""I2Nsb3VkLWNvbmZpZwp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgc2hlbGw6IC9iaW4vYmFzaAogICBob21lOiAvaG9tZS91YnVudHUKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
-  fedora-35: ""I2Nsb3VkLWNvbmZpZwp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgc2hlbGw6IC9iaW4vYmFzaAogICBob21lOiAvaG9tZS91YnVudHUKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
-  fedora-36: ""I2Nsb3VkLWNvbmZpZwp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgc2hlbGw6IC9iaW4vYmFzaAogICBob21lOiAvaG9tZS91YnVudHUKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
-  opensuse-leap-15: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
+  debian-9: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
+  debian-10: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
+  debian-11: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
+  fedora-35: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IGZlZG9yYQp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIGdyb3Vwczogd2hlZWwKICAgc3VkbzogJ0FMTD0oQUxMKSBOT1BBU1NXRDpBTEwnCiAgIHNoZWxsOiAvYmluL2Jhc2gKICAgbG9ja19wYXNzd2Q6IEZhbHNlCiAgIGhvbWU6IC9ob21lL2t1YmVzcHJheQogICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgIC0gc3NoLXJzYSBBQUFBQjNOemFDMXljMkVBQUFBREFRQUJBQUFCQVFDYW5UaS9lS3gwK3RIWUpBZURocStzRlMyT2JVUDEvSTY5ZjdpVjNVdGtLbFQyMEpmVzFmNkZlWHQvMDRWZjI3V1FxK05xczZ2R0JxRDlRWFNZdWYrdDAvczdFUExqVGVpOW1lMW1wcXIrdVRlK0tEdFRQMzlwZkQzL2VWQ2FlQjcyNkdQMkZrYUQwRnpwbUViNjZPM05xaHhPUTk2R3gvOVhUdXcvSzNsbGo0T1ZENkdyalIzQjdjNFh0RUJzWmNacHBNSi9vSDFtR3lHWGRoMzFtV1FTcUFSTy9QOFU4R3d0MCtIR3BVd2gvaGR5M3QrU1lvVEIyR3dWYjB6b3lWd3RWdmZEUXpzbThmcTNhdjRLdmV6OGtZdU5ESnYwNXg0bHZVWmdSMTVaRFJYc0FuZGhReXFvWGRDTEFlMCtlYUtYcTlCa1d4S0ZiOWhQZTBBVWpqYTU=""
+  fedora-36: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IGZlZG9yYQp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIGdyb3Vwczogd2hlZWwKICAgc3VkbzogJ0FMTD0oQUxMKSBOT1BBU1NXRDpBTEwnCiAgIHNoZWxsOiAvYmluL2Jhc2gKICAgbG9ja19wYXNzd2Q6IEZhbHNlCiAgIGhvbWU6IC9ob21lL2t1YmVzcHJheQogICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgIC0gc3NoLXJzYSBBQUFBQjNOemFDMXljMkVBQUFBREFRQUJBQUFCQVFDYW5UaS9lS3gwK3RIWUpBZURocStzRlMyT2JVUDEvSTY5ZjdpVjNVdGtLbFQyMEpmVzFmNkZlWHQvMDRWZjI3V1FxK05xczZ2R0JxRDlRWFNZdWYrdDAvczdFUExqVGVpOW1lMW1wcXIrdVRlK0tEdFRQMzlwZkQzL2VWQ2FlQjcyNkdQMkZrYUQwRnpwbUViNjZPM05xaHhPUTk2R3gvOVhUdXcvSzNsbGo0T1ZENkdyalIzQjdjNFh0RUJzWmNacHBNSi9vSDFtR3lHWGRoMzFtV1FTcUFSTy9QOFU4R3d0MCtIR3BVd2gvaGR5M3QrU1lvVEIyR3dWYjB6b3lWd3RWdmZEUXpzbThmcTNhdjRLdmV6OGtZdU5ESnYwNXg0bHZVWmdSMTVaRFJYc0FuZGhReXFvWGRDTEFlMCtlYUtYcTlCa1d4S0ZiOWhQZTBBVWpqYTU=""
+  opensuse-leap-15: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
   rhel-server-7: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
   amazon-linux-2: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
   ubuntu-1604: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
   ubuntu-1804: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
   ubuntu-2004: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
   ubuntu-2204: ""I2Nsb3VkLWNvbmZpZwogdXNlcnM6CiAgLSBuYW1lOiBrdWJlc3ByYXkKICAgIHN1ZG86IEFMTD0oQUxMKSBOT1BBU1NXRDpBTEwKICAgIHNoZWxsOiAvYmluL2Jhc2gKICAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICAgaG9tZTogL2hvbWUva3ViZXNwcmF5CiAgICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1""
-  oracle-7: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IGZlZG9yYQp1c2VyczoKIC0gbmFtZToga3ViZXNwcmF5CiAgIGdyb3Vwczogd2hlZWwKICAgc3VkbzogJ0FMTD0oQUxMKSBOT1BBU1NXRDpBTEwnCiAgIHNoZWxsOiAvYmluL2Jhc2gKICAgbG9ja19wYXNzd2Q6IEZhbHNlCiAgIGhvbWU6IC9ob21lL2t1YmVzcHJheQogICBzc2hfYXV0aG9yaXplZF9rZXlzOgogICAgIC0gc3NoLXJzYSBBQUFBQjNOemFDMXljMkVBQUFBREFRQUJBQUFCQVFDYW5UaS9lS3gwK3RIWUpBZURocStzRlMyT2JVUDEvSTY5ZjdpVjNVdGtLbFQyMEpmVzFmNkZlWHQvMDRWZjI3V1FxK05xczZ2R0JxRDlRWFNZdWYrdDAvczdFUExqVGVpOW1lMW1wcXIrdVRlK0tEdFRQMzlwZkQzL2VWQ2FlQjcyNkdQMkZrYUQwRnpwbUViNjZPM05xaHhPUTk2R3gvOVhUdXcvSzNsbGo0T1ZENkdyalIzQjdjNFh0RUJzWmNacHBNSi9vSDFtR3lHWGRoMzFtV1FTcUFSTy9QOFU4R3d0MCtIR3BVd2gvaGR5M3QrU1lvVEIyR3dWYjB6b3lWd3RWdmZEUXpzbThmcTNhdjRLdmV6OGtZdU5ESnYwNXg0bHZVWmdSMTVaRFJYc0FuZGhReXFvWGRDTEFlMCtlYUtYcTlCa1d4S0ZiOWhQZTBBVWpqYTU=""
+  oracle-7: ""I2Nsb3VkLWNvbmZpZwpzeXN0ZW1faW5mbzoKICBkaXN0cm86IHJoZWwKdXNlcnM6CiAtIG5hbWU6IGt1YmVzcHJheQogICBncm91cHM6IHdoZWVsCiAgIHN1ZG86ICdBTEw9KEFMTCkgTk9QQVNTV0Q6QUxMJwogICBzaGVsbDogL2Jpbi9iYXNoCiAgIGxvY2tfcGFzc3dkOiBGYWxzZQogICBob21lOiAvaG9tZS9rdWJlc3ByYXkKICAgc3NoX2F1dGhvcml6ZWRfa2V5czoKICAgICAtIHNzaC1yc2EgQUFBQUIzTnphQzF5YzJFQUFBQURBUUFCQUFBQkFRQ2FuVGkvZUt4MCt0SFlKQWVEaHErc0ZTMk9iVVAxL0k2OWY3aVYzVXRrS2xUMjBKZlcxZjZGZVh0LzA0VmYyN1dRcStOcXM2dkdCcUQ5UVhTWXVmK3QwL3M3RVBMalRlaTltZTFtcHFyK3VUZStLRHRUUDM5cGZEMy9lVkNhZUI3MjZHUDJGa2FEMEZ6cG1FYjY2TzNOcWh4T1E5Nkd4LzlYVHV3L0szbGxqNE9WRDZHcmpSM0I3YzRYdEVCc1pjWnBwTUovb0gxbUd5R1hkaDMxbVdRU3FBUk8vUDhVOEd3dDArSEdwVXdoL2hkeTN0K1NZb1RCMkd3VmIwem95Vnd0VnZmRFF6c204ZnEzYXY0S3ZlejhrWXVOREp2MDV4NGx2VVpnUjE1WkRSWHNBbmRoUXlxb1hkQ0xBZTArZWFLWHE5QmtXeEtGYjloUGUwQVVqamE1Cgo=""
",edit_var,check_sum,edit_var,,チェックサムじゃなくて、base64っぽい
